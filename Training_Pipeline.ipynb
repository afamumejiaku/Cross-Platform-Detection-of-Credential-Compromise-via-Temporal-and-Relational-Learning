{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763cfac7-2feb-4b5d-ac45-557b774f46a2",
   "metadata": {},
   "source": [
    "# Reading the Complete Email-Password "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba1c5f3-a4b0-4a03-b7a2-e77883995a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading downloaded files store, selected only gmail emails.\n",
    "Storing selected emails into single json file.\n",
    "json file contains selection email and passwords.\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "\n",
    "def process_files_in_folder(folder_path, email_passwords):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        full_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Skip directories\n",
    "        if os.path.isdir(full_path):\n",
    "            continue\n",
    "\n",
    "        # Skip files that start with a digit or are named 'symbols'\n",
    "        if filename[0].isdigit() or filename.lower() == \"symbols\":\n",
    "            continue\n",
    "        #if not filename.endswith('.txt'):\n",
    "           # continue\n",
    "\n",
    "        try:\n",
    "            with open(full_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                for line in file:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    line = line[1:]  # Remove first character (assumed '0')\n",
    "                    if ':' not in line:\n",
    "                        continue\n",
    "                    email, password = line.split(':', 1)\n",
    "                    if email.endswith(\"@gmail.com\"):\n",
    "                        if email not in email_passwords:\n",
    "                            email_passwords[email] = []\n",
    "                        email_passwords[email].append(password)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {full_path}: {e}\")\n",
    "\n",
    "def process_multiple_folders(parent_directory):\n",
    "    email_passwords = {}\n",
    "    for root, dirs, files in os.walk(parent_directory):\n",
    "        process_files_in_folder(root, email_passwords)\n",
    "    return email_passwords\n",
    "\n",
    "parent_dir = 'data'  # Folder path\n",
    "result = process_multiple_folders(parent_dir)\n",
    "with open(\"email_passwords.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f457d402-a604-4529-800a-4b9a4430700d",
   "metadata": {},
   "source": [
    "# Filtering out Gmail Accounts with multiple incidents of password breaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac4e82c-2766-4924-9f76-12618545815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to your input file\n",
    "input_file = \"email_passwords.json\"\n",
    "output_file = \"email_passwords_used.json\"\n",
    "\n",
    "# Read JSON data\n",
    "try:\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        result = json.load(f)\n",
    "    print(f\"Loaded {len(result)} total email entries from {input_file}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{input_file}' was not found.\")\n",
    "    exit(1)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON from '{input_file}': {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Filter to only Gmail accounts with at least 20 password breaches\n",
    "result_main = {\n",
    "    email: passwords\n",
    "    for email, passwords in result.items()\n",
    "    if email.lower().endswith(\"@gmail.com\") and len(passwords) >= 20\n",
    "}\n",
    "\n",
    "# Save filtered data\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result_main, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fecc2c-5eab-4872-ae86-3a583edbbed1",
   "metadata": {},
   "source": [
    "## Getting Metadata and additional information from haveibeenpwned.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9044aa-1f1a-412d-b2ec-7cd622789139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "pwned_Api_key = {{ HIBP_KEY }}\n",
    "\n",
    "def get_hibp_metadata_for_emails(\n",
    "    emails_or_dict,\n",
    "    api_key: str,\n",
    "    delay: float = 1.6,                 # HIBP >=1.5s between requests for my subscription\n",
    "    include_unverified: bool = True,\n",
    "    truncate: bool = False,\n",
    "    domain: str | None = None,\n",
    "    retries: int = 3,\n",
    "    backoff: float = 2.0,\n",
    "    return_nan_on_404: bool = False     # set True to return [\"NAN\"] instead of [] when no breaches\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch HIBP breach metadata for each email.\n",
    "\n",
    "    Inputs:\n",
    "      emails_or_dict: list/iterable of emails OR dict with emails as keys\n",
    "      api_key: your HIBP API key\n",
    "      delay: seconds to sleep between requests (rate limit)\n",
    "      include_unverified: include unverified breaches\n",
    "      truncate: HIBP 'truncateResponse' option\n",
    "      domain: optional domain filter (e.g., \"adobe.com\")\n",
    "      retries: request retries for transient errors (429/503/network)\n",
    "      backoff: exponential backoff base\n",
    "      return_nan_on_404: if True, store [\"NAN\"] when no breaches; else store []\n",
    "\n",
    "    Returns:\n",
    "      dict[email] = list[{\"Name\",\"Domain\",\"BreachDate\"}] or [] (or [\"NAN\"] if configured)\n",
    "    \"\"\"\n",
    "    # Accept dict or list\n",
    "    if isinstance(emails_or_dict, dict):\n",
    "        emails = list(emails_or_dict.keys())\n",
    "    else:\n",
    "        emails = list(emails_or_dict)\n",
    "\n",
    "    sess = requests.Session()\n",
    "    headers = {\n",
    "        \"hibp-api-key\": api_key,\n",
    "        \"User-Agent\": \"Research/1.0\"\n",
    "    }\n",
    "    base_url = \"https://haveibeenpwned.com/api/v3/breachedaccount/\"\n",
    "\n",
    "    results: dict[str, list | str] = {}\n",
    "\n",
    "    for email in emails:\n",
    "        url = base_url + urllib.parse.quote(email) \n",
    "        params = {\n",
    "            \"truncateResponse\": \"true\" if truncate else \"false\",\n",
    "            \"includeUnverified\": \"true\" if include_unverified else \"false\",\n",
    "        }\n",
    "        if domain:\n",
    "            params[\"domain\"] = domain\n",
    "\n",
    "        # Retry loop for transient errors\n",
    "        last_err = None\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                resp = sess.get(url, headers=headers, params=params, timeout=30)\n",
    "\n",
    "                if resp.status_code == 200:\n",
    "                    breaches = resp.json()  # list of breach dicts\n",
    "                    # Normalize to the requested minimal fields\n",
    "                    results[email] = [\n",
    "                        {\n",
    "                            \"Name\": b.get(\"Name\"),\n",
    "                            \"Domain\": b.get(\"Domain\"),\n",
    "                            \"BreachDate\": b.get(\"BreachDate\"),\n",
    "                        }\n",
    "                        for b in breaches\n",
    "                    ]\n",
    "                    break  # success\n",
    "\n",
    "                elif resp.status_code == 404:\n",
    "                    # No breach for this account\n",
    "                    results[email] = [\"NAN\"] if return_nan_on_404 else []\n",
    "                    break\n",
    "\n",
    "                elif resp.status_code in (429, 503):\n",
    "                    # Rate limited or service unavailable — backoff and retry\n",
    "                    sleep_s = backoff ** attempt\n",
    "                    time.sleep(sleep_s)\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    # Other HTTP errors — record a simple marker\n",
    "                    results[email] = [f\"ERROR {resp.status_code}: {resp.text[:120]}\"]\n",
    "                    break\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                last_err = e\n",
    "                time.sleep(backoff ** attempt)\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            # Exhausted retries\n",
    "            msg = f\"REQUEST_FAILED: {last_err}\" if last_err else \"REQUEST_FAILED\"\n",
    "            results[email] = [msg]\n",
    "\n",
    "        # Respect HIBP per-request delay\n",
    "        time.sleep(delay)\n",
    "\n",
    "    return results\n",
    "api_key = pwned_Api_key  \n",
    "hibp_metadata = get_hibp_metadata_for_emails(reduced_dict, api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281d5473-cead-4ba6-9548-4dc1220d843d",
   "metadata": {},
   "source": [
    "## Merging Email-Password data and Metadata using Roundrobin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf352e-5d0b-4d76-ac3a-4bed88757ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def merge_breach_password_dicts(\n",
    "    dict_a: Dict[str, List[str]],  # Email -> List of passwords\n",
    "    dict_b: Dict[str, List[Dict[str, str]]],  # Email -> List of breach dicts\n",
    "    allowed_names: List[str],  # List of allowed breach names\n",
    "    assignment_strategy: str = \"round_robin\",  # How to assign passwords to breaches\n",
    "    random_seed: int = 42\n",
    ") -> Dict[str, List[Dict[str, str]]]:\n",
    "    \"\"\"\n",
    "    Merge two dictionaries to create a combined breach-password mapping.\n",
    "    \n",
    "    Args:\n",
    "        dict_a: Dictionary mapping emails to lists of passwords\n",
    "        dict_b: Dictionary mapping emails to lists of breach information\n",
    "        allowed_names: List of breach names to include in the result\n",
    "        assignment_strategy: Method for assigning passwords to breaches\n",
    "            - \"round_robin\": Cycle through passwords for each breach\n",
    "            - \"random\": Randomly assign passwords to breaches\n",
    "            - \"sequential\": Assign passwords in order to breaches\n",
    "        random_seed: Seed for random assignment (if using random strategy)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping emails to filtered breach data with passwords\n",
    "    \"\"\"\n",
    "    \n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "    \n",
    "    # Convert allowed_names to set for faster lookup\n",
    "    allowed_names_set = set(allowed_names)\n",
    "    \n",
    "    # Find common emails between both dictionaries\n",
    "    common_emails = set(dict_a.keys()) & set(dict_b.keys())\n",
    "    \n",
    "    # print(f\"DictA has {len(dict_a)} emails\")\n",
    "    # print(f\"DictB has {len(dict_b)} emails\") \n",
    "    # print(f\"Common emails: {len(common_emails)}\")\n",
    "    # print(f\"Allowed breach names: {len(allowed_names)}\")\n",
    "    \n",
    "    result = {}\n",
    "    stats = {\n",
    "        \"processed_emails\": 0,\n",
    "        \"total_breaches_before_filter\": 0,\n",
    "        \"total_breaches_after_filter\": 0,\n",
    "        \"emails_with_no_valid_breaches\": 0,\n",
    "        \"breach_name_distribution\": defaultdict(int)\n",
    "    }\n",
    "    \n",
    "    for email in common_emails:\n",
    "        passwords = dict_a[email]\n",
    "        breaches = dict_b[email]\n",
    "        \n",
    "        stats[\"processed_emails\"] += 1\n",
    "        stats[\"total_breaches_before_filter\"] += len(breaches)\n",
    "        \n",
    "        # Filter breaches to only include allowed names\n",
    "        filtered_breaches = []\n",
    "        for breach in breaches:\n",
    "            if isinstance(breach, dict) and breach.get(\"Name\") in allowed_names_set:\n",
    "                filtered_breaches.append(breach.copy())  # Copy to avoid modifying original\n",
    "                stats[\"breach_name_distribution\"][breach[\"Name\"]] += 1\n",
    "        \n",
    "        stats[\"total_breaches_after_filter\"] += len(filtered_breaches)\n",
    "        \n",
    "        if not filtered_breaches:\n",
    "            stats[\"emails_with_no_valid_breaches\"] += 1\n",
    "            result[email] = []  # Empty list for emails with no valid breaches\n",
    "            continue\n",
    "        \n",
    "        if not passwords:\n",
    "            # If no passwords available, skip this email\n",
    "            continue\n",
    "        \n",
    "        # Assign passwords to breaches based on strategy\n",
    "        enhanced_breaches = assign_passwords_to_breaches(\n",
    "            filtered_breaches, passwords, assignment_strategy\n",
    "        )\n",
    "        \n",
    "        result[email] = enhanced_breaches\n",
    "    \n",
    "    # Print statistics\n",
    "    print_merge_statistics(stats, allowed_names)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def assign_passwords_to_breaches(\n",
    "    breaches: List[Dict[str, str]], \n",
    "    passwords: List[str], \n",
    "    strategy: str\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Assign passwords to breach records based on the specified strategy.\n",
    "    \"\"\"\n",
    "    enhanced_breaches = []\n",
    "    \n",
    "    if strategy == \"round_robin\":\n",
    "        # Cycle through passwords for each breach\n",
    "        for i, breach in enumerate(breaches):\n",
    "            password = passwords[i % len(passwords)]\n",
    "            enhanced_breach = breach.copy()\n",
    "            enhanced_breach[\"Password\"] = password\n",
    "            enhanced_breaches.append(enhanced_breach)\n",
    "            \n",
    "    elif strategy == \"random\":\n",
    "        # Randomly assign passwords to breaches\n",
    "        for breach in breaches:\n",
    "            password = random.choice(passwords)\n",
    "            enhanced_breach = breach.copy()\n",
    "            enhanced_breach[\"Password\"] = password\n",
    "            enhanced_breaches.append(enhanced_breach)\n",
    "            \n",
    "    elif strategy == \"sequential\":\n",
    "        # Assign passwords in order, cycling if needed\n",
    "        for i, breach in enumerate(breaches):\n",
    "            password = passwords[i % len(passwords)]\n",
    "            enhanced_breach = breach.copy()\n",
    "            enhanced_breach[\"Password\"] = password\n",
    "            enhanced_breaches.append(enhanced_breach)\n",
    "            \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown assignment strategy: {strategy}\")\n",
    "    \n",
    "    return enhanced_breaches\n",
    "\n",
    "def print_merge_statistics(stats: Dict[str, Any], allowed_names: List[str]) -> None:\n",
    "    \"\"\"Print detailed statistics about the merge operation.\"\"\"\n",
    "    # print(\"\\n=== MERGE STATISTICS ===\")\n",
    "    # print(f\"Processed emails: {stats['processed_emails']}\")\n",
    "    # print(f\"Total breaches before filtering: {stats['total_breaches_before_filter']}\")\n",
    "    # print(f\"Total breaches after filtering: {stats['total_breaches_after_filter']}\")\n",
    "    # print(f\"Emails with no valid breaches: {stats['emails_with_no_valid_breaches']}\")\n",
    "    \n",
    "    # if stats['total_breaches_before_filter'] > 0:\n",
    "    #     filter_rate = (stats['total_breaches_after_filter'] / stats['total_breaches_before_filter']) * 100\n",
    "    #     print(f\"Breach retention rate: {filter_rate:.2f}%\")\n",
    "    \n",
    "    # print(\"\\nBreach name distribution in result:\")\n",
    "    # for name in allowed_names:\n",
    "    #     count = stats['breach_name_distribution'].get(name, 0)\n",
    "    #     print(f\"  {name}: {count} occurrences\")\n",
    "    \n",
    "    # unused_names = set(allowed_names) - set(stats['breach_name_distribution'].keys())\n",
    "    # if unused_names:\n",
    "    #     print(f\"\\nUnused breach names: {sorted(unused_names)}\")\n",
    "\n",
    "def validate_input_data(\n",
    "    dict_a: Dict[str, List[str]], \n",
    "    dict_b: Dict[str, List[Dict[str, str]]], \n",
    "    allowed_names: List[str]\n",
    ") -> None:\n",
    "    \"\"\"Validate the input data structures.\"\"\"\n",
    "    \n",
    "    # # Validate dict_a\n",
    "    # print(f\"DictA validation:\")\n",
    "    valid_a = 0\n",
    "    for email, passwords in list(dict_a.items())[:5]:  # Check first 5\n",
    "        if isinstance(passwords, list) and all(isinstance(p, str) for p in passwords):\n",
    "            valid_a += 1\n",
    "        print(f\"  {email}: {len(passwords) if isinstance(passwords, list) else 'Invalid'} passwords\")\n",
    "    \n",
    "    # Validate dict_b\n",
    "    # print(f\"DictB validation:\")\n",
    "    valid_b = 0\n",
    "    for email, breaches in list(dict_b.items())[:5]:  # Check first 5\n",
    "        if isinstance(breaches, list):\n",
    "            valid_breaches = 0\n",
    "            for breach in breaches:\n",
    "                if isinstance(breach, dict) and all(k in breach for k in [\"Name\", \"Domain\", \"BreachDate\"]):\n",
    "                    valid_breaches += 1\n",
    "            # print(f\"  {email}: {len(breaches)} total, {valid_breaches} valid breach records\")\n",
    "            if valid_breaches == len(breaches):\n",
    "                valid_b += 1\n",
    "        else:\n",
    "            print(f\"  {email}: Invalid breach data\")\n",
    "    \n",
    "    # print(f\"Allowed names: {len(allowed_names)} names\")\n",
    "\n",
    "# Main function to use\n",
    "def create_merged_dictionary(\n",
    "    dict_a: Dict[str, List[str]], \n",
    "    dict_b: Dict[str, List[Dict[str, str]]], \n",
    "    allowed_names: List[str],\n",
    "    validate_inputs: bool = True,\n",
    "    show_sample: bool = True,\n",
    "    assignment_strategy: str = \"round_robin\"\n",
    ") -> Dict[str, List[Dict[str, str]]]:\n",
    "    \"\"\"\n",
    "    Main function to create the merged dictionary.\n",
    "    \n",
    "    Args:\n",
    "        dict_a: Email -> List of passwords\n",
    "        dict_b: Email -> List of breach information  \n",
    "        allowed_names: List of allowed breach names\n",
    "        validate_inputs: Whether to validate input data\n",
    "        assignment_strategy: How to assign passwords (\"round_robin\", \"random\", \"sequential\")\n",
    "    \n",
    "    Returns:\n",
    "        Merged dictionary with breach data + passwords\n",
    "    \"\"\"\n",
    "    \n",
    "    if validate_inputs:\n",
    "        validate_input_data(dict_a, dict_b, allowed_names)\n",
    "    \n",
    "    # Create the merged dictionary\n",
    "    merged_dict = merge_breach_password_dicts(\n",
    "        dict_a, dict_b, allowed_names, assignment_strategy\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return merged_dict\n",
    "\n",
    "##################\n",
    "if __name__ == \"__main__\":\n",
    "    # Email and passwords\n",
    "    dict_a = result_main\n",
    "    # Email and breach metadata\n",
    "    dict_b = hibp_metadata\n",
    "    # Selected the most dorminant 30 platforms \n",
    "    allowed_names = top_30_breach_names\n",
    "    \n",
    "    # Create the merged dictionary\n",
    "    train_data = create_merged_dictionary(\n",
    "        dict_a,\n",
    "        dict_b, \n",
    "        allowed_names,\n",
    "        assignment_strategy=\"round_robin\"\n",
    "    )\n",
    "    with open(\"trainData.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(train_data, f, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbee192-5b6c-4be7-b8f1-2a90eda1cc99",
   "metadata": {},
   "source": [
    "## Generating Honeyword using simple substitution and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8558d0-2803-4d9f-8bf9-6df8f43042b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate honeywords for each breach password in `trainData.json`.\n",
    "\n",
    "- Adds a \"Honeywords\" list (100 items) to each breach dict that has a \"Password\" key.\n",
    "- By default: 50 local heuristic-generated honeywords + 50 generated by OpenAI (optional).\n",
    "- Saves augmented dataset to `trainData_with_honeywords.json`.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "from typing import List, Set, Dict\n",
    "import openai\n",
    "\n",
    "# CONFIGURATION\n",
    "INPUT_FILE = \"trainData_enriched_only.json\"\n",
    "OUTPUT_FILE = \"trainData_enriched_with_honeywords.json\"\n",
    "\n",
    "TOTAL_HONEYWORDS = 100\n",
    "OPENAI_SHARE = 0#.5          # fraction generated by OpenAI (0.0..1.0). 0.5 -> 50%\n",
    "USE_OPENAI = True             # set False to skip OpenAI and use all-local generation\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")  \n",
    "OPENAI_MODEL = \"gpt-4o-mini\"  # change if you prefer another available model\n",
    "OPENAI_BATCH = 10             # how many passwords to ask in each model call (batching helps)\n",
    "OPENAI_DELAY = 1.5            # seconds between OpenAI requests to avoid throttling\n",
    "OPENAI_RETRIES = 3\n",
    "RANDOM_SEED = 42\n",
    "LOCAL_MUTATION_TRIES = 400    # safety cap on mutation attempts to reach uniqueness when needed\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "### ---------- Local honeyword generators (heuristics) ----------\n",
    "\n",
    "def leetify(s: str) -> str:\n",
    "    subs = str.maketrans({\n",
    "        'a': '@', 'A': '@',\n",
    "        'o': '0', 'O': '0',\n",
    "        'i': '1', 'I': '1',\n",
    "        'e': '3', 'E': '3',\n",
    "        's': '$', 'S': '$',\n",
    "        't': '7', 'T': '7',\n",
    "        'l': '1', 'L': '1'\n",
    "    })\n",
    "    return s.translate(subs)\n",
    "\n",
    "def append_suffix(s: str, suffix: str) -> str:\n",
    "    return s + suffix\n",
    "\n",
    "def prepend_prefix(s: str, prefix: str) -> str:\n",
    "    return prefix + s\n",
    "\n",
    "def inset_char_random(s: str) -> str:\n",
    "    pos = random.randint(0, max(0, len(s)))\n",
    "    ch = random.choice(string.ascii_letters + string.digits + \"!@#$%\")\n",
    "    return s[:pos] + ch + s[pos:]\n",
    "\n",
    "def transpose_two_chars(s: str) -> str:\n",
    "    if len(s) < 2:\n",
    "        return s\n",
    "    i = random.randint(0, len(s)-2)\n",
    "    lst = list(s)\n",
    "    lst[i], lst[i+1] = lst[i+1], lst[i]\n",
    "    return \"\".join(lst)\n",
    "\n",
    "def replace_vowel_with_digit(s: str) -> str:\n",
    "    return s.replace('a','4').replace('A','4').replace('e','3').replace('E','3').replace('o','0').replace('O','0')\n",
    "\n",
    "def reverse_string(s: str) -> str:\n",
    "    return s[::-1]\n",
    "\n",
    "COMMON_SUFFIXES = [\"123\", \"2020\", \"2021\", \"2022\", \"!\", \"!!\", \"@123\", \"#1\", \"XYZ\", \"_\", \"-\"]\n",
    "COMMON_PREFIXES = [\"!\", \"x\", \"the\", \"my\", \"NN\", \"00\"]\n",
    "\n",
    "def local_mutations(base: str) -> List[str]:\n",
    "    \"\"\"Produce a number of heuristic mutations of base password\"\"\"\n",
    "    muts = []\n",
    "    # Basic variations\n",
    "    muts.append(base + random.choice(COMMON_SUFFIXES))\n",
    "    muts.append(prepend_prefix(base, random.choice(COMMON_PREFIXES)))\n",
    "    muts.append(leetify(base))\n",
    "    muts.append(leetify(base) + random.choice(COMMON_SUFFIXES))\n",
    "    muts.append(replace_vowel_with_digit(base))\n",
    "    muts.append(reverse_string(base))\n",
    "    muts.append(transpose_two_chars(base))\n",
    "    muts.append(inset_char_random(base))\n",
    "    muts.append(base + str(random.randint(10,99)))\n",
    "    muts.append(base.capitalize())\n",
    "    muts.append(base.lower() + random.choice([\"!\", \"@\", \"#\"]))\n",
    "    muts.append(base + \"-\" + str(random.randint(100,999)))\n",
    "    # Add some random strings that resemble passwords\n",
    "    muts.append(\"\".join(random.choices(string.ascii_letters + string.digits, k=min(8, max(4, len(base)))))\n",
    "                )\n",
    "    muts.append(base + random.choice([\"_x\", \"_1\", \"_01\"]))\n",
    "    # and so on...\n",
    "    # return unique and filtered\n",
    "    unique = []\n",
    "    for m in muts:\n",
    "        if m and m != base and m not in unique:\n",
    "            unique.append(m)\n",
    "    return unique\n",
    "\n",
    "def generate_local_honeywords(password: str, count: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate `count` honeywords locally using deterministic+random mutations.\n",
    "    Attempt to produce plausible and diverse variants; ensure uniqueness and not equal to the real password.\n",
    "    \"\"\"\n",
    "    out: List[str] = []\n",
    "    tries = 0\n",
    "    # deterministic anchor mutations first\n",
    "    anchors = local_mutations(password)\n",
    "    for a in anchors:\n",
    "        if len(out) >= count:\n",
    "            break\n",
    "        if a != password and a not in out:write out this ipynb to py files for github\n",
    "            out.append(a)\n",
    "    # then continue with randomized heuristics\n",
    "    while len(out) < count and tries < LOCAL_MUTATION_TRIES:\n",
    "        tries += 1\n",
    "        choice = random.choice([\n",
    "            leetify, append_suffix, prepend_prefix, inset_char_random,\n",
    "            transpose_two_chars, replace_vowel_with_digit, reverse_string\n",
    "        ])\n",
    "        if choice in (append_suffix, prepend_prefix):\n",
    "            if choice is append_suffix:\n",
    "                candidate = append_suffix(password, random.choice(COMMON_SUFFIXES + [str(random.randint(1,9999))]))\n",
    "            else:\n",
    "                candidate = prepend_prefix(password, random.choice(COMMON_PREFIXES + [\"user\", \"admin\", \"sys\"]))\n",
    "        else:\n",
    "            candidate = choice(password)\n",
    "        # small chance to add a random digit tail\n",
    "        if random.random() < 0.25:\n",
    "            candidate = candidate + str(random.randint(0,9999))\n",
    "        if candidate != password and candidate not in out:\n",
    "            out.append(candidate)\n",
    "    # if still short, fill with random plausible strings\n",
    "    while len(out) < count:\n",
    "        length = max(6, min(12, len(password)))\n",
    "        candidate = \"\".join(random.choices(string.ascii_letters + string.digits + \"!@#\", k=length))\n",
    "        if candidate != password and candidate not in out:\n",
    "            out.append(candidate)\n",
    "    return out[:count]\n",
    "\n",
    "\n",
    "### ---------- OpenAI-based honeyword generation ----------\n",
    "\n",
    "def call_openai_generate_honeywords(passwords: List[str], per_password: int, api_key: str, model: str = OPENAI_MODEL) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Given a list of passwords, ask OpenAI to generate `per_password` honeywords for each one.\n",
    "    Returns dict password -> list[honeywords].\n",
    "    Note: This function batches requests if necessary and includes simple retry/backoff.\n",
    "    \"\"\"\n",
    "    if openai is None:\n",
    "        raise RuntimeError(\"openai package not available. Install via `pip install openai` or set USE_OPENAI=False.\")\n",
    "\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OpenAI API key not provided. Set OPENAI_API_KEY or pass api_key.\")\n",
    "\n",
    "    openai.api_key = api_key\n",
    "\n",
    "    results: Dict[str, List[str]] = {}\n",
    "    # We'll ask the model to output JSON mapping, but to keep it simple we do per-password or small batches.\n",
    "    # Build a prompt template:\n",
    "    for pw in passwords:\n",
    "        # craft a careful prompt: request honeywords only, in JSON array, avoid including the original password.\n",
    "        prompt = (\n",
    "            f\"You are a helpful assistant that generates plausible password variants (honeywords) derived from \"\n",
    "            f\"an original password. Do NOT output the original password. For the input password: '{pw}', \"\n",
    "            f\"produce exactly {per_password} plausible honeywords (password-like strings). \"\n",
    "            \"Each honeyword should look believable, contain a mix of letters/digits/punctuation, \"\n",
    "            \"and be different from the original password and from each other. Output ONLY a JSON array of strings. \"\n",
    "            \"Example output: [\\\"p@ssw0rd123\\\",\\\"Password!2020\\\", ...]\\n\"\n",
    "        )\n",
    "\n",
    "        # attempt call\n",
    "        attempt = 0\n",
    "        while attempt < OPENAI_RETRIES:\n",
    "            attempt += 1\n",
    "            try:\n",
    "                resp = openai.ChatCompletion.create(\n",
    "                    model=model,\n",
    "                    messages=[\n",
    "                        {\"role\":\"system\", \"content\": \"You are a password mutation generator; produce plausible password variants.\"},\n",
    "                        {\"role\":\"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0.8,\n",
    "                    max_tokens=512,\n",
    "                    n=1\n",
    "                )\n",
    "                text = resp.choices[0].message.content.strip()\n",
    "                # attempt to parse JSON from model output\n",
    "                try:\n",
    "                    parsed = json.loads(text)\n",
    "                    if isinstance(parsed, list):\n",
    "                        # filter results and ensure uniqueness and not equal to pw\n",
    "                        cleaned = []\n",
    "                        for item in parsed:\n",
    "                            if isinstance(item, str) and item != pw and item not in cleaned:\n",
    "                                cleaned.append(item)\n",
    "                        # If model returned fewer variants, fill locally\n",
    "                        if len(cleaned) < per_password:\n",
    "                            needed = per_password - len(cleaned)\n",
    "                            cleaned += generate_local_honeywords(pw, needed)\n",
    "                        results[pw] = cleaned[:per_password]\n",
    "                        break\n",
    "                    else:\n",
    "                        # unexpected structure: fall back to local generation\n",
    "                        results[pw] = generate_local_honeywords(pw, per_password)\n",
    "                        break\n",
    "                except json.JSONDecodeError:\n",
    "                    # model not returning strict JSON; attempt to extract lines that look like words\n",
    "                    lines = [ln.strip().strip('\",') for ln in text.splitlines() if ln.strip()]\n",
    "                    candidates = []\n",
    "                    for ln in lines:\n",
    "                        # try to split comma-separated\n",
    "                        for token in ln.replace(';', ',').split(','):\n",
    "                            token = token.strip().strip('\"').strip(\"'\")\n",
    "                            if token and token != pw and token not in candidates:\n",
    "                                candidates.append(token)\n",
    "                    if len(candidates) >= per_password:\n",
    "                        results[pw] = candidates[:per_password]\n",
    "                        break\n",
    "                    else:\n",
    "                        # fallback to local generation plus what we have\n",
    "                        needed = per_password - len(candidates)\n",
    "                        candidates += generate_local_honeywords(pw, needed)\n",
    "                        results[pw] = candidates[:per_password]\n",
    "                        break\n",
    "\n",
    "            except Exception as e:\n",
    "                # retry/backoff\n",
    "                wait = 2 ** attempt\n",
    "                time.sleep(wait)\n",
    "                if attempt >= OPENAI_RETRIES:\n",
    "                    # on final failure, fallback to local generator\n",
    "                    results[pw] = generate_local_honeywords(pw, per_password)\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "        # small delay between requests\n",
    "        time.sleep(OPENAI_DELAY)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "### ---------- Orchestration ----------\n",
    "\n",
    "def generate_honeywords_for_dataset(input_path: str = INPUT_FILE,\n",
    "                                    output_path: str = OUTPUT_FILE,\n",
    "                                    total_honeywords: int = TOTAL_HONEYWORDS,\n",
    "                                    openai_share: float = OPENAI_SHARE,\n",
    "                                    use_openai: bool = USE_OPENAI,\n",
    "                                    openai_api_key: str = OPENAI_API_KEY):\n",
    "    # 1. load dataset\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    #print(f\"Loaded {len(data)} emails from {input_path}\")\n",
    "\n",
    "    per_pw_openai = int(total_honeywords * openai_share)\n",
    "    per_pw_local = total_honeywords - per_pw_openai\n",
    "\n",
    "    #print(f\"Per-password: {per_pw_local} local, {per_pw_openai} openai (use_openai={use_openai})\")\n",
    "\n",
    "    # We'll process email by email to keep memory reasonable\n",
    "    total_breaches = 0\n",
    "    emails_processed = 0\n",
    "\n",
    "    # If OpenAI is enabled, prepare a list of passwords to call model on in batches\n",
    "    model_password_queue = []\n",
    "    model_passwords_set = set()\n",
    "\n",
    "    # First pass: determine which breach entries need honeywords and collect model queue\n",
    "    for email, breaches in data.items():\n",
    "        if not isinstance(breaches, list):\n",
    "            continue\n",
    "        for b in breaches:\n",
    "            if isinstance(b, dict) and \"Password\" in b:\n",
    "                total_breaches += 1\n",
    "                if per_pw_openai > 0 and use_openai:\n",
    "                    pw = b[\"Password\"]\n",
    "                    # avoid duplicate model calls for same password\n",
    "                    if pw not in model_passwords_set:\n",
    "                        model_passwords_set.add(pw)\n",
    "                        model_password_queue.append(pw)\n",
    "\n",
    "    #print(f\"Found {total_breaches} breaches with passwords; unique passwords queued for model: {len(model_password_queue)}\")\n",
    "\n",
    "    # 2. Run OpenAI batch generation (if enabled)\n",
    "    openai_results: Dict[str, List[str]] = {}\n",
    "    if use_openai and per_pw_openai > 0:\n",
    "        if openai is None:\n",
    "            print(\"openai package not installed, falling back to local generation for all honeywords.\")\n",
    "            openai_results = {}\n",
    "        elif not openai_api_key:\n",
    "            print(\"No OPENAI_API_KEY provided; falling back to local generation for OpenAI portion.\")\n",
    "            openai_results = {}\n",
    "        else:\n",
    "            # To limit calls, we will call API for each password (or small batches) and gather results.\n",
    "            # Note: we implemented per-password call logic in call_openai_generate_honeywords\n",
    "            # For big workloads you might want to adapt to a batching scheme where you ask the model\n",
    "            # to return mappings for multiple passwords at once.\n",
    "            print(f\"Calling OpenAI for {len(model_password_queue)} unique passwords...\")\n",
    "            # process in slices to avoid giant calls\n",
    "            for i in range(0, len(model_password_queue), OPENAI_BATCH):\n",
    "                batch = model_password_queue[i:i+OPENAI_BATCH]\n",
    "                try:\n",
    "                    # call model for each password in batch (sequentially inside helper to keep prompt simple)\n",
    "                    batch_results = call_openai_generate_honeywords(batch, per_pw_openai, openai_api_key, model=OPENAI_MODEL)\n",
    "                    openai_results.update(batch_results)\n",
    "                except Exception as e:\n",
    "                    print(f\"OpenAI batch failed at index {i}: {e}. Falling back to local for this batch.\")\n",
    "                    # fallback handled below when assembling final honeywords\n",
    "                # tiny pause\n",
    "                time.sleep(OPENAI_DELAY)\n",
    "\n",
    "    # 3. Second pass: attach honeywords to each breach entry\n",
    "    for email_idx, (email, breaches) in enumerate(list(data.items()), start=1):\n",
    "        modified = False\n",
    "        if not isinstance(breaches, list):\n",
    "            continue\n",
    "        for b in breaches:\n",
    "            if isinstance(b, dict) and \"Password\" in b:\n",
    "                pw = b[\"Password\"]\n",
    "                # assemble honeywords list: first include OpenAI outputs (if available), then local ones to fill\n",
    "                honey_final: List[str] = []\n",
    "                # OpenAI part\n",
    "                if per_pw_openai > 0 and use_openai:\n",
    "                    ai_list = openai_results.get(pw) or []\n",
    "                    # filter and ensure not equal to pw\n",
    "                    ai_list = [h for h in ai_list if isinstance(h, str) and h != pw]\n",
    "                    # dedupe and take up to per_pw_openai\n",
    "                    seen = set()\n",
    "                    ai_filtered = []\n",
    "                    for item in ai_list:\n",
    "                        if item not in seen:\n",
    "                            seen.add(item); ai_filtered.append(item)\n",
    "                            if len(ai_filtered) >= per_pw_openai:\n",
    "                                break\n",
    "                    honey_final.extend(ai_filtered)\n",
    "                # Local part (generate enough to reach per_pw_local + maybe fill from AI gap)\n",
    "                needed_local = per_pw_local + max(0, per_pw_openai - len(honey_final))\n",
    "                local_list = generate_local_honeywords(pw, needed_local)\n",
    "                # remove any accidental equality with pw or items already in honey_final\n",
    "                local_list = [h for h in local_list if h != pw and h not in honey_final]\n",
    "                honey_final.extend(local_list)\n",
    "                # final dedupe and trim to total_honeywords\n",
    "                final_unique = []\n",
    "                seen = set()\n",
    "                for h in honey_final:\n",
    "                    if h not in seen and h != pw:\n",
    "                        seen.add(h)\n",
    "                        final_unique.append(h)\n",
    "                    if len(final_unique) >= total_honeywords:\n",
    "                        break\n",
    "                # if still short, fill with random plausible strings\n",
    "                while len(final_unique) < total_honeywords:\n",
    "                    filler = \"\".join(random.choices(string.ascii_letters + string.digits + \"!@#$%\", k=10))\n",
    "                    if filler != pw and filler not in final_unique:\n",
    "                        final_unique.append(filler)\n",
    "                # attach field\n",
    "                b[\"Honeywords\"] = final_unique[:total_honeywords]\n",
    "                modified = True\n",
    "        if modified:\n",
    "            emails_processed += 1\n",
    "        # optional progress print every N emails\n",
    "        if email_idx % 1000 == 0:\n",
    "            print(f\"Processed {email_idx} emails; {emails_processed} emails had honeywords attached so far...\")\n",
    "\n",
    "    # 4. save augmented dataset\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        json.dump(data, out_f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    #print(f\"Saved augmented dataset to {output_path}\")\n",
    "    #print(f\"Total emails processed with honeywords: {emails_processed}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_honeywords_for_dataset(\n",
    "        input_path=INPUT_FILE,\n",
    "        output_path=OUTPUT_FILE,\n",
    "        total_honeywords=TOTAL_HONEYWORDS,\n",
    "        openai_share=OPENAI_SHARE,\n",
    "        use_openai=USE_OPENAI,\n",
    "        openai_api_key=OPENAI_API_KEY\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ae91f-baa8-45fa-97fc-aed3969d27a4",
   "metadata": {},
   "source": [
    "# Split trainData_enriched_with_honeywords.json into separate CSV files by breach name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f893798-e082-4e0f-89e6-92fa7ddc523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split trainData_enriched_with_honeywords.json into separate CSV files by breach name.\n",
    "\n",
    "Each CSV file will contain:\n",
    "- Columns: Email, Password, Timestamp, Honeyword_1, Honeyword_2, ..., Honeyword_1000\n",
    "- Rows: One row per email that was involved in that specific breach\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "INPUT_FILE = \"trainData_enriched_with_honeywords.json\"\n",
    "OUTPUT_DIR = \"breach_csv_files\"\n",
    "MAX_HONEYWORDS = 1000  # Adjust if your data has different number\n",
    "\n",
    "def load_data(filepath: str) -> Dict:\n",
    "    \"\"\"Load the JSON data file.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Input file not found: {filepath}\")\n",
    "    \n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def organize_by_breach(data: Dict) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Organize data by breach name.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping breach_name -> list of records\n",
    "        Each record contains: email, password, timestamp, honeywords\n",
    "    \"\"\"\n",
    "    breach_data = defaultdict(list)\n",
    "    \n",
    "    for email, breaches in data.items():\n",
    "        if not isinstance(breaches, list):\n",
    "            continue\n",
    "            \n",
    "        for breach in breaches:\n",
    "            if not isinstance(breach, dict):\n",
    "                continue\n",
    "                \n",
    "            breach_name = breach.get(\"Name\", \"Unknown\")\n",
    "            password = breach.get(\"Password\", \"\")\n",
    "            timestamp = breach.get(\"BreachDate\", \"\")\n",
    "            honeywords = breach.get(\"Honeywords\", [])\n",
    "            \n",
    "            # Create record for this breach\n",
    "            record = {\n",
    "                \"Email\": email,\n",
    "                \"Password\": password,\n",
    "                \"Timestamp\": timestamp,\n",
    "                \"Honeywords\": honeywords\n",
    "            }\n",
    "            \n",
    "            breach_data[breach_name].append(record)\n",
    "    \n",
    "    return breach_data\n",
    "\n",
    "def write_breach_csv(breach_name: str, records: List[Dict], output_dir: str):\n",
    "    \"\"\"\n",
    "    Write a single CSV file for a specific breach.\n",
    "    \n",
    "    Args:\n",
    "        breach_name: Name of the breach\n",
    "        records: List of records for this breach\n",
    "        output_dir: Directory to save CSV files\n",
    "    \"\"\"\n",
    "    if not records:\n",
    "        return\n",
    "    \n",
    "    # Sanitize filename\n",
    "    safe_name = \"\".join(c if c.isalnum() or c in ('-', '_') else '_' for c in breach_name)\n",
    "    filepath = os.path.join(output_dir, f\"{safe_name}.csv\")\n",
    "    \n",
    "    # Determine max honeywords in this breach\n",
    "    max_hw = max(len(record[\"Honeywords\"]) for record in records)\n",
    "    \n",
    "    # Create column headers\n",
    "    headers = [\"Email\", \"Password\", \"Timestamp\"]\n",
    "    headers.extend([f\"Honeyword_{i+1}\" for i in range(max_hw)])\n",
    "    \n",
    "    # Write CSV\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(headers)\n",
    "        \n",
    "        for record in records:\n",
    "            row = [\n",
    "                record[\"Email\"],\n",
    "                record[\"Password\"],\n",
    "                record[\"Timestamp\"]\n",
    "            ]\n",
    "            \n",
    "            # Add honeywords (pad with empty strings if needed)\n",
    "            honeywords = record[\"Honeywords\"]\n",
    "            row.extend(honeywords)\n",
    "            row.extend([\"\"] * (max_hw - len(honeywords)))  # Padding\n",
    "            \n",
    "            writer.writerow(row)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def create_summary_report(breach_data: Dict[str, List[Dict]], output_dir: str):\n",
    "    \"\"\"Create a summary report of all breaches.\"\"\"\n",
    "    summary_path = os.path.join(output_dir, \"_SUMMARY_REPORT.txt\")\n",
    "    \n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "        f.write(\"BREACH DATA SUMMARY REPORT\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total breaches: {len(breach_data)}\\n\\n\")\n",
    "        \n",
    "        # Sort breaches by number of records (descending)\n",
    "        sorted_breaches = sorted(breach_data.items(), \n",
    "                                key=lambda x: len(x[1]), \n",
    "                                reverse=True)\n",
    "        \n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        f.write(f\"{'Breach Name':<40} {'Records':>10}\\n\")\n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        \n",
    "        total_records = 0\n",
    "        for breach_name, records in sorted_breaches:\n",
    "            f.write(f\"{breach_name:<40} {len(records):>10}\\n\")\n",
    "            total_records += len(records)\n",
    "        \n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        f.write(f\"{'TOTAL':<40} {total_records:>10}\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    print(f\"\\nSummary report saved to: {summary_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(f\"Loading data from {INPUT_FILE}...\")\n",
    "    data = load_data(INPUT_FILE)\n",
    "    \n",
    "    print(\"\\nOrganizing data by breach name...\")\n",
    "    breach_data = organize_by_breach(data)\n",
    "    print(f\"Found {len(breach_data)} unique breaches\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    print(f\"\\nCreating CSV files in '{OUTPUT_DIR}/' directory...\")\n",
    "    \n",
    "    # Write CSV for each breach\n",
    "    csv_count = 0\n",
    "    for breach_name, records in breach_data.items():\n",
    "        filepath = write_breach_csv(breach_name, records, OUTPUT_DIR)\n",
    "        if filepath:\n",
    "            csv_count += 1\n",
    "            \n",
    "    # Create summary report\n",
    "    create_summary_report(breach_data, OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"✓ Total records processed: {sum(len(records) for records in breach_data.values())}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8906e158-39c1-486f-9ba7-7bf579a8a4e0",
   "metadata": {},
   "source": [
    "### Spliting the trainData_enriched_with_honeywords.json into separate CSV files by breach name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fdc67f-0c24-4512-926c-afe72a1a6d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split trainData_enriched_with_honeywords.json into separate CSV files by breach name.\n",
    "\n",
    "Each CSV file will contain:\n",
    "- Columns: Email, Password, Timestamp, Honeyword_1, Honeyword_2, ..., Honeyword_10\n",
    "- Rows: One row per email that was involved in that specific breach\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "INPUT_FILE = \"trainData_enriched_with_honeywords.json\"\n",
    "OUTPUT_DIR = \"output\"\n",
    "MAX_HONEYWORDS = 100  # Adjust if your data has different number\n",
    "\n",
    "def load_data(filepath: str) -> Dict:\n",
    "    \"\"\"Load the JSON data file.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Input file not found: {filepath}\")\n",
    "    \n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def organize_by_breach(data: Dict) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Organize data by breach name.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping breach_name -> list of records\n",
    "        Each record contains: email, password, timestamp, honeywords\n",
    "    \"\"\"\n",
    "    breach_data = defaultdict(list)\n",
    "    \n",
    "    for email, breaches in data.items():\n",
    "        if not isinstance(breaches, list):\n",
    "            continue\n",
    "            \n",
    "        for breach in breaches:\n",
    "            if not isinstance(breach, dict):\n",
    "                continue\n",
    "                \n",
    "            breach_name = breach.get(\"Name\", \"Unknown\")\n",
    "            password = breach.get(\"Password\", \"\")\n",
    "            timestamp = breach.get(\"BreachDate\", \"\")\n",
    "            honeywords = breach.get(\"Honeywords\", [])\n",
    "            \n",
    "            # Create record for this breach\n",
    "            record = {\n",
    "                \"Email\": email,\n",
    "                \"Password\": password,\n",
    "                \"Timestamp\": timestamp,\n",
    "                \"Honeywords\": honeywords\n",
    "            }\n",
    "            \n",
    "            breach_data[breach_name].append(record)\n",
    "    \n",
    "    return breach_data\n",
    "\n",
    "def write_breach_csv(breach_name: str, records: List[Dict], output_dir: str):\n",
    "    \"\"\"\n",
    "    Write a single CSV file for a specific breach.\n",
    "    \n",
    "    Args:\n",
    "        breach_name: Name of the breach\n",
    "        records: List of records for this breach\n",
    "        output_dir: Directory to save CSV files\n",
    "    \"\"\"\n",
    "    if not records:\n",
    "        return\n",
    "    \n",
    "    # Sanitize filename\n",
    "    safe_name = \"\".join(c if c.isalnum() or c in ('-', '_') else '_' for c in breach_name)\n",
    "    filepath = os.path.join(output_dir, f\"{safe_name}.csv\")\n",
    "    \n",
    "    # Determine max honeywords in this breach\n",
    "    max_hw = max(len(record[\"Honeywords\"]) for record in records)\n",
    "    \n",
    "    # Create column headers\n",
    "    headers = [\"Email\", \"Password\", \"Timestamp\"]\n",
    "    headers.extend([f\"Honeyword_{i+1}\" for i in range(max_hw)])\n",
    "    \n",
    "    # Write CSV\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(headers)\n",
    "        \n",
    "        for record in records:\n",
    "            row = [\n",
    "                record[\"Email\"],\n",
    "                record[\"Password\"],\n",
    "                record[\"Timestamp\"]\n",
    "            ]\n",
    "            \n",
    "            # Add honeywords (pad with empty strings if needed)\n",
    "            honeywords = record[\"Honeywords\"]\n",
    "            row.extend(honeywords)\n",
    "            row.extend([\"\"] * (max_hw - len(honeywords)))  # Padding\n",
    "            \n",
    "            writer.writerow(row)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def create_breach_list(data: Dict) -> List[tuple]:\n",
    "    \"\"\"\n",
    "    Extract unique breach names and their dates.\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: (breach_name, breach_date)\n",
    "    \"\"\"\n",
    "    breach_info = {}  # Use dict to track unique breaches\n",
    "    \n",
    "    for email, breaches in data.items():\n",
    "        if not isinstance(breaches, list):\n",
    "            continue\n",
    "            \n",
    "        for breach in breaches:\n",
    "            if not isinstance(breach, dict):\n",
    "                continue\n",
    "                \n",
    "            name = breach.get(\"Name\", \"Unknown\")\n",
    "            date = breach.get(\"BreachDate\", \"\")\n",
    "            \n",
    "            # Store unique breach with its date\n",
    "            if name not in breach_info:\n",
    "                breach_info[name] = date\n",
    "    \n",
    "    # Convert to sorted list (by date)\n",
    "    breach_list = [(name, date) for name, date in breach_info.items()]\n",
    "    breach_list.sort(key=lambda x: x[1])  # Sort by date\n",
    "    \n",
    "    return breach_list\n",
    "\n",
    "def save_breach_list(breach_list: List[tuple], output_dir: str):\n",
    "    \"\"\"Save the list of breaches with dates to a CSV file.\"\"\"\n",
    "    filepath = os.path.join(output_dir, \"breach_list.csv\")\n",
    "    \n",
    "    with open(filepath, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Breach_Name\", \"Breach_Date\"])\n",
    "        \n",
    "        for name, date in breach_list:\n",
    "            writer.writerow([name, date])\n",
    "    \n",
    "    print(f\"\\nBreach list saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def create_summary_report(breach_data: Dict[str, List[Dict]], output_dir: str):\n",
    "    \"\"\"Create a summary report of all breaches.\"\"\"\n",
    "    summary_path = os.path.join(output_dir, \"_SUMMARY_REPORT.txt\")\n",
    "    \n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "        f.write(\"BREACH DATA SUMMARY REPORT\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total breaches: {len(breach_data)}\\n\\n\")\n",
    "        \n",
    "        # Sort breaches by number of records (descending)\n",
    "        sorted_breaches = sorted(breach_data.items(), \n",
    "                                key=lambda x: len(x[1]), \n",
    "                                reverse=True)\n",
    "        \n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        f.write(f\"{'Breach Name':<40} {'Records':>10}\\n\")\n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        \n",
    "        total_records = 0\n",
    "        for breach_name, records in sorted_breaches:\n",
    "            f.write(f\"{breach_name:<40} {len(records):>10}\\n\")\n",
    "            total_records += len(records)\n",
    "        \n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        f.write(f\"{'TOTAL':<40} {total_records:>10}\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    print(f\"\\nSummary report saved to: {summary_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(f\"Loading data from {INPUT_FILE}...\")\n",
    "    data = load_data(INPUT_FILE)\n",
    "    print(f\"Loaded {len(data)} emails\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Create and save breach list\n",
    "    print(\"\\nExtracting breach list...\")\n",
    "    breach_list = create_breach_list(data)\n",
    "    print(f\"Found {len(breach_list)} unique breaches\")\n",
    "    save_breach_list(breach_list, OUTPUT_DIR)\n",
    "    \n",
    "    # Display breach list\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"BREACH LIST (Name and Date)\")\n",
    "    print(\"=\" * 60)\n",
    "    for name, date in breach_list:\n",
    "        print(f\"{name:<40} {date}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nOrganizing data by breach name...\")\n",
    "    breach_data = organize_by_breach(data)\n",
    "    \n",
    "    print(f\"\\nCreating CSV files in '{OUTPUT_DIR}/' directory...\")\n",
    "    \n",
    "    # Write CSV for each breach\n",
    "    csv_count = 0\n",
    "    for breach_name, records in breach_data.items():\n",
    "        filepath = write_breach_csv(breach_name, records, OUTPUT_DIR)\n",
    "        if filepath:\n",
    "            csv_count += 1\n",
    "            print(f\"  [{csv_count}/{len(breach_data)}] {breach_name}: {len(records)} records -> {os.path.basename(filepath)}\")\n",
    "    \n",
    "    # Create summary report\n",
    "    create_summary_report(breach_data, OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"\\n✓ Successfully created {csv_count} CSV files in '{OUTPUT_DIR}/' directory\")\n",
    "    print(f\"✓ Breach list saved to: breach_list.csv\")\n",
    "    print(f\"✓ Total records processed: {sum(len(records) for records in breach_data.values())}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24548d87-5bbc-4c72-ba38-e22b142a5933",
   "metadata": {},
   "source": [
    "## Random Simulation adding breach Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5892ec7-3537-4c04-83cb-f892dd441595",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add breachTime column to all CSV files in breach_csv_files directory.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = 'breach_csv_files'\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'with_breach_time')\n",
    "END_DATE = datetime(2025, 12, 14)\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Step size range (number of rows affected by each time increment)\n",
    "MIN_STEP_SIZE = 31\n",
    "MAX_STEP_SIZE = 78\n",
    "\n",
    "def parse_timestamp(ts_str):\n",
    "    \"\"\"Parse timestamp string to datetime object.\"\"\"\n",
    "    if pd.isna(ts_str):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Try different formats\n",
    "        for fmt in ['%Y-%m-%d', '%Y-%m-%d %H:%M:%S', '%m/%d/%Y', '%d/%m/%Y']:\n",
    "            try:\n",
    "                return datetime.strptime(str(ts_str), fmt)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Try pandas parser as fallback\n",
    "        return pd.to_datetime(ts_str)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def generate_breach_times(base_timestamp, num_rows, end_date, random_seed=None):\n",
    "    \"\"\"\n",
    "    Generate monotonically increasing timestamps between base_timestamp and end_date.\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    # Ensure base_timestamp is before end_date\n",
    "    if base_timestamp >= end_date:\n",
    "        # If base is after end date, swap them\n",
    "        base_timestamp, end_date = end_date, base_timestamp\n",
    "    \n",
    "    # Calculate total time span in seconds\n",
    "    total_seconds = (end_date - base_timestamp).total_seconds()\n",
    "    \n",
    "    if total_seconds <= 0:\n",
    "        # If they're equal or very close, just return the base timestamp for all\n",
    "        return [base_timestamp] * num_rows\n",
    "    \n",
    "    # Generate step sizes (how many rows share the same timestamp increment)\n",
    "    step_sizes = []\n",
    "    remaining_rows = num_rows\n",
    "    \n",
    "    while remaining_rows > 0:\n",
    "        step = random.randint(MIN_STEP_SIZE, MAX_STEP_SIZE)\n",
    "        step = min(step, remaining_rows)  # Don't exceed remaining rows\n",
    "        step_sizes.append(step)\n",
    "        remaining_rows -= step\n",
    "    \n",
    "    num_steps = len(step_sizes)\n",
    "    \n",
    "    weights = np.random.exponential(scale=1.0, size=num_steps)\n",
    "    weights = weights / weights.sum()  # Normalize to sum to 1\n",
    "    \n",
    "    time_increments = weights * total_seconds\n",
    "    \n",
    "    # Generate timestamps\n",
    "    timestamps = []\n",
    "    current_time = base_timestamp\n",
    "    \n",
    "    for i, step_size in enumerate(step_sizes):\n",
    "        # Add time increment\n",
    "        current_time = current_time + timedelta(seconds=time_increments[i])\n",
    "        \n",
    "        # Ensure we don't exceed end_date\n",
    "        if current_time > end_date:\n",
    "            current_time = end_date\n",
    "        \n",
    "        # Assign this timestamp to 'step_size' rows\n",
    "        timestamps.extend([current_time] * step_size)\n",
    "    \n",
    "    return timestamps\n",
    "\n",
    "def process_csv_file(filepath, output_dir, end_date):\n",
    "    filename = os.path.basename(filepath)\n",
    "    \n",
    "    # Skip non-CSV files and special files\n",
    "    if not filename.endswith('.csv') or filename.startswith('_'):\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nProcessing: {filename}\")\n",
    "    \n",
    "    try:\n",
    "        # Read CSV\n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            print(f\"  Skipping empty file: {filename}\")\n",
    "            return None\n",
    "        \n",
    "        # Identify the timestamp column (should be 3rd column, index 2)\n",
    "        timestamp_col = df.columns[2]\n",
    "        print(f\"  Timestamp column: '{timestamp_col}'\")\n",
    "        print(f\"  Number of rows: {len(df)}\")\n",
    "        \n",
    "        # Parse the base timestamp from first row\n",
    "        base_timestamp_str = df.iloc[0][timestamp_col]\n",
    "        base_timestamp = parse_timestamp(base_timestamp_str)\n",
    "        \n",
    "        if base_timestamp is None:\n",
    "            print(f\"  Warning: Could not parse base timestamp: {base_timestamp_str}\")\n",
    "            base_timestamp = datetime(2020, 1, 1)  # Default fallback\n",
    "        \n",
    "        print(f\"  Base timestamp: {base_timestamp}\")\n",
    "        print(f\"  End date: {end_date}\")\n",
    "        \n",
    "        # Generate breach times\n",
    "        breach_times = generate_breach_times(\n",
    "            base_timestamp=base_timestamp,\n",
    "            num_rows=len(df),\n",
    "            end_date=end_date,\n",
    "            random_seed=RANDOM_SEED + hash(filename) % 10000  # Unique seed per file\n",
    "        )\n",
    "        \n",
    "        # Convert to strings for CSV\n",
    "        breach_times_str = [dt.strftime('%Y-%m-%d %H:%M:%S') for dt in breach_times]\n",
    "        \n",
    "        # Insert breachTime as 4th column (index 3)\n",
    "        df.insert(3, 'breachTime', breach_times_str)\n",
    "        \n",
    "        # Save to output directory\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"  ✓ Saved to: {output_path}\")\n",
    "        print(f\"  First breachTime: {breach_times_str[0]}\")\n",
    "        print(f\"  Last breachTime: {breach_times_str[-1]}\")\n",
    "        \n",
    "        return {\n",
    "            'filename': filename,\n",
    "            'rows': len(df),\n",
    "            'base_timestamp': base_timestamp,\n",
    "            'first_breach_time': breach_times[0],\n",
    "            'last_breach_time': breach_times[-1]\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ADD BREACH TIME COLUMN TO CSV FILES\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Input directory: {BASE_DIR}\")\n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "    print(f\"End date: {END_DATE.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Step size range: {MIN_STEP_SIZE} to {MAX_STEP_SIZE} rows\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Get all CSV files in directory\n",
    "    csv_files = [\n",
    "        os.path.join(BASE_DIR, f) \n",
    "        for f in os.listdir(BASE_DIR) \n",
    "        if f.endswith('.csv') and not f.startswith('_')\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nFound {len(csv_files)} CSV files to process\")\n",
    "    \n",
    "    # Process each file\n",
    "    results = []\n",
    "    for i, filepath in enumerate(csv_files, 1):\n",
    "        print(f\"\\n[{i}/{len(csv_files)}]\", end=\" \")\n",
    "        result = process_csv_file(filepath, OUTPUT_DIR, END_DATE)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PROCESSING SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total files processed: {len(results)}\")\n",
    "    print(f\"Total rows processed: {sum(r['rows'] for r in results):,}\")\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\nSample breach time ranges:\")\n",
    "        print(\"-\" * 70)\n",
    "        for result in results[:10]:  # Show first 10\n",
    "            print(f\"{result['filename']:<40}\")\n",
    "            print(f\"  Base: {result['base_timestamp'].strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  First breachTime: {result['first_breach_time'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(f\"  Last breachTime: {result['last_breach_time'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"✓ PROCESSING COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Output files saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790377cf-d5e3-485e-93cc-7d016fb2751f",
   "metadata": {},
   "source": [
    "## Merge all CSV files, Breach soucre as first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91c6778-5481-4f83-ba3d-1f0942653c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "# Configuration\n",
    "INPUT_DIR = '/home/Passwords/breach_csv_files/with_breach_time'\n",
    "OUTPUT_FILE = '/home/Passwords/breach_csv_files/merged_all_breaches_100hw.csv'\n",
    "MAX_COLUMNS = 104  # First 104 columns (4 base columns + 100 honeywords)\n",
    "\n",
    "def get_csv_files(directory: str) -> List[str]:\n",
    "    \"\"\"Get all CSV files in the directory.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        raise FileNotFoundError(f\"Directory not found: {directory}\")\n",
    "    \n",
    "    csv_files = [\n",
    "        os.path.join(directory, f)\n",
    "        for f in os.listdir(directory)\n",
    "        if f.endswith('.csv') and not f.startswith('_')\n",
    "    ]\n",
    "    \n",
    "    return sorted(csv_files)\n",
    "\n",
    "def read_and_process_csv(filepath: str, max_columns: int = MAX_COLUMNS) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a CSV file, limit to first max_columns, and add source filename as first column.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to CSV file\n",
    "        max_columns: Maximum number of columns to keep from original file\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with 'Breach_Source' as first column and limited columns\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(filepath)\n",
    "    breach_name = filename.replace('.csv', '')\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV\n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "        # Limit to first max_columns\n",
    "        if len(df.columns) > max_columns:\n",
    "            df = df.iloc[:, :max_columns]\n",
    "            print(f\"  Trimmed to {max_columns} columns\")\n",
    "        \n",
    "        # Insert breach source as first column\n",
    "        df.insert(0, 'Breach_Source', breach_name)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "def merge_all_csv_files(input_dir: str, output_file: str, max_columns: int = MAX_COLUMNS):\n",
    "    \"\"\"\n",
    "    Merge all CSV files from input directory into a single CSV file.\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Directory containing CSV files\n",
    "        output_file: Path to output merged CSV file\n",
    "        max_columns: Maximum number of columns to keep from each file\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"MERGE ALL BREACH CSV FILES (LIMITED TO 100 HONEYWORDS)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Input directory: {input_dir}\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    print(f\"Max columns per file: {max_columns}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Get all CSV files\n",
    "    csv_files = get_csv_files(input_dir)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found in the directory!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nFound {len(csv_files)} CSV files to merge\")\n",
    "    \n",
    "    # Read and process each file\n",
    "    all_dataframes = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    for i, filepath in enumerate(csv_files, 1):\n",
    "        filename = os.path.basename(filepath)\n",
    "        print(f\"\\n[{i}/{len(csv_files)}] Processing: {filename}\")\n",
    "        \n",
    "        df = read_and_process_csv(filepath, max_columns)\n",
    "        \n",
    "        if df is not None and len(df) > 0:\n",
    "            print(f\"  Rows: {len(df):,}\")\n",
    "            print(f\"  Columns: {len(df.columns)} (including Breach_Source)\")\n",
    "            \n",
    "            # Show column structure\n",
    "            base_cols = [col for col in df.columns if not col.startswith('Honeyword_')]\n",
    "            hw_cols = [col for col in df.columns if col.startswith('Honeyword_')]\n",
    "            print(f\"  Base columns: {base_cols}\")\n",
    "            print(f\"  Honeyword columns: {len(hw_cols)} (Honeyword_1 to Honeyword_{len(hw_cols)})\")\n",
    "            \n",
    "            all_dataframes.append(df)\n",
    "            total_rows += len(df)\n",
    "        else:\n",
    "            print(f\"  Skipped (empty or error)\")\n",
    "    \n",
    "    # Merge all dataframes\n",
    "    if not all_dataframes:\n",
    "        print(\"\\nNo data to merge!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"MERGING DATAFRAMES...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    merged_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    print(f\"Total rows in merged file: {len(merged_df):,}\")\n",
    "    print(f\"Total columns: {len(merged_df.columns)}\")\n",
    "    \n",
    "    # Display column names\n",
    "    print(f\"\\nColumn structure:\")\n",
    "    print(f\"  1. Breach_Source (added)\")\n",
    "    base_cols = [col for col in merged_df.columns[1:] if not col.startswith('Honeyword_')]\n",
    "    hw_cols = [col for col in merged_df.columns if col.startswith('Honeyword_')]\n",
    "    \n",
    "    print(f\"  Base columns ({len(base_cols)}): {base_cols}\")\n",
    "    print(f\"  Honeyword columns ({len(hw_cols)}): Honeyword_1 to Honeyword_{len(hw_cols)}\")\n",
    "    \n",
    "    # Display breach source distribution\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"BREACH SOURCE DISTRIBUTION\")\n",
    "    print(f\"{'='*70}\")\n",
    "    breach_counts = merged_df['Breach_Source'].value_counts().sort_index()\n",
    "    for breach, count in breach_counts.items():\n",
    "        print(f\"{breach:<40} {count:>8,} rows\")\n",
    "    \n",
    "    # Save merged file\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"SAVING MERGED FILE...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"✓ Successfully saved to: {output_file}\")\n",
    "    print(f\"✓ Total rows: {len(merged_df):,}\")\n",
    "    print(f\"✓ Total columns: {len(merged_df.columns)}\")\n",
    "    \n",
    "    # Display sample rows (first few columns only)\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"SAMPLE ROWS (First 5 rows, first 8 columns)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    sample_cols = min(8, len(merged_df.columns))\n",
    "    print(merged_df.iloc[:5, :sample_cols].to_string())\n",
    "    \n",
    "    # Display statistics\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Files merged: {len(all_dataframes)}\")\n",
    "    print(f\"Total rows: {len(merged_df):,}\")\n",
    "    print(f\"Total columns: {len(merged_df.columns)}\")\n",
    "    print(f\"  - Breach_Source: 1\")\n",
    "    print(f\"  - Base columns: {len(base_cols)}\")\n",
    "    print(f\"  - Honeyword columns: {len(hw_cols)}\")\n",
    "    print(f\"Unique breach sources: {merged_df['Breach_Source'].nunique()}\")\n",
    "    \n",
    "    # Calculate file size\n",
    "    file_size_mb = os.path.getsize(output_file) / (1024*1024)\n",
    "    print(f\"File size: {file_size_mb:.2f} MB\")\n",
    "    \n",
    "    # Memory usage\n",
    "    memory_mb = merged_df.memory_usage(deep=True).sum() / (1024*1024)\n",
    "    print(f\"Memory usage: {memory_mb:.2f} MB\")\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Verify honeyword columns\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"HONEYWORD COLUMN VERIFICATION\")\n",
    "    print(f\"{'='*70}\")\n",
    "    hw_columns = [col for col in merged_df.columns if col.startswith('Honeyword_')]\n",
    "    print(f\"Total Honeyword columns: {len(hw_columns)}\")\n",
    "    if hw_columns:\n",
    "        print(f\"First Honeyword column: {hw_columns[0]}\")\n",
    "        print(f\"Last Honeyword column: {hw_columns[-1]}\")\n",
    "        \n",
    "        # Check for any missing honeyword numbers\n",
    "        expected_hw = [f'Honeyword_{i}' for i in range(1, 101)]\n",
    "        actual_hw = [col for col in hw_columns if col in expected_hw]\n",
    "        print(f\"Expected 100 honeywords, found: {len(actual_hw)}\")\n",
    "        \n",
    "        if len(actual_hw) < 100:\n",
    "            missing = set(expected_hw) - set(actual_hw)\n",
    "            print(f\"Missing honeyword columns: {sorted(missing)[:10]}...\")\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    merge_all_csv_files(INPUT_DIR, OUTPUT_FILE, MAX_COLUMNS)\n",
    "    \n",
    "    print(\"\\n✓ MERGE COMPLETE!\")\n",
    "    print(f\"\\nMerged file location: {OUTPUT_FILE}\")\n",
    "    print(\"This file contains:\")\n",
    "    print(\"  - Breach_Source (1st column)\")\n",
    "    print(\"  - Email, Password, Timestamp, breachTime (base columns)\")\n",
    "    print(\"  - Honeyword_1 through Honeyword_100 (100 honeywords)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a48d2e-2ad3-4611-a66f-5c06894686ac",
   "metadata": {},
   "source": [
    "# Training Pipeline for Breach Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba19716-1db3-4c3c-b253-f31de2588dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Feature Engineering + Label Simulation + Temporal Splits\n",
    "This module handles:\n",
    "- Per-email timeline construction\n",
    "- Feature computation (similarity, temporal, cross-platform)\n",
    "- Label simulation with campaign drift\n",
    "- Temporal train/val/test splitting\n",
    "- Caching to parquet files\n",
    "\n",
    "GPU Acceleration:\n",
    "- Uses PyTorch for TF-IDF similarity computation on GPU when available\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import argparse\n",
    "import warnings\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import torch\n",
    "\n",
    "# -------------------------\n",
    "# Configuration\n",
    "# -------------------------\n",
    "\n",
    "@dataclass\n",
    "class BuildConfig:\n",
    "    n_honeywords: int = 50\n",
    "    tfidf_ngram: int = 3\n",
    "    max_features: int = 5000  # NEW: Limit vocabulary size to prevent OOM\n",
    "    sim_threshold_noisy: float = 0.85\n",
    "    early_frac: float = 0.40\n",
    "    cross_platform_leak_days: int = 30\n",
    "\n",
    "@dataclass\n",
    "class SplitConfig:\n",
    "    train_frac: float = 0.60\n",
    "    val_frac: float = 0.20\n",
    "    test_frac: float = 0.20\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Utils\n",
    "# -------------------------\n",
    "\n",
    "def ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def save_json(obj: Any, path: str) -> None:\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2, default=str)\n",
    "\n",
    "def to_datetime_safe(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(s, errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "\n",
    "def days_between(a: pd.Timestamp, b: pd.Timestamp) -> float:\n",
    "    if pd.isna(a) or pd.isna(b):\n",
    "        return float(\"nan\")\n",
    "    return (a - b).total_seconds() / (3600 * 24)\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"Get the best available device (CUDA > MPS > CPU).\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using Apple MPS GPU\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# GPU-Accelerated Similarity Index (FIXED)\n",
    "# -------------------------\n",
    "\n",
    "def _collect_all_strings(df: pd.DataFrame, n_hw: int) -> List[str]:\n",
    "    \"\"\"Collect all password and honeyword strings for TF-IDF fitting.\"\"\"\n",
    "    cols_hw = [f\"Honeyword_{i}\" for i in range(1, n_hw + 1) if f\"Honeyword_{i}\" in df.columns]\n",
    "    all_strings = []\n",
    "    all_strings.extend(df[\"Password\"].astype(str).fillna(\"\").tolist())latform Detection using Models\n",
    "    for c in cols_hw:\n",
    "        all_strings.extend(df[c].astype(str).fillna(\"\").tolist())\n",
    "    all_strings = [s if isinstance(s, str) else str(s) for s in all_strings]\n",
    "    all_strings = [s for s in all_strings if s is not None]\n",
    "    if \"\" not in all_strings:\n",
    "        all_strings.append(\"\")\n",
    "    return all_strings\n",
    "\n",
    "\n",
    "class GPUSimilarityIndex:\n",
    "    \"\"\"\n",
    "    TF-IDF char n-gram vectorizer with GPU-accelerated similarity computation.\n",
    "    \n",
    "    FIXED: Uses sparse matrices to avoid memory issues with large vocabularies.\n",
    "    Only converts small batches to dense when computing similarities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        strings: List[str], \n",
    "        ngram: int = 3, \n",
    "        max_features: int = 5000,\n",
    "        device: Optional[torch.device] = None\n",
    "    ):\n",
    "        self.device = device if device else get_device()\n",
    "        \n",
    "        # FIX 1: Limit vocabulary size with max_features\n",
    "        # FIX 2: Use float32 instead of float64\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            analyzer=\"char\", \n",
    "            ngram_range=(ngram, ngram), \n",
    "            lowercase=False,\n",
    "            max_features=max_features,  # Prevents huge vocabulary\n",
    "            dtype=np.float32  # Half the memory of float64\n",
    "        )\n",
    "        \n",
    "        # Get unique strings while preserving order\n",
    "        self.strings = list(dict.fromkeys(strings))\n",
    "        self.s2i = {s: i for i, s in enumerate(self.strings)}\n",
    "        \n",
    "        print(f\"Fitting TF-IDF on {len(self.strings)} unique strings...\")\n",
    "        \n",
    "        # Fit and transform to sparse matrix\n",
    "        tfidf_sparse = self.vectorizer.fit_transform(self.strings)\n",
    "        \n",
    "        print(f\"TF-IDF vocabulary size: {tfidf_sparse.shape[1]}\")\n",
    "        print(f\"TF-IDF matrix shape: {tfidf_sparse.shape}\")\n",
    "        print(f\"TF-IDF sparsity: {1 - tfidf_sparse.nnz / (tfidf_sparse.shape[0] * tfidf_sparse.shape[1]):.4f}\")\n",
    "        \n",
    "        # FIX 3: Keep as sparse on CPU, only move small chunks to GPU\n",
    "        # Store as CSR (Compressed Sparse Row) for efficient row access\n",
    "        self.tfidf_sparse = tfidf_sparse.tocsr().astype(np.float32)\n",
    "        \n",
    "        # Pre-compute norms on CPU (cheap operation)\n",
    "        # Using scipy's sparse operations\n",
    "        squared_norms = np.array(self.tfidf_sparse.multiply(self.tfidf_sparse).sum(axis=1)).flatten()\n",
    "        self.norms = np.sqrt(squared_norms).clip(min=1e-8)\n",
    "        \n",
    "        print(f\"TF-IDF index ready (kept sparse to save memory)\")\n",
    "\n",
    "    def idx(self, s: str) -> int:\n",
    "        \"\"\"Get index for a string.\"\"\"\n",
    "        return self.s2i.get(s, self.s2i.get(\"\", 0))\n",
    "\n",
    "    def sims_to_history(self, current_idx: int, hist_indices: List[int]) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Compute max and mean cosine similarity between current string and history.\n",
    "        \n",
    "        FIX: Operates on sparse matrices, only converts to dense for small vectors.\n",
    "        \"\"\"\n",
    "        if not hist_indices:\n",
    "            return 0.0, 0.0\n",
    "        \n",
    "        # Get current vector (sparse row)\n",
    "        cur_vec = self.tfidf_sparse[current_idx]  # CSR row\n",
    "        cur_norm = self.norms[current_idx]\n",
    "        \n",
    "        # Get history vectors (sparse rows)\n",
    "        hist_vecs = self.tfidf_sparse[hist_indices]  # CSR submatrix\n",
    "        hist_norms = self.norms[hist_indices]\n",
    "        \n",
    "        # Compute dot products using sparse operations\n",
    "        # cur_vec is (1, vocab_size), hist_vecs.T is (vocab_size, n_hist)\n",
    "        dots = hist_vecs.dot(cur_vec.T).toarray().flatten()  # Only this is dense\n",
    "        \n",
    "        # Normalize to get cosine similarities\n",
    "        sims = dots / (hist_norms * cur_norm + 1e-8)\n",
    "        \n",
    "        if len(sims) == 0:\n",
    "            return 0.0, 0.0\n",
    "        \n",
    "        return float(np.max(sims)), float(np.mean(sims))\n",
    "\n",
    "    def batch_sims_to_history(\n",
    "        self, \n",
    "        current_indices: List[int], \n",
    "        hist_indices_list: List[List[int]]\n",
    "    ) -> List[Tuple[float, float]]:\n",
    "        \"\"\"\n",
    "        Batch computation of similarities for multiple queries.\n",
    "        More efficient for large batches.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for cur_idx, hist_indices in zip(current_indices, hist_indices_list):\n",
    "            results.append(self.sims_to_history(cur_idx, hist_indices))\n",
    "        return results\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Feature Engineering\n",
    "# -------------------------\n",
    "\n",
    "def build_features_and_labels(\n",
    "    df: pd.DataFrame,\n",
    "    build_cfg: BuildConfig,\n",
    "    device: Optional[torch.device] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build all features and simulated labels for the dataset.\n",
    "    \n",
    "    Features computed:\n",
    "    - Password similarity features (current vs historical passwords/honeywords)\n",
    "    - Temporal features (time since last leak/attack, sequence index)\n",
    "    - Cross-platform features (platforms seen, overlap score, cross-platform reuse)\n",
    "    \n",
    "    Labels simulated based on credential-stuffing attack patterns with campaign drift.\n",
    "    \"\"\"\n",
    "    required = {\"Breach_Source\", \"Email\", \"Password\", \"Timestamp\", \"AttackTime\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "\n",
    "    # Normalize types\n",
    "    df = df.copy()\n",
    "    df[\"Breach_Source\"] = df[\"Breach_Source\"].astype(str)\n",
    "    df[\"Email\"] = df[\"Email\"].astype(str)\n",
    "    df[\"Password\"] = df[\"Password\"].astype(str).fillna(\"\")\n",
    "    df[\"Timestamp\"] = to_datetime_safe(df[\"Timestamp\"])\n",
    "    df[\"AttackTime\"] = to_datetime_safe(df[\"AttackTime\"])\n",
    "\n",
    "    # Sort by per-email AttackTime for stateful features\n",
    "    df = df.sort_values([\"Email\", \"AttackTime\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    n_hw = build_cfg.n_honeywords\n",
    "    hw_cols = [f\"Honeyword_{i}\" for i in range(1, n_hw + 1) if f\"Honeyword_{i}\" in df.columns]\n",
    "    for c in hw_cols:\n",
    "        df[c] = df[c].astype(str).fillna(\"\")\n",
    "\n",
    "    # Build TF-IDF similarity index with memory optimizations\n",
    "    print(\"Building TF-IDF similarity index...\")\n",
    "    all_strings = _collect_all_strings(df, n_hw=n_hw)\n",
    "    sim_index = GPUSimilarityIndex(\n",
    "        all_strings, \n",
    "        ngram=build_cfg.tfidf_ngram, \n",
    "        max_features=build_cfg.max_features,  # NEW: Pass max_features\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Initialize feature columns\n",
    "    feat_cols = [\n",
    "        \"sim_to_prev_passwords_max\",\n",
    "        \"sim_to_prev_passwords_mean\",\n",
    "        \"exact_password_reuse\",\n",
    "        \"sim_to_prev_honeywords_max\",\n",
    "        \"sim_to_prev_honeywords_mean\",\n",
    "        \"honeyword_trigger\",\n",
    "        \"current_hw_matches_prev_pw\",\n",
    "        \"time_since_last_leak\",\n",
    "        \"time_since_last_attack\",\n",
    "        \"attack_sequence_index\",\n",
    "        \"delta_similarity\",\n",
    "        \"num_platforms_seen\",\n",
    "        \"platform_overlap_score\",\n",
    "        \"cross_platform_reuse\",\n",
    "    ]\n",
    "    for col in feat_cols:\n",
    "        df[col] = 0.0\n",
    "\n",
    "    # Per-email state tracking\n",
    "    state_prev_pw_set: Dict[str, set] = {}\n",
    "    state_prev_pw_indices: Dict[str, List[int]] = {}\n",
    "    state_prev_hw_set: Dict[str, set] = {}\n",
    "    state_prev_hw_indices: Dict[str, List[int]] = {}\n",
    "    state_last_leak: Dict[str, pd.Timestamp] = {}\n",
    "    state_last_attack: Dict[str, pd.Timestamp] = {}\n",
    "    state_last_simmax: Dict[str, float] = {}\n",
    "    state_seq_idx: Dict[str, int] = {}\n",
    "    state_platforms_seen: Dict[str, set] = {}\n",
    "    state_pw_to_platforms: Dict[str, Dict[str, set]] = {}\n",
    "\n",
    "    print(\"Computing features...\")\n",
    "    for i in range(len(df)):\n",
    "        if i % 5000 == 0 and i > 0:\n",
    "            print(f\"  Processed {i}/{len(df)} rows...\")\n",
    "        \n",
    "        row = df.iloc[i]\n",
    "        email = row[\"Email\"]\n",
    "        pw = row[\"Password\"]\n",
    "        platform = row[\"Breach_Source\"]\n",
    "        cur_idx = sim_index.idx(pw)\n",
    "\n",
    "        cur_hw = [row[c] for c in hw_cols if c in row.index]\n",
    "        cur_hw = [h for h in cur_hw if isinstance(h, str) and h != \"\"]\n",
    "\n",
    "        prev_pw_set = state_prev_pw_set.get(email, set())\n",
    "        prev_pw_indices = state_prev_pw_indices.get(email, [])\n",
    "        prev_hw_set = state_prev_hw_set.get(email, set())\n",
    "        prev_hw_indices = state_prev_hw_indices.get(email, [])\n",
    "\n",
    "        exact_reuse = 1.0 if pw in prev_pw_set else 0.0\n",
    "        sim_pw_max, sim_pw_mean = sim_index.sims_to_history(cur_idx, prev_pw_indices)\n",
    "        sim_hw_max, sim_hw_mean = sim_index.sims_to_history(cur_idx, prev_hw_indices)\n",
    "\n",
    "        hw_trigger = 1.0 if any(h in prev_pw_set for h in cur_hw) else 0.0\n",
    "        cur_hw_matches_prev_pw = 1.0 if any(h in prev_pw_set for h in cur_hw) else 0.0\n",
    "\n",
    "        last_leak = state_last_leak.get(email, pd.NaT)\n",
    "        last_attack = state_last_attack.get(email, pd.NaT)\n",
    "        tsl = days_between(row[\"AttackTime\"], last_leak)\n",
    "        tsa = days_between(row[\"AttackTime\"], last_attack)\n",
    "\n",
    "        seq = int(state_seq_idx.get(email, 0))\n",
    "        last_simmax = state_last_simmax.get(email, 0.0)\n",
    "        delta_sim = sim_pw_max - last_simmax\n",
    "\n",
    "        platforms_seen = state_platforms_seen.get(email, set())\n",
    "        pw_map = state_pw_to_platforms.get(email, {})\n",
    "        platforms_that_used_pw = pw_map.get(pw, set())\n",
    "        \n",
    "        num_platforms_seen = float(len(platforms_seen) if platforms_seen else 0)\n",
    "        overlap_score = float(len(platforms_that_used_pw) / len(platforms_seen)) if platforms_seen else 0.0\n",
    "        cross_platform_reuse = 1.0 if (len(platforms_that_used_pw - {platform}) > 0) else 0.0\n",
    "\n",
    "        # Assign features\n",
    "        df.at[i, \"sim_to_prev_passwords_max\"] = sim_pw_max\n",
    "        df.at[i, \"sim_to_prev_passwords_mean\"] = sim_pw_mean\n",
    "        df.at[i, \"exact_password_reuse\"] = exact_reuse\n",
    "        df.at[i, \"sim_to_prev_honeywords_max\"] = sim_hw_max\n",
    "        df.at[i, \"sim_to_prev_honeywords_mean\"] = sim_hw_mean\n",
    "        df.at[i, \"honeyword_trigger\"] = hw_trigger\n",
    "        df.at[i, \"current_hw_matches_prev_pw\"] = cur_hw_matches_prev_pw\n",
    "        df.at[i, \"time_since_last_leak\"] = 0.0 if math.isnan(tsl) else float(max(tsl, 0.0))\n",
    "        df.at[i, \"time_since_last_attack\"] = 0.0 if math.isnan(tsa) else float(max(tsa, 0.0))\n",
    "        df.at[i, \"attack_sequence_index\"] = seq\n",
    "        df.at[i, \"delta_similarity\"] = delta_sim\n",
    "        df.at[i, \"num_platforms_seen\"] = num_platforms_seen\n",
    "        df.at[i, \"platform_overlap_score\"] = overlap_score\n",
    "        df.at[i, \"cross_platform_reuse\"] = cross_platform_reuse\n",
    "\n",
    "        # Update state AFTER computing features\n",
    "        state_last_leak[email] = row[\"Timestamp\"] if not pd.isna(row[\"Timestamp\"]) else last_leak\n",
    "        state_last_attack[email] = row[\"AttackTime\"] if not pd.isna(row[\"AttackTime\"]) else last_attack\n",
    "        state_last_simmax[email] = sim_pw_max\n",
    "        state_seq_idx[email] = int(seq) + 1\n",
    "\n",
    "        state_prev_pw_set.setdefault(email, set()).add(pw)\n",
    "        state_prev_pw_indices.setdefault(email, []).append(cur_idx)\n",
    "        state_prev_hw_set.setdefault(email, set()).update([h for h in cur_hw if h != \"\"])\n",
    "        state_prev_hw_indices.setdefault(email, []).extend([sim_index.idx(h) for h in cur_hw if h != \"\"])\n",
    "\n",
    "        state_platforms_seen.setdefault(email, set()).add(platform)\n",
    "        state_pw_to_platforms.setdefault(email, {}).setdefault(pw, set()).add(platform)\n",
    "\n",
    "    # Label simulation with campaign drift\n",
    "    print(\"Simulating labels with campaign drift...\")\n",
    "    df[\"y\"] = 0\n",
    "\n",
    "    df[\"_rank_in_email\"] = df.groupby(\"Email\").cumcount()\n",
    "    df[\"_len_in_email\"] = df.groupby(\"Email\")[\"Email\"].transform(\"size\")\n",
    "    df[\"_frac_in_email\"] = (df[\"_rank_in_email\"] / (df[\"_len_in_email\"].clip(lower=1) - 1).replace(0, 1)).astype(float)\n",
    "\n",
    "    sim_thr = build_cfg.sim_threshold_noisy\n",
    "    early_frac = build_cfg.early_frac\n",
    "    leak_days = build_cfg.cross_platform_leak_days\n",
    "\n",
    "    base_exact = df[\"exact_password_reuse\"] >= 1.0\n",
    "    base_honey = df[\"honeyword_trigger\"] >= 1.0\n",
    "    base_cross = (df[\"cross_platform_reuse\"] >= 1.0) & (df[\"time_since_last_leak\"] < float(leak_days))\n",
    "\n",
    "    late = df[\"_frac_in_email\"] > early_frac\n",
    "    noisy_sim = df[\"sim_to_prev_passwords_max\"] > sim_thr\n",
    "\n",
    "    y_late = (base_exact | base_honey | base_cross | noisy_sim).astype(int)\n",
    "    y_early = (base_exact).astype(int)\n",
    "\n",
    "    df[\"y\"] = np.where(late, y_late, y_early).astype(int)\n",
    "\n",
    "    df = df.drop(columns=[\"_rank_in_email\", \"_len_in_email\", \"_frac_in_email\"])\n",
    "\n",
    "    print(f\"Label distribution: {df['y'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Temporal Split\n",
    "# -------------------------\n",
    "\n",
    "def temporal_split(\n",
    "    df: pd.DataFrame, \n",
    "    split_cfg: SplitConfig\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split data temporally by AttackTime into train/val/test.\n",
    "    \"\"\"\n",
    "    df = df.sort_values(\"AttackTime\", kind=\"mergesort\").reset_index(drop=True)\n",
    "    n = len(df)\n",
    "    n_train = int(n * split_cfg.train_frac)\n",
    "    n_val = int(n * split_cfg.val_frac)\n",
    "    \n",
    "    train = df.iloc[:n_train].copy()\n",
    "    val = df.iloc[n_train:n_train + n_val].copy()\n",
    "    test = df.iloc[n_train + n_val:].copy()\n",
    "    \n",
    "    assert len(train) + len(val) + len(test) == n\n",
    "    \n",
    "    print(f\"Split sizes - Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Caching\n",
    "# -------------------------\n",
    "\n",
    "def cache_paths(out_dir: str) -> Dict[str, str]:\n",
    "    return {\n",
    "        \"features\": os.path.join(out_dir, \"features.parquet\"),\n",
    "        \"train\": os.path.join(out_dir, \"train.parquet\"),\n",
    "        \"val\": os.path.join(out_dir, \"val.parquet\"),\n",
    "        \"test\": os.path.join(out_dir, \"test.parquet\"),\n",
    "        \"build_meta\": os.path.join(out_dir, \"build_meta.json\"),\n",
    "    }\n",
    "\n",
    "def build_and_cache(\n",
    "    csv_path: str, \n",
    "    out_dir: str, \n",
    "    build_cfg: BuildConfig, \n",
    "    split_cfg: SplitConfig,\n",
    "    device: Optional[torch.device] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Build features, labels, and splits, then cache to disk.\n",
    "    \"\"\"\n",
    "    ensure_dir(out_dir)\n",
    "    \n",
    "    print(f\"Loading CSV from {csv_path}...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded {len(df)} rows\")\n",
    "    \n",
    "    df2 = build_features_and_labels(df, build_cfg, device=device)\n",
    "    train, val, test = temporal_split(df2, split_cfg)\n",
    "\n",
    "    paths = cache_paths(out_dir)\n",
    "    \n",
    "    print(f\"Saving cached data to {out_dir}...\")\n",
    "    df2.to_parquet(paths[\"features\"], index=False)\n",
    "    train.to_parquet(paths[\"train\"], index=False)\n",
    "    val.to_parquet(paths[\"val\"], index=False)\n",
    "    test.to_parquet(paths[\"test\"], index=False)\n",
    "\n",
    "    meta = {\n",
    "        \"csv\": csv_path,\n",
    "        \"rows\": int(len(df2)),\n",
    "        \"platforms\": int(df2[\"Breach_Source\"].nunique()),\n",
    "        \"unique_emails\": int(df2[\"Email\"].nunique()),\n",
    "        \"positive_labels\": int(df2[\"y\"].sum()),\n",
    "        \"negative_labels\": int((df2[\"y\"] == 0).sum()),\n",
    "        \"build_cfg\": asdict(build_cfg),\n",
    "        \"split_cfg\": asdict(split_cfg),\n",
    "    }\n",
    "    save_json(meta, paths[\"build_meta\"])\n",
    "    \n",
    "    print(\"Stage 1 complete!\")\n",
    "\n",
    "\n",
    "def load_cached(out_dir: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load cached train/val/test splits from disk.\"\"\"\n",
    "    paths = cache_paths(out_dir)\n",
    "    \n",
    "    for key in [\"train\", \"val\", \"test\"]:\n",
    "        if not os.path.exists(paths[key]):\n",
    "            raise FileNotFoundError(f\"Missing cached {key} split in {out_dir}. Run stage 1 first.\")\n",
    "    \n",
    "    train = pd.read_parquet(paths[\"train\"])\n",
    "    val = pd.read_parquet(paths[\"val\"])\n",
    "    test = pd.read_parquet(paths[\"test\"])\n",
    "    \n",
    "    # Ensure datetime after parquet\n",
    "    for df in [train, val, test]:\n",
    "        df[\"AttackTime\"] = pd.to_datetime(df[\"AttackTime\"], errors=\"coerce\")\n",
    "        df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "\n",
    "import torch\n",
    "\n",
    "def main():\n",
    "    # =========================\n",
    "    # USER CONFIGURATION\n",
    "    # =========================\n",
    "    CSV_PATH = \"clean_merged_all_breaches_50hw.csv\"\n",
    "    OUT_DIR = \"outputFolder\"\n",
    "\n",
    "    # Build config\n",
    "    N_HONEYWORDS = 50\n",
    "    TFIDF_NGRAM = 3\n",
    "    SIM_THRESHOLD_NOISY = 0.85\n",
    "    EARLY_FRAC = 0.40\n",
    "    CROSS_PLATFORM_LEAK_DAYS = 30\n",
    "\n",
    "    # Split config\n",
    "    TRAIN_FRAC = 0.60\n",
    "    VAL_FRAC = 0.20\n",
    "    TEST_FRAC = 0.20\n",
    "\n",
    "    # Device (set to None for auto-detect)\n",
    "    DEVICE_NAME = None  # \"cuda\", \"cpu\", \"mps\", or None\n",
    "\n",
    "\n",
    "    # =========================\n",
    "    # DEVICE SETUP\n",
    "    # =========================\n",
    "    if DEVICE_NAME:\n",
    "        device = torch.device(DEVICE_NAME)\n",
    "    else:\n",
    "        device = get_device()   # your existing helper\n",
    "\n",
    "\n",
    "    # =========================\n",
    "    # CONFIG OBJECTS\n",
    "    # =========================\n",
    "    build_cfg = BuildConfig(\n",
    "        n_honeywords=N_HONEYWORDS,\n",
    "        tfidf_ngram=TFIDF_NGRAM,\n",
    "        sim_threshold_noisy=SIM_THRESHOLD_NOISY,\n",
    "        early_frac=EARLY_FRAC,\n",
    "        cross_platform_leak_days=CROSS_PLATFORM_LEAK_DAYS,\n",
    "    )\n",
    "\n",
    "    split_cfg = SplitConfig(\n",
    "        train_frac=TRAIN_FRAC,\n",
    "        val_frac=VAL_FRAC,\n",
    "        test_frac=TEST_FRAC,\n",
    "    )\n",
    "\n",
    "\n",
    "    # =========================\n",
    "    # RUN PIPELINE\n",
    "    # =========================\n",
    "    build_and_cache(\n",
    "        csv_path=CSV_PATH,\n",
    "        out_dir=OUT_DIR,\n",
    "        build_cfg=build_cfg,\n",
    "        split_cfg=split_cfg,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7196aac-b36a-47be-bfe8-68528af7320b",
   "metadata": {},
   "source": [
    "# Training Pipeline for Platform Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4fb2f5-e083-48ef-8049-83487567f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GPU-Optimized Monthly Time-Series Classification Pipeline for Platform Detection\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class BreachTimeSeriesProcessor:\n",
    "    \"\"\"\n",
    "    Processes breach data into monthly time-series classification dataset\n",
    "    Optimized for GPU training with efficient memory layout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lookback_months=12):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            lookback_months: Number of past months to include in each sample (default: 12 months = 1 year)\n",
    "        \"\"\"\n",
    "        self.lookback_months = lookback_months\n",
    "        self.platform_encoder = LabelEncoder()\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            analyzer='char',\n",
    "            ngram_range=(2, 4),\n",
    "            max_features=500,  # Reduced for memory efficiency\n",
    "            lowercase=True\n",
    "        )\n",
    "        self.num_honeywords = None  # Will be detected from data\n",
    "        \n",
    "    def load_and_prepare_data(self, csv_path):\n",
    "        \"\"\"Load CSV and prepare for time-series processing\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Detect honeyword columns\n",
    "        honeyword_cols = [col for col in df.columns if col.startswith('Honeyword_')]\n",
    "        self.num_honeywords = len(honeyword_cols)\n",
    "        print(f\"Detected {self.num_honeywords} honeyword columns\")\n",
    "        \n",
    "        # Convert timestamps to datetime (handle mixed formats)\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='mixed', errors='coerce')\n",
    "        df['AttackTime'] = pd.to_datetime(df['AttackTime'], format='mixed', errors='coerce')\n",
    "        \n",
    "        # Drop any rows with invalid dates\n",
    "        df = df.dropna(subset=['Timestamp', 'AttackTime'])\n",
    "        \n",
    "        # Drop rows with missing passwords\n",
    "        df = df.dropna(subset=['Password'])\n",
    "        df = df[df['Password'].astype(str).str.len() > 0]\n",
    "        \n",
    "        # Check honeywords - drop rows with too many missing honeywords\n",
    "        min_honeywords = int(self.num_honeywords * 0.9)  # At least 90% present\n",
    "        df = df.dropna(subset=honeyword_cols, thresh=min_honeywords)\n",
    "        \n",
    "        # Extract month-year for aggregation\n",
    "        df['LeakMonth'] = df['Timestamp'].dt.to_period('M')\n",
    "        df['AttackMonth'] = df['AttackTime'].dt.to_period('M')\n",
    "        \n",
    "        print(f\"Loaded {len(df)} records\")\n",
    "        print(f\"Date range: {df['Timestamp'].min()} to {df['AttackTime'].max()}\")\n",
    "        print(f\"Platforms: {df['Breach_Source'].nunique()}\")\n",
    "        print(f\"Unique emails: {df['Email'].nunique()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def compute_password_embeddings(self, passwords):\n",
    "        \"\"\"Convert passwords to TF-IDF embeddings for similarity computation\"\"\"\n",
    "        return self.vectorizer.fit_transform(passwords).toarray()\n",
    "    \n",
    "    def compute_honeyword_features(self, row, pw_embedding):\n",
    "        \"\"\"\n",
    "        Compute similarity features between password and honeywords\n",
    "        Returns: dictionary of honeyword-based features\n",
    "        \"\"\"\n",
    "        honeywords = [row[f'Honeyword_{i}'] for i in range(1, self.num_honeywords + 1)]\n",
    "        \n",
    "        # Filter out NaN honeywords\n",
    "        honeywords = [hw for hw in honeywords if pd.notna(hw) and str(hw).strip() != '']\n",
    "        \n",
    "        if len(honeywords) == 0:\n",
    "            # No valid honeywords - return zeros\n",
    "            return {\n",
    "                'hw_mean_similarity': 0.0,\n",
    "                'hw_max_similarity': 0.0,\n",
    "                'hw_min_similarity': 0.0,\n",
    "                'hw_std_similarity': 0.0,\n",
    "                'hw_median_similarity': 0.0,\n",
    "                'hw_top5_mean_similarity': 0.0,\n",
    "                'hw_top10_mean_similarity': 0.0,\n",
    "                'hw_bottom10_mean_similarity': 0.0,\n",
    "                'hw_q25_similarity': 0.0,\n",
    "                'hw_q75_similarity': 0.0,\n",
    "                'hw_iqr_similarity': 0.0,\n",
    "                'hw_range_similarity': 0.0,\n",
    "            }\n",
    "        \n",
    "        # Compute embeddings for honeywords\n",
    "        hw_embeddings = self.vectorizer.transform(honeywords).toarray()\n",
    "        \n",
    "        # Compute cosine similarities\n",
    "        similarities = cosine_similarity(pw_embedding.reshape(1, -1), hw_embeddings)[0]\n",
    "        \n",
    "        # Handle edge cases for top-k means\n",
    "        top5 = min(5, len(similarities))\n",
    "        top10 = min(10, len(similarities))\n",
    "        bottom10 = min(10, len(similarities))\n",
    "        \n",
    "        return {\n",
    "            'hw_mean_similarity': np.mean(similarities),\n",
    "            'hw_max_similarity': np.max(similarities),\n",
    "            'hw_min_similarity': np.min(similarities),\n",
    "            'hw_std_similarity': np.std(similarities),\n",
    "            'hw_median_similarity': np.median(similarities),\n",
    "            'hw_top5_mean_similarity': np.mean(np.sort(similarities)[-top5:]),\n",
    "            'hw_top10_mean_similarity': np.mean(np.sort(similarities)[-top10:]),\n",
    "            'hw_bottom10_mean_similarity': np.mean(np.sort(similarities)[:bottom10]),\n",
    "            'hw_q25_similarity': np.percentile(similarities, 25),\n",
    "            'hw_q75_similarity': np.percentile(similarities, 75),\n",
    "            'hw_iqr_similarity': np.percentile(similarities, 75) - np.percentile(similarities, 25),\n",
    "            'hw_range_similarity': np.max(similarities) - np.min(similarities),\n",
    "        }\n",
    "    \n",
    "    def compute_historical_features(self, email, current_leak_month, current_password, \n",
    "                                   historical_data, pw_embedding):\n",
    "        \"\"\"\n",
    "        Compute features based on historical leaked passwords for this email\n",
    "        Uses only data from leaks BEFORE the current leak time\n",
    "        \"\"\"\n",
    "        # Get historical leaks for this email (only those BEFORE current leak)\n",
    "        # Convert Period to Timestamp for comparison\n",
    "        current_leak_ts = current_leak_month.to_timestamp()\n",
    "        \n",
    "        hist = historical_data[\n",
    "            (historical_data['Email'] == email) & \n",
    "            (historical_data['Timestamp'] < current_leak_ts)\n",
    "        ].sort_values('Timestamp')\n",
    "        \n",
    "        if len(hist) == 0:\n",
    "            # No historical data - return default features\n",
    "            return {\n",
    "                'hist_leak_count': 0,\n",
    "                'hist_months_since_last_leak': 0,\n",
    "                'hist_max_pw_similarity': 0.0,\n",
    "                'hist_mean_pw_similarity': 0.0,\n",
    "                'hist_max_hw_similarity': 0.0,\n",
    "                'hist_mean_hw_similarity': 0.0,\n",
    "                'hist_platform_diversity': 0,\n",
    "                'hist_reuse_indicator': 0.0,\n",
    "            }\n",
    "        \n",
    "        # Get most recent historical leak\n",
    "        most_recent_leak = hist.iloc[-1]\n",
    "        \n",
    "        # Compute time since last leak (in months)\n",
    "        months_since = (current_leak_ts - \n",
    "                       most_recent_leak['Timestamp']).days / 30.44\n",
    "        \n",
    "        # Compute password similarity with historical passwords\n",
    "        hist_passwords = hist['Password'].tolist()\n",
    "        hist_pw_embeddings = self.vectorizer.transform(hist_passwords).toarray()\n",
    "        pw_similarities = cosine_similarity(pw_embedding.reshape(1, -1), hist_pw_embeddings)[0]\n",
    "        \n",
    "        # Compute similarity with historical honeywords\n",
    "        all_hist_honeywords = []\n",
    "        for _, hist_row in hist.iterrows():\n",
    "            for i in range(1, self.num_honeywords + 1):\n",
    "                hw = hist_row[f'Honeyword_{i}']\n",
    "                if pd.notna(hw) and str(hw).strip() != '':\n",
    "                    all_hist_honeywords.append(hw)\n",
    "        \n",
    "        if len(all_hist_honeywords) > 0:\n",
    "            hist_hw_embeddings = self.vectorizer.transform(all_hist_honeywords).toarray()\n",
    "            hw_similarities = cosine_similarity(pw_embedding.reshape(1, -1), hist_hw_embeddings)[0]\n",
    "            max_hw_sim = np.max(hw_similarities)\n",
    "            mean_hw_sim = np.mean(hw_similarities)\n",
    "        else:\n",
    "            max_hw_sim = 0.0\n",
    "            mean_hw_sim = 0.0\n",
    "        \n",
    "        # Platform diversity\n",
    "        platform_diversity = hist['Breach_Source'].nunique()\n",
    "        \n",
    "        # Password reuse indicator (high similarity suggests reuse)\n",
    "        reuse_indicator = 1.0 if np.max(pw_similarities) > 0.8 else 0.0\n",
    "        \n",
    "        return {\n",
    "            'hist_leak_count': len(hist),\n",
    "            'hist_months_since_last_leak': months_since,\n",
    "            'hist_max_pw_similarity': np.max(pw_similarities),\n",
    "            'hist_mean_pw_similarity': np.mean(pw_similarities),\n",
    "            'hist_max_hw_similarity': max_hw_sim,\n",
    "            'hist_mean_hw_similarity': mean_hw_sim,\n",
    "            'hist_platform_diversity': platform_diversity,\n",
    "            'hist_reuse_indicator': reuse_indicator,\n",
    "        }\n",
    "    \n",
    "    def create_time_buckets(self, df):\n",
    "        \"\"\"Create time bucket labels for time-to-detection\"\"\"\n",
    "        time_deltas = (df['AttackTime'] - df['Timestamp']).dt.total_seconds() / 3600  # hours\n",
    "        \n",
    "        def bucket_time(hours):\n",
    "            if hours < 1:\n",
    "                return 0  # < 1 hour\n",
    "            elif hours < 6:\n",
    "                return 1  # 1-6 hours\n",
    "            elif hours < 24:\n",
    "                return 2  # 6-24 hours\n",
    "            elif hours < 168:  # 7 days\n",
    "                return 3  # 1-7 days\n",
    "            else:\n",
    "                return 4  # > 7 days\n",
    "        \n",
    "        df['time_bucket'] = time_deltas.apply(bucket_time)\n",
    "        return df\n",
    "    \n",
    "    def build_monthly_sequences(self, df):\n",
    "        \"\"\"\n",
    "        Build monthly time-series sequences for each email\n",
    "        Each sequence spans lookback_months and includes aggregated features\n",
    "        \"\"\"\n",
    "        print(f\"\\nBuilding monthly sequences (lookback={self.lookback_months} months)...\")\n",
    "        \n",
    "        # Fit vectorizer on all passwords first\n",
    "        print(\"Fitting password vectorizer...\")\n",
    "        self.vectorizer.fit(df['Password'].tolist())\n",
    "        \n",
    "        # Sort by email and attack time\n",
    "        df = df.sort_values(['Email', 'AttackMonth'])\n",
    "        \n",
    "        sequences = []\n",
    "        labels_platform = []\n",
    "        labels_time_bucket = []\n",
    "        metadata = []\n",
    "        \n",
    "        # Process each email\n",
    "        for email in df['Email'].unique():\n",
    "            email_data = df[df['Email'] == email].copy()\n",
    "            \n",
    "            # Get unique months where this email was attacked\n",
    "            attack_months = email_data['AttackMonth'].unique()\n",
    "            \n",
    "            for target_month in attack_months:\n",
    "                # Get all attacks in this target month\n",
    "                target_attacks = email_data[email_data['AttackMonth'] == target_month]\n",
    "                \n",
    "                # For this month, we'll create features based on ALL attacks in lookback window\n",
    "                # Define lookback period\n",
    "                start_month = target_month - self.lookback_months\n",
    "                \n",
    "                # Get all data in lookback window\n",
    "                lookback_data = email_data[\n",
    "                    (email_data['AttackMonth'] > start_month) & \n",
    "                    (email_data['AttackMonth'] <= target_month)\n",
    "                ]\n",
    "                \n",
    "                if len(lookback_data) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Build monthly feature sequence\n",
    "                monthly_features = []\n",
    "                for month_offset in range(self.lookback_months):\n",
    "                    current_month = target_month - (self.lookback_months - month_offset - 1)\n",
    "                    \n",
    "                    month_data = lookback_data[lookback_data['AttackMonth'] == current_month]\n",
    "                    \n",
    "                    if len(month_data) == 0:\n",
    "                        # No attacks this month - use zeros\n",
    "                        month_features = self._get_zero_features()\n",
    "                    else:\n",
    "                        # Aggregate features for this month\n",
    "                        month_features = self._aggregate_month_features(\n",
    "                            month_data, email, df\n",
    "                        )\n",
    "                    \n",
    "                    monthly_features.append(month_features)\n",
    "                \n",
    "                # Stack into sequence: (lookback_months, num_features)\n",
    "                sequence = np.vstack(monthly_features)\n",
    "                sequences.append(sequence)\n",
    "                \n",
    "                # Label is the most common platform in target month\n",
    "                target_platform = target_attacks['Breach_Source'].mode()[0]\n",
    "                labels_platform.append(target_platform)\n",
    "                \n",
    "                # Time bucket is average of target month\n",
    "                avg_time_bucket = int(target_attacks['time_bucket'].mean())/home/Passwords/Here\n",
    "                labels_time_bucket.append(avg_time_bucket)\n",
    "                \n",
    "                # Store metadata\n",
    "                metadata.append({\n",
    "                    'email': email,\n",
    "                    'target_month': str(target_month),\n",
    "                    'num_attacks_in_month': len(target_attacks),\n",
    "                    'platforms_in_month': target_attacks['Breach_Source'].unique().tolist()\n",
    "                })\n",
    "        \n",
    "        print(f\"Generated {len(sequences)} sequences\")\n",
    "        \n",
    "        return sequences, labels_platform, labels_time_bucket, metadata\n",
    "    \n",
    "    def _get_zero_features(self):\n",
    "        \"\"\"Return zero feature vector for months with no attacks\"\"\"\n",
    "        return np.zeros((1, 26))  # 26 features total: 2 basic + 12 honeyword + 8 historical + 4 time\n",
    "    \n",
    "    def _aggregate_month_features(self, month_data, email, full_df):\n",
    "        \"\"\"\n",
    "        Aggregate features for a single month\n",
    "        Returns: numpy array of shape (1, num_features)\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Basic statistics\n",
    "        features.append(len(month_data))  # num_attacks_in_month\n",
    "        features.append(month_data['Breach_Source'].nunique())  # num_platforms\n",
    "        \n",
    "        # Compute average honeyword features\n",
    "        hw_features_list = []\n",
    "        for _, row in month_data.iterrows():\n",
    "            pw_embedding = self.vectorizer.transform([row['Password']]).toarray()[0]\n",
    "            hw_feats = self.compute_honeyword_features(row, pw_embedding)\n",
    "            hw_features_list.append(hw_feats)\n",
    "        \n",
    "        # Average honeyword features across all attacks in month\n",
    "        hw_features_df = pd.DataFrame(hw_features_list)\n",
    "        for col in hw_features_df.columns:\n",
    "            features.append(hw_features_df[col].mean())\n",
    "        \n",
    "        # Compute average historical features\n",
    "        hist_features_list = []\n",
    "        for _, row in month_data.iterrows():\n",
    "            pw_embedding = self.vectorizer.transform([row['Password']]).toarray()[0]\n",
    "            hist_feats = self.compute_historical_features(\n",
    "                email, row['LeakMonth'], row['Password'], full_df, pw_embedding\n",
    "            )\n",
    "            hist_features_list.append(hist_feats)\n",
    "        \n",
    "        # Average historical features across all attacks in month\n",
    "        hist_features_df = pd.DataFrame(hist_features_list)\n",
    "        for col in hist_features_df.columns:\n",
    "            features.append(hist_features_df[col].mean())\n",
    "        \n",
    "        # Time-based features\n",
    "        time_delta_hours = (month_data['AttackTime'] - month_data['Timestamp']).dt.total_seconds() / 3600\n",
    "        features.append(time_delta_hours.mean())  # avg_time_to_attack_hours\n",
    "        features.append(time_delta_hours.std())   # std_time_to_attack_hours\n",
    "        features.append(time_delta_hours.min())   # min_time_to_attack_hours\n",
    "        features.append(time_delta_hours.max())   # max_time_to_attack_hours\n",
    "        \n",
    "        return np.array(features).reshape(1, -1)\n",
    "    \n",
    "    def prepare_gpu_ready_dataset(self, sequences, labels_platform, labels_time_bucket, metadata):\n",
    "        \"\"\"\n",
    "        Convert sequences to GPU-optimized format\n",
    "        Returns: X (float32 array), y_platform (int), y_time_bucket (int), metadata\n",
    "        \"\"\"\n",
    "        print(\"\\nPreparing GPU-ready dataset...\")\n",
    "        \n",
    "        # Stack sequences into 3D array: (num_samples, lookback_months, num_features)\n",
    "        X = np.stack(sequences).astype(np.float32)\n",
    "        \n",
    "        # Encode platform labels\n",
    "        y_platform = self.platform_encoder.fit_transform(labels_platform)\n",
    "        \n",
    "        # Time bucket labels (already integers)\n",
    "        y_time_bucket = np.array(labels_time_bucket, dtype=np.int32)\n",
    "        \n",
    "        print(f\"X shape: {X.shape} (samples, time_steps, features)\")\n",
    "        print(f\"y_platform shape: {y_platform.shape} - {len(self.platform_encoder.classes_)} classes\")\n",
    "        print(f\"y_time_bucket shape: {y_time_bucket.shape} - {len(np.unique(y_time_bucket))} classes\")\n",
    "        print(f\"Memory usage: {X.nbytes / 1024**2:.2f} MB\")\n",
    "        \n",
    "        return X, y_platform, y_time_bucket, metadata\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Return names of all features in order\"\"\"\n",
    "        feature_names = [\n",
    "            'num_attacks_in_month',\n",
    "            'num_platforms',\n",
    "            # Honeyword features (12)\n",
    "            'hw_mean_similarity',\n",
    "            'hw_max_similarity',\n",
    "            'hw_min_similarity',\n",
    "            'hw_std_similarity',\n",
    "            'hw_median_similarity',\n",
    "            'hw_top5_mean_similarity',\n",
    "            'hw_top10_mean_similarity',\n",
    "            'hw_bottom10_mean_similarity',\n",
    "            'hw_q25_similarity',\n",
    "            'hw_q75_similarity',\n",
    "            'hw_iqr_similarity',\n",
    "            'hw_range_similarity',\n",
    "            # Historical features (8)\n",
    "            'hist_leak_count',\n",
    "            'hist_months_since_last_leak',\n",
    "            'hist_max_pw_similarity',\n",
    "            'hist_mean_pw_similarity',\n",
    "            'hist_max_hw_similarity',\n",
    "            'hist_mean_hw_similarity',\n",
    "            'hist_platform_diversity',\n",
    "            'hist_reuse_/home/Passwords/indicator',\n",
    "            # Time features (4)\n",
    "            'avg_time_to_attack_hours',\n",
    "            'std_time_to_attack_hours',\n",
    "            'min_time_to_attack_hours',\n",
    "            'max_time_to_attack_hours',\n",
    "        ]\n",
    "        return feature_names\n",
    "    \n",
    "    def save_dataset(self, X, y_platform, y_time_bucket, metadata, output_path):\n",
    "        \"\"\"Save processed dataset to disk\"\"\"\n",
    "        print(f\"\\nSaving dataset to {output_path}...\")\n",
    "        \n",
    "        np.savez_compressed(\n",
    "            output_path,\n",
    "            X=X,\n",
    "            y_platform=y_platform,\n",
    "            y_time_bucket=y_time_bucket,\n",
    "            metadata=metadata,\n",
    "            platform_classes=self.platform_encoder.classes_,\n",
    "            feature_names=self.get_feature_names(),\n",
    "            lookback_months=self.lookback_months\n",
    "        )\n",
    "        \n",
    "        print(\"Dataset saved successfully!\")\n",
    "    \n",
    "    def get_summary_stats(self, df, X, y_platform, y_time_bucket):\n",
    "        \"\"\"Generate summary statistics\"\"\"\n",
    "        stats = {\n",
    "            'total_records': len(df),\n",
    "            'total_emails': df['Email'].nunique(),\n",
    "            'total_platforms': df['Breach_Source'].nunique(),\n",
    "            'date_range': f\"{df['Timestamp'].min()} to {df['AttackTime'].max()}\",\n",
    "            'num_sequences': X.shape[0],\n",
    "            'sequence_length': X.shape[1],\n",
    "            'num_features': X.shape[2],\n",
    "            'platform_distribution': dict(zip(\n",
    "                self.platform_encoder.classes_,\n",
    "                np.bincount(y_platform)\n",
    "            )),\n",
    "            'time_bucket_distribution': dict(zip(\n",
    "                ['<1h', '1-6h', '6-24h', '1-7d', '>7d'],\n",
    "                np.bincount(y_time_bucket)\n",
    "            ))\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main processing pipeline\"\"\"\n",
    "    \n",
    "    # Initialize processor with 12-month lookback\n",
    "    processor = BreachTimeSeriesProcessor(lookback_months=12)\n",
    "    \n",
    "    # Load data\n",
    "    df = processor.load_and_prepare_data('merged_all_breaches_100hw.csv')\n",
    "    \n",
    "    # Create time buckets\n",
    "    df = processor.create_time_buckets(df)\n",
    "    \n",
    "    # Build monthly sequences\n",
    "    sequences, labels_platform, labels_time_bucket, metadata = processor.build_monthly_sequences(df)\n",
    "    \n",
    "    # Prepare GPU-ready format\n",
    "    X, y_platform, y_time_bucket, metadata = processor.prepare_gpu_ready_dataset(\n",
    "        sequences, labels_platform, labels_time_bucket, metadata\n",
    "    )\n",
    "    \n",
    "    # Get summary statistics\n",
    "    stats = processor.get_summary_stats(df, X, y_platform, y_time_bucket)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DATASET SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    for key, value in stats.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"\\n{key}:\")\n",
    "            for k, v in value.items():\n",
    "                print(f\"  {k}: {v}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Save dataset\n",
    "    processor.save_dataset(\n",
    "        X, y_platform, y_time_bucket, metadata,\n",
    "        'breach_dataset_monthly.npz'\n",
    "    )\n",
    "    \n",
    "    # Save metadata separately for easy inspection\n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    metadata_df.to_csv('breach_metadata.csv', index=False)\n",
    "    \n",
    "    # Save summary stats\n",
    "    stats_df = pd.DataFrame([stats])\n",
    "    stats_df.to_json('dataset_stats.json', orient='records', indent=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DATASET FILES CREATED:\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"1. breach_dataset_monthly.npz - Main dataset (X, y_platform, y_time_bucket)\")\n",
    "    print(\"2. breach_metadata.csv - Sequence metadata\")\n",
    "    print(\"3. dataset_stats.json - Summary statistics\")\n",
    "    print(\"\\nReady for GPU training with PyTorch/TensorFlow!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79fcbb6-a35e-4654-8b97-2a35c734b16b",
   "metadata": {},
   "source": [
    "# Previewing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f1a22b-86e9-42e7-b6ed-9d1086f8dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load your generated dataset\n",
    "data = np.load('breach_dataset_monthly.npz', allow_pickle=True)\n",
    "\n",
    "X = data['X']                    # Shape: (num_samples, 12, 26)\n",
    "                                 # num_samples varies by your dataset size\n",
    "                                 # 12 = months lookback\n",
    "                                 # 26 = features per month\n",
    "\n",
    "y_platform = data['y_platform']  # Shape: (num_samples,)\n",
    "                                 # Platform labels (integer encoded)\n",
    "\n",
    "y_time_bucket = data['y_time_bucket']  # Time-to-attack classification\n",
    "\n",
    "platform_classes = data['platform_classes']  # Your platform names\n",
    "\n",
    "feature_names = data['feature_names']  # List of 26 feature names\n",
    "print(feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
