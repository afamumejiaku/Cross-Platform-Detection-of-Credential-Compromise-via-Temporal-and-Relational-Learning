{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a87badef-ae7b-4cde-8d5a-164c2fb2dec6",
   "metadata": {},
   "source": [
    "# Models to detect Breaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408bf7e0-d8f4-4c60-85a4-f42f825a726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Breach detection\n",
    "This module handles:\n",
    "- LSTM classifier (PyTorch with CUDA)\n",
    "- GRU classifier (PyTorch with CUDA)\n",
    "- FTA-GRU simplified classifier (PyTorch with CUDA)\n",
    "- FTA-LSTM simplified classifier (PyTorch with CUDA)\n",
    "- Temporal GNN classifier (timesteps-as-nodes; PyTorch with CUDA)\n",
    "- IFCNN-TPP classifier (CNN + temporal encoder; PyTorch with CUDA)\n",
    "- CEP3 classifier (dilated temporal convolution encoder; PyTorch with CUDA)\n",
    "- GCN bipartite classifier (PyTorch with CUDA)\n",
    "- GAT bipartite classifier (PyTorch with CUDA)\n",
    "\n",
    "All models support:\n",
    "- Baseline A: Per-platform models\n",
    "- Model B: Cross-platform model \n",
    "GPU Acceleration:\n",
    "- All PyTorch models use CUDA/MPS when available\n",
    "- Automatic mixed precision (AMP) for faster training\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import traceback\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_auc_score,\n",
    "    average_precision_score, confusion_matrix\n",
    ")\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# ---- Multiprocessing FD pressure fix (Linux) ----\n",
    "import torch.multiprocessing as mp\n",
    "try:\n",
    "    mp.set_sharing_strategy(\"file_system\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Configuration\n",
    "# -------------------------\n",
    "\n",
    "BASE_FEATURES = [\n",
    "    \"sim_to_prev_passwords_max\",\n",
    "    \"sim_to_prev_passwords_mean\",\n",
    "    \"exact_password_reuse\",\n",
    "    \"sim_to_prev_honeywords_max\",\n",
    "    \"sim_to_prev_honeywords_mean\",\n",
    "    \"honeyword_trigger\",\n",
    "    \"current_hw_matches_prev_pw\",\n",
    "    \"time_since_last_leak\",\n",
    "    \"time_since_last_attack\",\n",
    "    \"attack_sequence_index\",\n",
    "    \"delta_similarity\",\n",
    "]\n",
    "\n",
    "CROSS_FEATURES = [\n",
    "    \"num_platforms_seen\",\n",
    "    \"platform_overlap_score\",\n",
    "    \"cross_platform_reuse\",\n",
    "]\n",
    "\n",
    "ID_COLS = [\"Email\", \"Breach_Source\", \"AttackTime\", \"Timestamp\"]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Utils\n",
    "# -------------------------\n",
    "\n",
    "def seed_everything(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def save_json(obj: Any, path: str) -> None:\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2, default=str)\n",
    "\n",
    "def load_json(path: str) -> Any:\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"Get the best available device.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using Apple MPS GPU\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "def make_loader(dataset, batch_size, shuffle, device, setting: str):\n",
    "    \"\"\"\n",
    "    SAFE DataLoader builder (prevents Errno 24 \"Too many open files\"):\n",
    "\n",
    "    - We force num_workers=0 for ALL settings/models because you are running a grid\n",
    "      (cross_platform + per_platform + multiple models) inside one long process.\n",
    "    - persistent_workers is always False.\n",
    "    \"\"\"\n",
    "    pin = (device.type == \"cuda\")\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0,              # IMPORTANT: avoid FD exhaustion\n",
    "        pin_memory=pin,\n",
    "        persistent_workers=False,   # IMPORTANT: don't keep workers around\n",
    "    )\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Data Loading\n",
    "# -------------------------\n",
    "\n",
    "def load_cached(out_dir: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load cached train/val/test splits from Stage 1.\"\"\"\n",
    "    paths = {\n",
    "        \"train\": os.path.join(out_dir, \"train.parquet\"),\n",
    "        \"val\": os.path.join(out_dir, \"val.parquet\"),\n",
    "        \"test\": os.path.join(out_dir, \"test.parquet\"),\n",
    "    }\n",
    "\n",
    "    for key, path in paths.items():\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Missing {path}. Run Stage 1 first.\")\n",
    "\n",
    "    train = pd.read_parquet(paths[\"train\"])\n",
    "    val = pd.read_parquet(paths[\"val\"])\n",
    "    test = pd.read_parquet(paths[\"test\"])\n",
    "\n",
    "    for df in [train, val, test]:\n",
    "        df[\"AttackTime\"] = pd.to_datetime(df[\"AttackTime\"], errors=\"coerce\")\n",
    "        df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Design Matrices\n",
    "# -------------------------\n",
    "\n",
    "def build_design_matrices(\n",
    "    train: pd.DataFrame,\n",
    "    val: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    setting: str,\n",
    "    platform_encoder: Optional[OneHotEncoder] = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, Optional[OneHotEncoder], Dict]:\n",
    "    \"\"\"\n",
    "    Build feature matrices based on setting:\n",
    "    - per_platform: base features only\n",
    "    - cross_platform: base + cross features + one-hot platform\n",
    "    \"\"\"\n",
    "    if setting not in {\"per_platform\", \"pooled\", \"cross_platform\"}:\n",
    "        raise ValueError(f\"Unknown setting={setting}\")\n",
    "\n",
    "    use_cross = (setting == \"cross_platform\")\n",
    "    feat_cols = list(BASE_FEATURES) + (list(CROSS_FEATURES) if use_cross else [])\n",
    "\n",
    "    def _num_df(d: pd.DataFrame) -> np.ndarray:\n",
    "        return d[feat_cols].astype(float).values\n",
    "\n",
    "    Xtr_num, Xva_num, Xte_num = _num_df(train), _num_df(val), _num_df(test)\n",
    "\n",
    "    if setting == \"cross_platform\":\n",
    "        if platform_encoder is None:\n",
    "            platform_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "            platform_encoder.fit(train[[\"Breach_Source\"]].astype(str))\n",
    "        Ptr = platform_encoder.transform(train[[\"Breach_Source\"]].astype(str))\n",
    "        Pva = platform_encoder.transform(val[[\"Breach_Source\"]].astype(str))\n",
    "        Pte = platform_encoder.transform(test[[\"Breach_Source\"]].astype(str))\n",
    "        Xtr = np.concatenate([Xtr_num, Ptr], axis=1)\n",
    "        Xva = np.concatenate([Xva_num, Pva], axis=1)\n",
    "        Xte = np.concatenate([Xte_num, Pte], axis=1)\n",
    "    else:\n",
    "        Xtr, Xva, Xte = Xtr_num, Xva_num, Xte_num\n",
    "\n",
    "    ytr = train[\"y\"].astype(int).values\n",
    "    yva = val[\"y\"].astype(int).values\n",
    "    yte = test[\"y\"].astype(int).values\n",
    "\n",
    "    meta = {\n",
    "        \"feat_cols\": feat_cols,\n",
    "        \"setting\": setting,\n",
    "        \"platform_onehot_dim\": int(Xtr.shape[1] - len(feat_cols)) if setting == \"cross_platform\" else 0,\n",
    "        \"X_dim\": int(Xtr.shape[1]),\n",
    "    }\n",
    "    return Xtr, ytr, Xva, yva, Xte, yte, platform_encoder, meta\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Metrics (Accuracy-based threshold)\n",
    "# -------------------------\n",
    "\n",
    "def choose_threshold_by_accuracy(y_true: np.ndarray, y_prob: np.ndarray) -> float:\n",
    "    \"\"\"Find optimal threshold by ACCURACY on validation set.\"\"\"\n",
    "    best_thr, best_acc = 0.5, -1.0\n",
    "    for thr in np.linspace(0.05, 0.95, 19):\n",
    "        y_pred = (y_prob >= thr).astype(int)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        if acc > best_acc:\n",
    "            best_acc, best_thr = acc, float(thr)\n",
    "    return best_thr\n",
    "\n",
    "def compute_metrics(y_true: np.ndarray, y_prob: np.ndarray, thr: float) -> Dict[str, Any]:\n",
    "    \"\"\"Compute evaluation metrics (accuracy, F1, ROC-AUC, etc.).\"\"\"\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    acc = float(accuracy_score(y_true, y_pred))\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "\n",
    "    out = {\n",
    "        \"threshold\": float(thr),\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": float(p),\n",
    "        \"recall\": float(r),\n",
    "        \"f1\": float(f1),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist(),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        out[\"auroc\"] = float(roc_auc_score(y_true, y_prob))\n",
    "    except Exception:\n",
    "        out[\"auroc\"] = None\n",
    "    try:\n",
    "        out[\"auprc\"] = float(average_precision_score(y_true, y_prob))\n",
    "    except Exception:\n",
    "        out[\"auprc\"] = None\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Sequence Dataset\n",
    "# -------------------------\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X_seq: np.ndarray, y: np.ndarray):\n",
    "        self.X = torch.tensor(X_seq, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def make_sequences_by_email(\n",
    "    df: pd.DataFrame,\n",
    "    feature_cols: List[str],\n",
    "    seq_len: int,\n",
    "    include_current: bool = True,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Build sequences per email for RNN models.\"\"\"\n",
    "    df = df.sort_values([\"Email\", \"AttackTime\"], kind=\"mergesort\")\n",
    "    F = len(feature_cols)\n",
    "\n",
    "    X_list, y_list, idx_list = [], [], []\n",
    "\n",
    "    for email, g in df.groupby(\"Email\", sort=False):\n",
    "        g = g.sort_values(\"AttackTime\", kind=\"mergesort\")\n",
    "        feats = g[feature_cols].astype(float).values\n",
    "        ys = g[\"y\"].astype(int).values\n",
    "        inds = g.index.values\n",
    "\n",
    "        for i in range(len(g)):\n",
    "            end = i + 1 if include_current else i\n",
    "            start = max(0, end - seq_len)\n",
    "            seq = feats[start:end]\n",
    "\n",
    "            if seq.shape[0] < seq_len:\n",
    "                pad = np.zeros((seq_len - seq.shape[0], F), dtype=np.float32)\n",
    "                seq = np.vstack([pad, seq]).astype(np.float32)\n",
    "\n",
    "            X_list.append(seq)\n",
    "            y_list.append(ys[i])\n",
    "            idx_list.append(inds[i])\n",
    "\n",
    "    return (\n",
    "        np.stack(X_list, axis=0).astype(np.float32),\n",
    "        np.array(y_list, dtype=np.int64),\n",
    "        np.array(idx_list, dtype=np.int64),\n",
    "    )\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# PyTorch Models (Sequence)\n",
    "# -------------------------\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden: int = 128, layers: int = 2, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_dim, hidden, num_layers=layers, batch_first=True, dropout=dropout)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out, _ = self.rnn(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.head(last).squeeze(-1)\n",
    "\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden: int = 128, layers: int = 2, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_dim, hidden, num_layers=layers, batch_first=True, dropout=dropout)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out, _ = self.rnn(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.head(last).squeeze(-1)\n",
    "\n",
    "\n",
    "class FTAGru(nn.Module):\n",
    "    \"\"\"\n",
    "    - Feature attention per timestep\n",
    "    - Temporal GRU encoder\n",
    "    - Temporal attention pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden: int = 128, attn_dim: int = 64, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.feature_gate = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.gru = nn.GRU(input_dim, hidden, num_layers=1, batch_first=True)\n",
    "        self.temporal_attn = nn.Sequential(\n",
    "            nn.Linear(hidden, attn_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attn_dim, 1)\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        gates = self.feature_gate(x)\n",
    "        xg = x * gates\n",
    "        h, _ = self.gru(xg)\n",
    "        a = self.temporal_attn(h).squeeze(-1)\n",
    "        w = torch.softmax(a, dim=1).unsqueeze(-1)\n",
    "        pooled = (h * w).sum(dim=1)\n",
    "        return self.head(pooled).squeeze(-1)\n",
    "\n",
    "\n",
    "class FTALstm(nn.Module):\n",
    "    \"\"\"\n",
    "    - Feature attention per timestep\n",
    "    - Temporal LSTM encoder\n",
    "    - Temporal attention pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden: int = 128, attn_dim: int = 64, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.feature_gate = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_dim, hidden, num_layers=1, batch_first=True)\n",
    "        self.temporal_attn = nn.Sequential(\n",
    "            nn.Linear(hidden, attn_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attn_dim, 1)\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        gates = self.feature_gate(x)\n",
    "        xg = x * gates\n",
    "        h, _ = self.lstm(xg)\n",
    "        a = self.temporal_attn(h).squeeze(-1)\n",
    "        w = torch.softmax(a, dim=1).unsqueeze(-1)\n",
    "        pooled = (h * w).sum(dim=1)\n",
    "        return self.head(pooled).squeeze(-1)\n",
    "\n",
    "# -------------------------\n",
    "# IFCNN_TPP (CNN + GRU + attention pooling)\n",
    "# -------------------------\n",
    "\n",
    "class IFCNNTpp(nn.Module):\n",
    "    \"\"\"\n",
    "    IFCNN_TPP-style binary classifier:\n",
    "      - 1D conv over time (feature channels)\n",
    "      - GRU encoder\n",
    "      - Attention pooling\n",
    "      - MLP head -> logits\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden: int = 128,\n",
    "        conv_channels: int = 128,\n",
    "        kernel_size: int = 3,\n",
    "        gru_layers: int = 1,\n",
    "        attn_dim: int = 64,\n",
    "        dropout: float = 0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        # Conv expects (B, C, T) where C=input_dim (treat features as channels)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, conv_channels, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(conv_channels, conv_channels, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        self.proj = nn.Linear(conv_channels, hidden)\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=hidden,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=gru_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if gru_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        self.temporal_attn = nn.Sequential(\n",
    "            nn.Linear(hidden, attn_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attn_dim, 1),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, T, F)\n",
    "        x = x.transpose(1, 2)                 # (B, F, T)\n",
    "        z = self.conv(x)                      # (B, C, T)\n",
    "        z = z.transpose(1, 2)                 # (B, T, C)\n",
    "        z = self.proj(z)                      # (B, T, H)\n",
    "\n",
    "        h, _ = self.gru(z)                    # (B, T, H)\n",
    "\n",
    "        a = self.temporal_attn(h).squeeze(-1) # (B, T)\n",
    "        w = torch.softmax(a, dim=1).unsqueeze(-1)\n",
    "        pooled = (h * w).sum(dim=1)           # (B, H)\n",
    "\n",
    "        logits = self.head(pooled).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# CEP3 (Causal dilated conv stack + attention pooling)\n",
    "# -------------------------\n",
    "\n",
    "class CEP3(nn.Module):\n",
    "    \"\"\"\n",
    "    CEP3-style temporal classifier:\n",
    "      - linear projection to hidden\n",
    "      - 3-layer causal-ish dilated conv over time\n",
    "      - attention pooling\n",
    "      - MLP head -> logits\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden: int = 128,\n",
    "        kernel_size: int = 3,\n",
    "        dropout: float = 0.2,\n",
    "        attn_dim: int = 64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(input_dim, hidden)\n",
    "\n",
    "        # We'll do \"same\" padding then mask via slicing for a causal feel.\n",
    "        # This is stable and avoids heavy TPP math.\n",
    "        def conv_block(dilation: int):\n",
    "            pad = (kernel_size - 1) * dilation\n",
    "            return nn.Sequential(\n",
    "                nn.Conv1d(hidden, hidden, kernel_size=kernel_size, dilation=dilation, padding=pad),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "            )\n",
    "\n",
    "        self.conv1 = conv_block(dilation=1)\n",
    "        self.conv2 = conv_block(dilation=2)\n",
    "        self.conv3 = conv_block(dilation=4)\n",
    "\n",
    "        self.temporal_attn = nn.Sequential(\n",
    "            nn.Linear(hidden, attn_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attn_dim, 1),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, T, F)\n",
    "        h = self.in_proj(x)                   # (B, T, H)\n",
    "        h = h.transpose(1, 2)                 # (B, H, T)\n",
    "\n",
    "        # Convs with padding; slice to keep length T (causal-ish)\n",
    "        T = h.size(-1)\n",
    "        h1 = self.conv1(h)[..., :T]\n",
    "        h2 = self.conv2(h1)[..., :T]\n",
    "        h3 = self.conv3(h2)[..., :T]\n",
    "\n",
    "        out = h3.transpose(1, 2)              # (B, T, H)\n",
    "\n",
    "        a = self.temporal_attn(out).squeeze(-1)  # (B, T)\n",
    "        w = torch.softmax(a, dim=1).unsqueeze(-1)\n",
    "        pooled = (out * w).sum(dim=1)            # (B, H)\n",
    "\n",
    "        logits = self.head(pooled).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Temporal GNN (timesteps-as-nodes) for sequences\n",
    "# -------------------------\n",
    "\n",
    "class TemporalGraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Dense (batch) graph-attention over timesteps.\n",
    "    Treat each timestep as a node; computes attention between all timestep pairs.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, dropout: float = 0.2, alpha: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.W = nn.Parameter(torch.empty(in_features, out_features))\n",
    "        nn.init.xavier_uniform_(self.W, gain=1.414)\n",
    "\n",
    "        self.a = nn.Parameter(torch.empty(2 * out_features, 1))\n",
    "        nn.init.xavier_uniform_(self.a, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, adj: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, T, Fin)\n",
    "        adj: optional (T, T) or (B, T, T) binary mask where 0 means \"no edge\"\n",
    "        returns: (B, T, Fout)\n",
    "        \"\"\"\n",
    "        B, T, _ = x.shape\n",
    "        h = torch.matmul(x, self.W)  # (B, T, Fout)\n",
    "\n",
    "        # Build pairwise concat (B, T, T, 2*Fout)\n",
    "        h1 = h.unsqueeze(2).expand(B, T, T, h.size(-1))\n",
    "        h2 = h.unsqueeze(1).expand(B, T, T, h.size(-1))\n",
    "        hcat = torch.cat([h1, h2], dim=-1)\n",
    "\n",
    "        e = self.leakyrelu(torch.matmul(hcat, self.a).squeeze(-1))  # (B, T, T)\n",
    "\n",
    "        if adj is not None:\n",
    "            if adj.dim() == 2:\n",
    "                adj = adj.unsqueeze(0)  # (1, T, T)\n",
    "            e = e.masked_fill(adj == 0, -1e9)\n",
    "\n",
    "        attn = torch.softmax(e, dim=-1)  # along neighbors\n",
    "        attn = F.dropout(attn, self.dropout, training=self.training)\n",
    "\n",
    "        out = torch.matmul(attn, h)  # (B, T, Fout)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TemporalGNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal GNN classifier:\n",
    "      - GRU feature extraction over time\n",
    "      - Multi-head temporal graph attention (timesteps as nodes)\n",
    "      - Temporal self-attention\n",
    "      - Mean pooling -> MLP head\n",
    "\n",
    "    Works with your SeqDataset and train_torch_binary().\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden: int = 128,\n",
    "        gru_layers: int = 2,\n",
    "        gat_heads: int = 4,\n",
    "        dropout: float = 0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.gat_heads = gat_heads\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_dim,\n",
    "            hidden,\n",
    "            num_layers=gru_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if gru_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        # Multi-head temporal graph attention\n",
    "        # Each head outputs hidden/gat_heads dims; concatenation returns hidden\n",
    "        head_dim = max(1, hidden // gat_heads)\n",
    "        self.gat_layers = nn.ModuleList([\n",
    "            TemporalGraphAttentionLayer(hidden, head_dim, dropout=dropout)\n",
    "            for _ in range(gat_heads)\n",
    "        ])\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(head_dim * gat_heads)\n",
    "\n",
    "        self.temporal_attention = nn.MultiheadAttention(\n",
    "            embed_dim=head_dim * gat_heads,\n",
    "            num_heads=4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(head_dim * gat_heads, (head_dim * gat_heads) // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear((head_dim * gat_heads) // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, T, Fin)\n",
    "        h, _ = self.gru(x)  # (B, T, hidden)\n",
    "\n",
    "        # Dense temporal GAT over timesteps\n",
    "        gat_outs = [gat(h) for gat in self.gat_layers]          # list of (B, T, head_dim)\n",
    "        h_gat = torch.cat(gat_outs, dim=-1)                     # (B, T, head_dim*heads)\n",
    "\n",
    "        # Residual-ish normalization (dims match by construction)\n",
    "        h_gat = self.layer_norm(h_gat)\n",
    "\n",
    "        # Temporal self-attention\n",
    "        attn_out, _ = self.temporal_attention(h_gat, h_gat, h_gat)\n",
    "\n",
    "        # Pool and classify\n",
    "        pooled = attn_out.mean(dim=1)                           # (B, D)\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.head(pooled).squeeze(-1)                  # (B,)\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# Graph Components (Bipartite Email-Platform)\n",
    "# -------------------------\n",
    "\n",
    "def build_bipartite_graph(train_df: pd.DataFrame) -> Tuple[Dict[str, int], Dict[str, int], torch.Tensor]:\n",
    "    \"\"\"Build bipartite email-platform graph.\"\"\"\n",
    "    emails = sorted(train_df[\"Email\"].astype(str).unique().tolist())\n",
    "    plats = sorted(train_df[\"Breach_Source\"].astype(str).unique().tolist())\n",
    "    email2n = {e: i for i, e in enumerate(emails)}\n",
    "    plat2n = {p: i for i, p in enumerate(plats)}\n",
    "\n",
    "    E, P = len(emails), len(plats)\n",
    "    edges = set()\n",
    "\n",
    "    for _, r in train_df[[\"Email\", \"Breach_Source\"]].drop_duplicates().iterrows():\n",
    "        u = email2n[str(r[\"Email\"])]\n",
    "        v = E + plat2n[str(r[\"Breach_Source\"])]\n",
    "        edges.add((u, v))\n",
    "        edges.add((v, u))\n",
    "\n",
    "    if not edges:\n",
    "        raise ValueError(\"No edges in bipartite graph\")\n",
    "\n",
    "    idx = torch.tensor(list(edges), dtype=torch.long).t().contiguous()\n",
    "    N = E + P\n",
    "    val = torch.ones(idx.shape[1], dtype=torch.float32)\n",
    "    A = torch.sparse_coo_tensor(idx, val, (N, N))\n",
    "\n",
    "    return email2n, plat2n, A.coalesce()\n",
    "\n",
    "\n",
    "def normalize_sparse_adj(A: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Symmetric normalization: D^{-1/2} A D^{-1/2}\"\"\"\n",
    "    A = A.coalesce()\n",
    "    idx = A.indices()\n",
    "    val = A.values()\n",
    "    N = A.shape[0]\n",
    "\n",
    "    deg = torch.zeros(N, dtype=torch.float32, device=val.device)\n",
    "    deg.index_add_(0, idx[0], val)\n",
    "    deg_inv_sqrt = torch.pow(deg.clamp(min=1.0), -0.5)\n",
    "    norm_val = deg_inv_sqrt[idx[0]] * val * deg_inv_sqrt[idx[1]]\n",
    "\n",
    "    return torch.sparse_coo_tensor(idx, norm_val, (N, N)).coalesce()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# GCN Components\n",
    "# -------------------------\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, A_norm: torch.Tensor, X: torch.Tensor) -> torch.Tensor:\n",
    "        # Sparse matmul doesn't support fp16 on many CUDA builds.\n",
    "        # Force this operation to fp32 even if AMP/autocast is enabled.\n",
    "        if X.is_cuda:\n",
    "            with torch.cuda.amp.autocast(enabled=False):\n",
    "                AX = torch.sparse.mm(A_norm.float(), X.float())\n",
    "        else:\n",
    "            AX = torch.sparse.mm(A_norm, X)\n",
    "        return self.lin(AX)\n",
    "\n",
    "\n",
    "class BipartiteGCNClassifier(nn.Module):\n",
    "    def __init__(self, node_in_dim: int, gcn_hidden: int, gcn_out: int, event_in_dim: int, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.gcn1 = GCNLayer(node_in_dim, gcn_hidden)\n",
    "        self.gcn2 = GCNLayer(gcn_hidden, gcn_out)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(event_in_dim + 2 * gcn_out, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        A_norm: torch.Tensor,\n",
    "        X_nodes: torch.Tensor,\n",
    "        email_nodes: torch.Tensor,\n",
    "        plat_nodes: torch.Tensor,\n",
    "        X_event: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        h = self.act(self.gcn1(A_norm, X_nodes))\n",
    "        h = self.dropout(h)\n",
    "        h = self.gcn2(A_norm, h)\n",
    "\n",
    "        he = h[email_nodes]\n",
    "        hp = h[plat_nodes]\n",
    "        z = torch.cat([X_event, he, hp], dim=1)\n",
    "        return self.mlp(z).squeeze(-1)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# GAT Components (COO edges; AMP-safe softmax)\n",
    "# -------------------------\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse GAT layer using COO edge list.\n",
    "    Computes attention only over edges in edge_index.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, out_dim: int, dropout: float = 0.2, negative_slope: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        self.attn = nn.Linear(2 * out_dim, 1, bias=False)\n",
    "        self.leakyrelu = nn.LeakyReLU(negative_slope)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def _segment_softmax(scores: torch.Tensor, dst: torch.Tensor, num_nodes: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Softmax over incoming edges per destination node.\n",
    "        Runs in FP32 for numerical stability under AMP, then casts back.\n",
    "        \"\"\"\n",
    "        orig_dtype = scores.dtype\n",
    "        scores_f = scores.float()  # fp32\n",
    "\n",
    "        max_per_dst = torch.full((num_nodes,), -1e9, device=scores.device, dtype=torch.float32)\n",
    "        max_per_dst.scatter_reduce_(0, dst, scores_f, reduce=\"amax\", include_self=True)\n",
    "\n",
    "        scores_f = (scores_f - max_per_dst[dst]).clamp(min=-20.0, max=20.0)\n",
    "        scores_exp = torch.exp(scores_f)\n",
    "\n",
    "        denom = torch.zeros((num_nodes,), device=scores.device, dtype=torch.float32)\n",
    "        denom.scatter_add_(0, dst, scores_exp)\n",
    "\n",
    "        out = scores_exp / denom[dst].clamp_min(1e-12)\n",
    "        return out.to(orig_dtype)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, edge_index: torch.Tensor, num_nodes: int) -> torch.Tensor:\n",
    "        H = self.W(X)  # [N, Fout]\n",
    "\n",
    "        src, dst = edge_index[0], edge_index[1]  # [E], [E]\n",
    "        h_src = H[src]\n",
    "        h_dst = H[dst]\n",
    "\n",
    "        e = self.attn(torch.cat([h_src, h_dst], dim=1)).squeeze(-1)\n",
    "        e = self.leakyrelu(e)\n",
    "\n",
    "        alpha = self._segment_softmax(e, dst, num_nodes)\n",
    "        alpha = self.dropout(alpha)\n",
    "\n",
    "        out = torch.zeros((num_nodes, H.shape[1]), device=H.device, dtype=H.dtype)\n",
    "        out.index_add_(0, dst, h_src * alpha.unsqueeze(-1))\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadGATLayer(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, heads: int = 4, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([GATLayer(in_dim, out_dim, dropout=dropout) for _ in range(heads)])\n",
    "\n",
    "    def forward(self, X: torch.Tensor, edge_index: torch.Tensor, num_nodes: int) -> torch.Tensor:\n",
    "        outs = [h(X, edge_index, num_nodes) for h in self.heads]\n",
    "        return torch.cat(outs, dim=1)\n",
    "\n",
    "\n",
    "class BipartiteGATClassifier(nn.Module):\n",
    "    \"\"\"Bipartite GAT classifier over the email-platform graph.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_dim: int,\n",
    "        gat_hidden: int,\n",
    "        gat_out: int,\n",
    "        heads1: int,\n",
    "        heads2: int,\n",
    "        event_in_dim: int,\n",
    "        dropout: float = 0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "        self.gat1 = MultiHeadGATLayer(node_in_dim, gat_hidden, heads=heads1, dropout=dropout)\n",
    "        self.gat2 = MultiHeadGATLayer(gat_hidden * heads1, gat_out, heads=heads2, dropout=dropout)\n",
    "\n",
    "        gnn_dim = gat_out * heads2\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(event_in_dim + 2 * gnn_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        edge_index: torch.Tensor,\n",
    "        X_nodes: torch.Tensor,\n",
    "        email_nodes: torch.Tensor,\n",
    "        plat_nodes: torch.Tensor,\n",
    "        X_event: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        N = X_nodes.shape[0]\n",
    "        h = self.gat1(X_nodes, edge_index, N)\n",
    "        h = self.act(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.gat2(h, edge_index, N)\n",
    "        h = self.act(h)\n",
    "\n",
    "        he = h[email_nodes]\n",
    "        hp = h[plat_nodes]\n",
    "        z = torch.cat([X_event, he, hp], dim=1)\n",
    "        return self.mlp(z).squeeze(-1)\n",
    "\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, X_event: np.ndarray, y: np.ndarray, email_nodes: np.ndarray, plat_nodes: np.ndarray):\n",
    "        self.Xe = torch.tensor(X_event, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.en = torch.tensor(email_nodes, dtype=torch.long)\n",
    "        self.pn = torch.tensor(plat_nodes, dtype=torch.long)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.Xe.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.Xe[idx], self.y[idx], self.en[idx], self.pn[idx]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Training Loops with AMP (Accuracy Early Stop)\n",
    "# -------------------------\n",
    "\n",
    "def train_torch_binary(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    lr: float = 1e-3,\n",
    "    weight_decay: float = 1e-5,\n",
    "    epochs: int = 30,\n",
    "    patience: int = 8,\n",
    "    class_weight_pos: Optional[float] = None,\n",
    "    use_amp: bool = True,\n",
    ") -> Tuple[nn.Module, Dict[str, Any]]:\n",
    "    \"\"\"Train sequence model with AMP support. Uses ACCURACY for early stopping.\"\"\"\n",
    "    model = model.to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    use_amp = use_amp and device.type == \"cuda\"\n",
    "    scaler = GradScaler() if use_amp else None\n",
    "\n",
    "    if class_weight_pos is None:\n",
    "        bce = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        pos_weight = torch.tensor([class_weight_pos], dtype=torch.float32, device=device)\n",
    "        bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    best_state = None\n",
    "    best_val_acc = -1.0\n",
    "    bad_epochs = 0\n",
    "    history = {\"val_acc\": [], \"val_f1\": [], \"val_loss\": [], \"train_loss\": []}\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "            opt.zero_grad()\n",
    "\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    logits = model(xb)\n",
    "                    loss = bce(logits, yb)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(opt)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                logits = model(xb)\n",
    "                loss = bce(logits, yb)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "                opt.step()\n",
    "\n",
    "            train_losses.append(float(loss.item()))\n",
    "\n",
    "        model.eval()\n",
    "        all_prob, all_y, val_losses = [], [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "\n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        logits = model(xb)\n",
    "                        loss = bce(logits, yb)\n",
    "                else:\n",
    "                    logits = model(xb)\n",
    "                    loss = bce(logits, yb)\n",
    "\n",
    "                val_losses.append(float(loss.item()))\n",
    "                prob = torch.sigmoid(logits).cpu().numpy()\n",
    "                all_prob.append(prob)\n",
    "                all_y.append(yb.cpu().numpy())\n",
    "\n",
    "        y_prob = np.concatenate(all_prob)\n",
    "        y_true = np.concatenate(all_y).astype(int)\n",
    "\n",
    "        thr = choose_threshold_by_accuracy(y_true, y_prob)\n",
    "        met = compute_metrics(y_true, y_prob, thr)\n",
    "        val_acc = met[\"accuracy\"]\n",
    "        val_f1 = met[\"f1\"]\n",
    "\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"val_f1\"].append(val_f1)\n",
    "        history[\"val_loss\"].append(float(np.mean(val_losses)))\n",
    "        history[\"train_loss\"].append(float(np.mean(train_losses)))\n",
    "\n",
    "        print(\n",
    "            f\"  Epoch {ep}: \"\n",
    "            f\"train_loss={np.mean(train_losses):.4f}, \"\n",
    "            f\"val_loss={np.mean(val_losses):.4f}, \"\n",
    "            f\"val_acc={val_acc:.4f}, val_f1={val_f1:.4f}\"\n",
    "        )\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad_epochs = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= patience:\n",
    "                print(f\"  Early stopping at epoch {ep}\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, {\"best_val_acc\": best_val_acc, \"history\": history}\n",
    "\n",
    "\n",
    "def predict_torch_seq(model: nn.Module, loader: DataLoader, device: torch.device, use_amp: bool = True) -> np.ndarray:\n",
    "    \"\"\"Predict with sequence model.\"\"\"\n",
    "    model.eval()\n",
    "    use_amp = use_amp and device.type == \"cuda\"\n",
    "    probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    logits = model(xb)\n",
    "            else:\n",
    "                logits = model(xb)\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            probs.append(prob)\n",
    "\n",
    "    return np.concatenate(probs).astype(float)\n",
    "\n",
    "\n",
    "def train_graph_binary(\n",
    "    model: nn.Module,\n",
    "    graph_obj: torch.Tensor,        # A_norm (sparse) for GCN or edge_index (long) for GAT\n",
    "    X_nodes: torch.Tensor,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    lr: float = 1e-3,\n",
    "    weight_decay: float = 1e-5,\n",
    "    epochs: int = 30,\n",
    "    patience: int = 8,\n",
    "    class_weight_pos: Optional[float] = None,\n",
    "    use_amp: bool = True,\n",
    ") -> Tuple[nn.Module, Dict[str, Any]]:\n",
    "    \"\"\"Train GCN/GAT classifier with AMP support. Uses ACCURACY for early stopping.\"\"\"\n",
    "    model = model.to(device)\n",
    "    graph_obj = graph_obj.to(device)\n",
    "    X_nodes = X_nodes.to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    use_amp = use_amp and device.type == \"cuda\"\n",
    "    scaler = GradScaler() if use_amp else None\n",
    "\n",
    "    if class_weight_pos is None:\n",
    "        bce = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        pos_weight = torch.tensor([class_weight_pos], dtype=torch.float32, device=device)\n",
    "        bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    best_state = None\n",
    "    best_val_acc = -1.0\n",
    "    bad_epochs = 0\n",
    "    history = {\"val_acc\": [], \"val_f1\": []}\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "\n",
    "        for Xe, yb, en, pn in train_loader:\n",
    "            Xe, yb = Xe.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "            en, pn = en.to(device, non_blocking=True), pn.to(device, non_blocking=True)\n",
    "\n",
    "            opt.zero_grad()\n",
    "\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    logits = model(graph_obj, X_nodes, en, pn, Xe)\n",
    "                    loss = bce(logits, yb)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(opt)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                logits = model(graph_obj, X_nodes, en, pn, Xe)\n",
    "                loss = bce(logits, yb)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "                opt.step()\n",
    "\n",
    "        model.eval()\n",
    "        all_prob, all_y = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for Xe, yb, en, pn in val_loader:\n",
    "                Xe, yb = Xe.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "                en, pn = en.to(device, non_blocking=True), pn.to(device, non_blocking=True)\n",
    "\n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        logits = model(graph_obj, X_nodes, en, pn, Xe)\n",
    "                else:\n",
    "                    logits = model(graph_obj, X_nodes, en, pn, Xe)\n",
    "\n",
    "                prob = torch.sigmoid(logits).cpu().numpy()\n",
    "                all_prob.append(prob)\n",
    "                all_y.append(yb.cpu().numpy())\n",
    "\n",
    "        y_prob = np.concatenate(all_prob)\n",
    "        y_true = np.concatenate(all_y).astype(int)\n",
    "\n",
    "        thr = choose_threshold_by_accuracy(y_true, y_prob)\n",
    "        met = compute_metrics(y_true, y_prob, thr)\n",
    "        val_acc = met[\"accuracy\"]\n",
    "        val_f1 = met[\"f1\"]\n",
    "\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"val_f1\"].append(val_f1)\n",
    "\n",
    "        print(f\"  Epoch {ep}: val_acc={val_acc:.4f}, val_f1={val_f1:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad_epochs = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= patience:\n",
    "                print(f\"  Early stopping at epoch {ep}\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, {\"best_val_acc\": best_val_acc, \"history\": history}\n",
    "\n",
    " =============================================================================\n",
    "# SUMMARY TABLE PRINTING (requested)\n",
    "# =============================================================================\n",
    "def predict_graph(\n",
    "    model: nn.Module,\n",
    "    graph_obj: torch.Tensor,\n",
    "    X_nodes: torch.Tensor,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    use_amp: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Predict with GCN/GAT model.\"\"\"\n",
    "    model.eval()\n",
    "    graph_obj = graph_obj.to(device)\n",
    "    X_nodes = X_nodes.to(device)\n",
    "\n",
    "    use_amp = use_amp and device.type == \"cuda\"\n",
    "    probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for Xe, _, en, pn in loader:\n",
    "            Xe = Xe.to(device, non_blocking=True)\n",
    "            en, pn = en.to(device, non_blocking=True), pn.to(device, non_blocking=True)\n",
    "\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    logits = model(graph_obj, X_nodes, en, pn, Xe)\n",
    "            else:\n",
    "                logits = model(graph_obj, X_nodes, en, pn, Xe)\n",
    "\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            probs.append(prob)\n",
    "\n",
    "    return np.concatenate(probs).astype(float)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Main Training Orchestration\n",
    "# -------------------------\n",
    "\n",
    "def train_and_evaluate(\n",
    "    train: pd.DataFrame,\n",
    "    val: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    setting: str,\n",
    "    model_name: str,\n",
    "    seq_len: int = 10,\n",
    "    batch: int = 256,\n",
    "    epochs: int = 30,\n",
    "    lr: float = 1e-3,\n",
    "    weight_decay: float = 1e-5,\n",
    "    patience: int = 8,\n",
    "    seed: int = 42,\n",
    "    device: Optional[torch.device] = None,\n",
    "    use_amp: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    ensure_dir(out_dir)\n",
    "    seed_everything(seed)\n",
    "\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "\n",
    "    results = {\n",
    "        \"setting\": setting,\n",
    "        \"model\": model_name,\n",
    "        \"seq_len\": seq_len,\n",
    "        \"seed\": seed,\n",
    "        \"device\": str(device),\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "\n",
    "    if setting == \"per_platform\":\n",
    "        all_platforms = sorted(train[\"Breach_Source\"].astype(str).unique().tolist())\n",
    "        per_plat_metrics = {}\n",
    "        per_plat_pred_files = {}\n",
    "\n",
    "        for plat in all_platforms:\n",
    "            trp = train[train[\"Breach_Source\"] == plat].copy()\n",
    "            vap = val[val[\"Breach_Source\"] == plat].copy()\n",
    "            tep = test[test[\"Breach_Source\"] == plat].copy()\n",
    "\n",
    "            if len(trp) < 50 or len(vap) < 20 or len(tep) < 20:\n",
    "                print(f\"Skipping platform {plat}: insufficient data\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nTraining for platform: {plat}\")\n",
    "            plat_out = os.path.join(out_dir, f\"per_platform_{plat}\")\n",
    "            ensure_dir(plat_out)\n",
    "\n",
    "            met, pred_path, thr = _train_eval_single(\n",
    "                trp, vap, tep, plat_out, setting=\"per_platform\", model_name=model_name,\n",
    "                seq_len=seq_len, batch=batch, epochs=epochs, lr=lr,\n",
    "                weight_decay=weight_decay, patience=patience, seed=seed,\n",
    "                device=device, use_amp=use_amp\n",
    "            )\n",
    "            per_plat_metrics[plat] = met\n",
    "            per_plat_pred_files[plat] = {\"predictions_csv\": pred_path, \"threshold\": thr}\n",
    "\n",
    "            # Optional cleanup for long loops\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        results[\"per_platform_metrics\"] = per_plat_metrics\n",
    "        results[\"per_platform_predictions\"] = per_plat_pred_files\n",
    "        save_json(results, os.path.join(out_dir, f\"results_{setting}_{model_name}.json\"))\n",
    "        return results\n",
    "\n",
    "    met, pred_path, thr = _train_eval_single(\n",
    "        train, val, test, out_dir, setting=setting, model_name=model_name,\n",
    "        seq_len=seq_len, batch=batch, epochs=epochs, lr=lr,\n",
    "        weight_decay=weight_decay, patience=patience, seed=seed,\n",
    "        device=device, use_amp=use_amp\n",
    "    )\n",
    "    results[\"metrics\"] = met\n",
    "    results[\"predictions_csv\"] = pred_path\n",
    "    results[\"threshold\"] = thr\n",
    "    save_json(results, os.path.join(out_dir, f\"results_{setting}_{model_name}.json\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def _train_eval_single(\n",
    "    train: pd.DataFrame,\n",
    "    val: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    setting: str,\n",
    "    model_name: str,\n",
    "    seq_len: int,\n",
    "    batch: int,\n",
    "    epochs: int,\n",
    "    lr: float,\n",
    "    weight_decay: float,\n",
    "    patience: int,\n",
    "    seed: int,\n",
    "    device: torch.device,\n",
    "    use_amp: bool = True,\n",
    ") -> Tuple[Dict[str, Any], str, float]:\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    Xtr, ytr, Xva, yva, Xte, yte, platform_encoder, meta = build_design_matrices(\n",
    "        train, val, test, setting=setting, platform_encoder=None\n",
    "    )\n",
    "\n",
    "    pos = max(int((ytr == 1).sum()), 1)\n",
    "    neg = max(int((ytr == 0).sum()), 1)\n",
    "    class_weight_pos = float(neg / pos)\n",
    "    print(f\"  Class balance: {pos} positive, {neg} negative (weight={class_weight_pos:.2f})\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Sequence models\n",
    "    # -------------------------\n",
    "    if model_name in {\"lstm\", \"gru\", \"fta_gru\", \"fta_lstm\", \"gnn\", \"ifcnn_tpp\", \"cep3\"}:\n",
    "        print(f\"Training {model_name.upper()}...\")\n",
    "\n",
    "        use_cross = (setting == \"cross_platform\")\n",
    "        feature_cols = list(BASE_FEATURES) + (list(CROSS_FEATURES) if use_cross else [])\n",
    "\n",
    "        if setting == \"cross_platform\":\n",
    "            platform_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "            platform_encoder.fit(train[[\"Breach_Source\"]].astype(str))\n",
    "\n",
    "            def add_plat_onehot(d: pd.DataFrame):\n",
    "                oh = platform_encoder.transform(d[[\"Breach_Source\"]].astype(str))\n",
    "                oh_cols = [f\"plat_oh_{i}\" for i in range(oh.shape[1])]\n",
    "                dd = d.copy()\n",
    "                for j, c in enumerate(oh_cols):\n",
    "                    dd[c] = oh[:, j]\n",
    "                return dd, oh_cols\n",
    "\n",
    "            train2, oh_cols = add_plat_onehot(train)\n",
    "            val2, _ = add_plat_onehot(val)\n",
    "            test2, _ = add_plat_onehot(test)\n",
    "            feature_cols = feature_cols + oh_cols\n",
    "        else:\n",
    "            train2, val2, test2 = train, val, test\n",
    "\n",
    "        Xtr_seq, ytr_seq, idx_tr = make_sequences_by_email(train2, feature_cols, seq_len=seq_len)\n",
    "        Xva_seq, yva_seq, idx_va = make_sequences_by_email(val2, feature_cols, seq_len=seq_len)\n",
    "        Xte_seq, yte_seq, idx_te = make_sequences_by_email(test2, feature_cols, seq_len=seq_len)\n",
    "\n",
    "        tr_loader = make_loader(SeqDataset(Xtr_seq, ytr_seq), batch, True,  device, setting)\n",
    "        va_loader = make_loader(SeqDataset(Xva_seq, yva_seq), batch, False, device, setting)\n",
    "        te_loader = make_loader(SeqDataset(Xte_seq, yte_seq), batch, False, device, setting)\n",
    "\n",
    "        input_dim = Xtr_seq.shape[-1]\n",
    "\n",
    "        if model_name == \"lstm\":\n",
    "            net = LSTMClassifier(input_dim=input_dim, hidden=128, layers=2, dropout=0.2)\n",
    "        elif model_name == \"gru\":\n",
    "            net = GRUClassifier(input_dim=input_dim, hidden=128, layers=2, dropout=0.2)\n",
    "        elif model_name == \"fta_gru\":\n",
    "            net = FTAGru(input_dim=input_dim, hidden=128, attn_dim=64, dropout=0.2)\n",
    "        elif model_name == \"fta_lstm\":\n",
    "            net = FTALstm(input_dim=input_dim, hidden=128, attn_dim=64, dropout=0.2)\n",
    "        elif model_name == \"gnn\":\n",
    "            net = TemporalGNNClassifier(input_dim=input_dim, hidden=128, gru_layers=2, gat_heads=4, dropout=0.2)\n",
    "        elif model_name == \"ifcnn_tpp\":\n",
    "            net = IFCNNTpp(input_dim=input_dim, hidden=128, conv_channels=128, kernel_size=3, gru_layers=1, dropout=0.2)\n",
    "        elif model_name == \"cep3\":\n",
    "            net = CEP3(input_dim=input_dim, hidden=128, kernel_size=3, dropout=0.2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unhandled model_name: {model_name}\")\n",
    "\n",
    "\n",
    "        net, train_info = train_torch_binary(\n",
    "            net, tr_loader, va_loader, device=device,\n",
    "            lr=lr, weight_decay=weight_decay, epochs=epochs, patience=patience,\n",
    "            class_weight_pos=class_weight_pos, use_amp=use_amp\n",
    "        )\n",
    "\n",
    "        va_prob = predict_torch_seq(net, va_loader, device=device, use_amp=use_amp)\n",
    "        best_thr = choose_threshold_by_accuracy(yva_seq, va_prob)\n",
    "        te_prob = predict_torch_seq(net, te_loader, device=device, use_amp=use_amp)\n",
    "\n",
    "        metrics = compute_metrics(yte_seq, te_prob, best_thr)\n",
    "        metrics[\"train_info\"] = train_info\n",
    "\n",
    "        torch.save(net.state_dict(), os.path.join(out_dir, f\"model_{model_name}.pt\"))\n",
    "        save_json({\"meta\": meta, \"feature_cols\": feature_cols}, os.path.join(out_dir, \"meta.json\"))\n",
    "\n",
    "        test_pred_df = test2.loc[idx_te, ID_COLS + [\"y\"]].copy()\n",
    "        test_pred_df[\"pred_prob\"] = te_prob\n",
    "        pred_path = os.path.join(out_dir, \"predictions_test.csv\")\n",
    "        test_pred_df.to_csv(pred_path, index=False)\n",
    "\n",
    "        return metrics, pred_path, best_thr\n",
    "\n",
    "    # -------------------------\n",
    "    # Graph models: GCN + GAT\n",
    "    # -------------------------\n",
    "    if model_name in {\"gcn\", \"gat\"}:\n",
    "        if setting != \"cross_platform\":\n",
    "            raise ValueError(f\"{model_name.upper()} requires setting='cross_platform'\")\n",
    "\n",
    "        print(f\"Training {model_name.upper()}...\")\n",
    "\n",
    "        platform_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "        platform_encoder.fit(train[[\"Breach_Source\"]].astype(str))\n",
    "\n",
    "        def event_X(d: pd.DataFrame) -> np.ndarray:\n",
    "            Xn = d[list(BASE_FEATURES) + list(CROSS_FEATURES)].astype(float).values\n",
    "            oh = platform_encoder.transform(d[[\"Breach_Source\"]].astype(str))\n",
    "            return np.concatenate([Xn, oh], axis=1)\n",
    "\n",
    "        Xtr_e, Xva_e, Xte_e = event_X(train), event_X(val), event_X(test)\n",
    "        ytr_e = train[\"y\"].astype(int).values\n",
    "        yva_e = val[\"y\"].astype(int).values\n",
    "        yte_e = test[\"y\"].astype(int).values\n",
    "\n",
    "        email2n, plat2n, A = build_bipartite_graph(train)\n",
    "        A_norm = normalize_sparse_adj(A)\n",
    "        edge_index = A.indices()  # COO edges for GAT\n",
    "\n",
    "        E, P = len(email2n), len(plat2n)\n",
    "        N = E + P\n",
    "\n",
    "        node_in_dim = Xtr_e.shape[1]\n",
    "        X_nodes = np.zeros((N, node_in_dim), dtype=np.float32)\n",
    "        counts = np.zeros(N, dtype=np.float32)\n",
    "\n",
    "        # Aggregate event features into node features (mean per node)\n",
    "        for i, r in train.iterrows():\n",
    "            e, p = str(r[\"Email\"]), str(r[\"Breach_Source\"])\n",
    "            pos_i = train.index.get_loc(i)\n",
    "\n",
    "            if e in email2n:\n",
    "                ni = email2n[e]\n",
    "                X_nodes[ni] += Xtr_e[pos_i]\n",
    "                counts[ni] += 1.0\n",
    "            if p in plat2n:\n",
    "                nj = E + plat2n[p]\n",
    "                X_nodes[nj] += Xtr_e[pos_i]\n",
    "                counts[nj] += 1.0\n",
    "\n",
    "        counts = np.clip(counts, 1.0, None)\n",
    "        X_nodes = (X_nodes / counts[:, None]).astype(np.float32)\n",
    "\n",
    "        def map_nodes(d: pd.DataFrame):\n",
    "            en = np.array([email2n.get(str(e), 0) for e in d[\"Email\"].astype(str).values], dtype=np.int64)\n",
    "            pn = np.array([E + plat2n.get(str(p), 0) for p in d[\"Breach_Source\"].astype(str).values], dtype=np.int64)\n",
    "            return en, pn\n",
    "\n",
    "        en_tr, pn_tr = map_nodes(train)\n",
    "        en_va, pn_va = map_nodes(val)\n",
    "        en_te, pn_te = map_nodes(test)\n",
    "\n",
    "        tr_loader = make_loader(GraphDataset(Xtr_e, ytr_e, en_tr, pn_tr), batch, True,  device, setting)\n",
    "        va_loader = make_loader(GraphDataset(Xva_e, yva_e, en_va, pn_va), batch, False, device, setting)\n",
    "        te_loader = make_loader(GraphDataset(Xte_e, yte_e, en_te, pn_te), batch, False, device, setting)\n",
    "\n",
    "        if model_name == \"gcn\":\n",
    "            net = BipartiteGCNClassifier(\n",
    "                node_in_dim=node_in_dim,\n",
    "                gcn_hidden=128,\n",
    "                gcn_out=64,\n",
    "                event_in_dim=Xtr_e.shape[1],\n",
    "                dropout=0.2\n",
    "            )\n",
    "            graph_obj = A_norm\n",
    "        else:\n",
    "            net = BipartiteGATClassifier(\n",
    "                node_in_dim=node_in_dim,\n",
    "                gat_hidden=32,\n",
    "                gat_out=32,\n",
    "                heads1=4,\n",
    "                heads2=4,\n",
    "                event_in_dim=Xtr_e.shape[1],\n",
    "                dropout=0.2\n",
    "            )\n",
    "            graph_obj = edge_index\n",
    "\n",
    "        net, train_info = train_graph_binary(\n",
    "            net,\n",
    "            graph_obj=graph_obj,\n",
    "            X_nodes=torch.tensor(X_nodes, dtype=torch.float32),\n",
    "            train_loader=tr_loader,\n",
    "            val_loader=va_loader,\n",
    "            device=device,\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            epochs=epochs,\n",
    "            patience=patience,\n",
    "            class_weight_pos=class_weight_pos,\n",
    "            use_amp=use_amp\n",
    "        )\n",
    "\n",
    "        va_prob = predict_graph(net, graph_obj, torch.tensor(X_nodes, dtype=torch.float32),\n",
    "                                va_loader, device=device, use_amp=use_amp)\n",
    "        best_thr = choose_threshold_by_accuracy(yva_e, va_prob)\n",
    "\n",
    "        te_prob = predict_graph(net, graph_obj, torch.tensor(X_nodes, dtype=torch.float32),\n",
    "                                te_loader, device=device, use_amp=use_amp)\n",
    "\n",
    "        metrics = compute_metrics(yte_e, te_prob, best_thr)\n",
    "        metrics[\"train_info\"] = train_info\n",
    "\n",
    "        torch.save(net.state_dict(), os.path.join(out_dir, f\"model_{model_name}.pt\"))\n",
    "        save_json({\"meta\": meta, \"node_in_dim\": node_in_dim, \"email_count\": E, \"platform_count\": P},\n",
    "                  os.path.join(out_dir, \"meta.json\"))\n",
    "\n",
    "        test_pred_df = test[ID_COLS + [\"y\"]].copy()\n",
    "        test_pred_df[\"pred_prob\"] = te_prob\n",
    "        pred_path = os.path.join(out_dir, \"predictions_test.csv\")\n",
    "        test_pred_df.to_csv(pred_path, index=False)\n",
    "\n",
    "        return metrics, pred_path, best_thr\n",
    "\n",
    "    raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "\n",
    "def main():\n",
    "    # =========================\n",
    "    # USER CONFIGURATION\n",
    "    # =========================\n",
    "    DATA_DIR = \"/home/Passwords/Analysis\"\n",
    "    OUT_DIR = \"/home/Passwords/Analysis/Detection_Output\"\n",
    "\n",
    "    SETTINGS = [\"cross_platform\", \"per_platform\"]\n",
    "    MODELS = [\"fta_lstm\", \"fta_gru\", \"lstm\", \"gru\", \"gnn\", \"ifcnn_tpp\", \"cep3\", \"gcn\", \"gat\"]\n",
    "\n",
    "    SEQ_LEN = 10\n",
    "    BATCH_SIZE = 256\n",
    "    EPOCHS = 30\n",
    "    LR = 1e-3\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    PATIENCE = 8\n",
    "    SEED = 42\n",
    "\n",
    "    DEVICE = None          # \"cuda\" | \"cpu\" | \"mps\" | None(auto)\n",
    "    USE_AMP = True         # set False to disable\n",
    "    FAIL_FAST = False      # True: stop on first failure; False: continue\n",
    "\n",
    "    # =========================\n",
    "    # DEVICE SETUP\n",
    "    # =========================\n",
    "    if DEVICE:\n",
    "        device = torch.device(DEVICE)\n",
    "    else:\n",
    "        device = get_device()\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    # =========================\n",
    "    # LOAD DATA ONCE\n",
    "    # =========================\n",
    "    print(f\"Loading cached data from: {DATA_DIR}\")\n",
    "    train, val, test = load_cached(DATA_DIR)\n",
    "\n",
    "    # =========================\n",
    "    # RUN GRID: settings x models\n",
    "    # =========================\n",
    "    all_runs = []\n",
    "    total = len(SETTINGS) * len(MODELS)\n",
    "    run_idx = 0\n",
    "\n",
    "    for setting in SETTINGS:\n",
    "        for model_name in MODELS:\n",
    "            if model_name in {\"gcn\", \"gat\"} and setting != \"cross_platform\":\n",
    "                print(f\"Skipping invalid combo: {setting} + {model_name}\")\n",
    "                continue\n",
    "\n",
    "            run_idx += 1\n",
    "            run_tag = f\"{setting}__{model_name}\"\n",
    "            run_out_dir = os.path.join(OUT_DIR, run_tag)\n",
    "            os.makedirs(run_out_dir, exist_ok=True)\n",
    "\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(f\"[{run_idx}/{total}] Training model={model_name} setting={setting}\")\n",
    "            print(f\"Run output dir: {run_out_dir}\")\n",
    "            print(\"=\" * 80)\n",
    "\n",
    "            started = time.time()\n",
    "            run_record = {\n",
    "                \"setting\": setting,\n",
    "                \"model\": model_name,\n",
    "                \"out_dir\": run_out_dir,\n",
    "                \"status\": \"started\",\n",
    "                \"started_at\": started,\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                results = train_and_evaluate(\n",
    "                    train=train,\n",
    "                    val=val,\n",
    "                    test=test,\n",
    "                    out_dir=run_out_dir,\n",
    "                    setting=setting,\n",
    "                    model_name=model_name,\n",
    "                    seq_len=SEQ_LEN,\n",
    "                    batch=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    lr=LR,\n",
    "                    weight_decay=WEIGHT_DECAY,\n",
    "                    patience=PATIENCE,\n",
    "                    seed=SEED,\n",
    "                    device=device,\n",
    "                    use_amp=USE_AMP,\n",
    "                )\n",
    "\n",
    "                elapsed = time.time() - started\n",
    "                run_record[\"status\"] = \"ok\"\n",
    "                run_record[\"elapsed_sec\"] = elapsed\n",
    "\n",
    "                if isinstance(results, dict) and \"metrics\" in results and isinstance(results[\"metrics\"], dict):\n",
    "                    run_record[\"metrics\"] = results[\"metrics\"]\n",
    "                    acc = results[\"metrics\"].get(\"accuracy\", None)\n",
    "                    f1 = results[\"metrics\"].get(\"f1\", None)\n",
    "                    auroc = results[\"metrics\"].get(\"auroc\", None)\n",
    "                    auroc_str = f\"{auroc:.4f}\" if auroc is not None else \"N/A\"\n",
    "                    print(f\"Done: {run_tag} | elapsed={elapsed:.1f}s | ACC={acc:.4f} | F1={f1:.4f} | AUROC={auroc_str}\")\n",
    "                else:\n",
    "                    run_record[\"metrics\"] = None\n",
    "                    print(f\"Done: {run_tag} | elapsed={elapsed:.1f}s | (no metrics returned)\")\n",
    "\n",
    "                with open(os.path.join(run_out_dir, \"run_summary.json\"), \"w\") as f:\n",
    "                    json.dump(run_record, f, indent=2)\n",
    "\n",
    "            except Exception as e:\n",
    "                elapsed = time.time() - started\n",
    "                run_record[\"status\"] = \"error\"\n",
    "                run_record[\"elapsed_sec\"] = elapsed\n",
    "                run_record[\"error\"] = str(e)\n",
    "                run_record[\"traceback\"] = traceback.format_exc()\n",
    "\n",
    "                print(f\"ERROR in {run_tag} after {elapsed:.1f}s: {e}\")\n",
    "\n",
    "                with open(os.path.join(run_out_dir, \"run_summary.json\"), \"w\") as f:\n",
    "                    json.dump(run_record, f, indent=2)\n",
    "\n",
    "                if FAIL_FAST:\n",
    "                    raise\n",
    "\n",
    "            # Per-run cleanup (helps long grids)\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            all_runs.append(run_record)\n",
    "\n",
    "    # =========================\n",
    "    # SAVE MASTER SUMMARY\n",
    "    # =========================\n",
    "    master_path = os.path.join(OUT_DIR, \"all_runs_summary.json\")\n",
    "    with open(master_path, \"w\") as f:\n",
    "        json.dump(all_runs, f, indent=2)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ALL RUNS COMPLETE\")\n",
    "    print(f\"Master summary: {master_path}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    scored = []\n",
    "    for r in all_runs:\n",
    "        if r.get(\"status\") == \"ok\" and isinstance(r.get(\"metrics\"), dict):\n",
    "            acc = r[\"metrics\"].get(\"accuracy\", None)\n",
    "            f1 = r[\"metrics\"].get(\"f1\", None)\n",
    "            auroc = r[\"metrics\"].get(\"auroc\", None)\n",
    "            if acc is not None:\n",
    "                scored.append((float(acc), f1, auroc, r[\"setting\"], r[\"model\"], r[\"out_dir\"]))\n",
    "\n",
    "    if scored:\n",
    "        scored.sort(reverse=True)\n",
    "        print(\"\\nTop runs by Accuracy:\")\n",
    "        for i, (acc, f1, auroc, setting, model, out_dir) in enumerate(scored[:10], 1):\n",
    "            f1_str = f\"{f1:.4f}\" if f1 is not None else \"N/A\"\n",
    "            auroc_str = f\"{auroc:.4f}\" if auroc is not None else \"N/A\"\n",
    "            print(f\"{i:>2}. ACC={acc:.4f} F1={f1_str} AUROC={auroc_str} | {setting:>13} | {model:>8}\")\n",
    "    else:\n",
    "        print(\"\\nNo accuracy scores found in returned metrics.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ccb4f5-db1d-45bb-b403-9d03e22d1a65",
   "metadata": {},
   "source": [
    "# Detection and Delay for Breach Dection Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6806f1bf-9270-4912-bcc5-05959d4a11d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import traceback\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# USER CONFIG (SET THIS)\n",
    "# =============================================================================\n",
    "OUT_DIR = \"Detection_Output\"\n",
    "MODELS = [\"fta_lstm\", \"fta_gru\", \"lstm\", \"gru\", \"gnn\", \"ifcnn_tpp\", \"cep3\", \"gcn\", \"gat\"]\n",
    "SETTINGS = [\"per_platform\", \"cross_platform\"]\n",
    "\n",
    "STAGE3_DIR = os.path.join(OUT_DIR, \"stage3\")\n",
    "DELAY_DIR = os.path.join(STAGE3_DIR, \"delay\")\n",
    "BENEFIT_DIR = os.path.join(STAGE3_DIR, \"benefit\")\n",
    "REPORT_PATH = os.path.join(STAGE3_DIR, \"summary_report.json\")\n",
    "SUMMARY_TABLE_CSV = os.path.join(STAGE3_DIR, \"summary_table.csv\")\n",
    "\n",
    "os.makedirs(DELAY_DIR, exist_ok=True)\n",
    "os.makedirs(BENEFIT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SAFE HELPERS\n",
    "# =============================================================================\n",
    "def _exists_file(path: str) -> bool:\n",
    "    if not path or not os.path.exists(path):\n",
    "        print(f\"[WARNING] Missing file: {path}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def _exists_dir(path: str) -> bool:\n",
    "    if not path or not os.path.isdir(path):\n",
    "        print(f\"[WARNING] Missing directory: {path}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def _safe_json_load(path: str) -> Optional[Dict[str, Any]]:\n",
    "    if not _exists_file(path):\n",
    "        return None\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] Failed to read JSON {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def _safe_json_dump(obj: Any, path: str) -> None:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2, default=str)\n",
    "\n",
    "def _safe_run(step_name: str, fn, **kwargs) -> Tuple[bool, Optional[Any]]:\n",
    "    print(f\"\\n--- {step_name} ---\")\n",
    "    try:\n",
    "        out = fn(**kwargs)\n",
    "        print(f\"[OK] {step_name}\")\n",
    "        return True, out\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {step_name}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return False, None\n",
    "\n",
    "def _fmt(x, nd=4):\n",
    "    if x is None:\n",
    "        return \"N/A\"\n",
    "    try:\n",
    "        if isinstance(x, (int, float, np.floating)) and np.isnan(x):\n",
    "            return \"N/A\"\n",
    "        return f\"{float(x):.{nd}f}\"\n",
    "    except Exception:\n",
    "        return str(x)\n",
    "\n",
    "def _run_dir(setting: str, model: str) -> str:\n",
    "    return os.path.join(OUT_DIR, f\"{setting}__{model}\")\n",
    "\n",
    "def _results_json_path(setting: str, model: str) -> str:\n",
    "    return os.path.join(_run_dir(setting, model), f\"results_{setting}_{model}.json\")\n",
    "\n",
    "def _predictions_csv_path(setting: str, model: str) -> str:\n",
    "    return os.path.join(_run_dir(setting, model), \"predictions_test.csv\")\n",
    "\n",
    "def load_stage2_run_index() -> List[Dict[str, Any]]:\n",
    "    master = os.path.join(OUT_DIR, \"all_runs_summary.json\")\n",
    "    data = _safe_json_load(master)\n",
    "    if isinstance(data, list) and data:\n",
    "        ok = [r for r in data if r.get(\"status\") == \"ok\" and r.get(\"out_dir\")]\n",
    "        print(f\"[INFO] Loaded {len(ok)} successful runs from {master}\")\n",
    "        return ok\n",
    "\n",
    "    runs = []\n",
    "    if _exists_dir(OUT_DIR):\n",
    "        for name in os.listdir(OUT_DIR):\n",
    "            if \"__\" in name:\n",
    "                full = os.path.join(OUT_DIR, name)\n",
    "                if os.path.isdir(full):\n",
    "                    s, m = name.split(\"__\", 1)\n",
    "                    runs.append({\"setting\": s, \"model\": m, \"out_dir\": full, \"status\": \"unknown\"})\n",
    "    print(f\"[INFO] Scanned {len(runs)} run dirs from OUT_DIR\")\n",
    "    return runs\n",
    "\n",
    "def get_predictions_jobs(setting: str, model: str) -> List[Dict[str, Any]]:\n",
    "    res = _safe_json_load(_results_json_path(setting, model))\n",
    "    jobs = []\n",
    "\n",
    "    if not res:\n",
    "        pred = _predictions_csv_path(setting, model)\n",
    "        if _exists_file(pred):\n",
    "            jobs.append({\n",
    "                \"tag\": f\"{setting}__{model}\",\n",
    "                \"predictions_csv\": pred,\n",
    "                \"threshold\": 0.5,\n",
    "                \"output_json\": os.path.join(DELAY_DIR, f\"{setting}__{model}.json\"),\n",
    "            })\n",
    "        return jobs\n",
    "\n",
    "    if setting == \"per_platform\":\n",
    "        pred_map = res.get(\"per_platform_predictions\", {}) or {}\n",
    "        for plat, info in pred_map.items():\n",
    "            jobs.append({\n",
    "                \"tag\": f\"{setting}__{model}__{plat}\",\n",
    "                \"predictions_csv\": info.get(\"predictions_csv\"),\n",
    "                \"threshold\": float(info.get(\"threshold\", 0.5)),\n",
    "                \"output_json\": os.path.join(DELAY_DIR, f\"{setting}__{model}__{plat}.json\"),\n",
    "            })\n",
    "        return jobs\n",
    "\n",
    "    # cross_platform\n",
    "    jobs.append({\n",
    "        \"tag\": f\"{setting}__{model}\",\n",
    "        \"predictions_csv\": res.get(\"predictions_csv\") or _predictions_csv_path(setting, model),\n",
    "        \"threshold\": float(res.get(\"threshold\", 0.5)),\n",
    "        \"output_json\": os.path.join(DELAY_DIR, f\"{setting}__{model}.json\"),\n",
    "    })\n",
    "    return jobs\n",
    "\n",
    "\n",
    "def _pick_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    cols = set(df.columns)\n",
    "    for c in candidates:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def compute_detection_delay_from_predictions(predictions_path: str, threshold: float) -> Dict[str, Any]:\n",
    "   df = pd.read_csv(predictions_path)\n",
    "    y_col = _pick_col(df, [\"y_true\", \"label\", \"target\", \"y\", \"gt\", \"ground_truth\"])\n",
    "    s_col = _pick_col(df, [\"y_score\", \"score\", \"prob\", \"proba\", \"y_pred_proba\", \"pred_prob\", \"prediction\", \"pred\"])\n",
    "    t_col = _pick_col(df, [\"AttackTime\", \"attack_time\", \"time\", \"t\", \"Timestamp\", \"timestamp\"])\n",
    "    e_col = _pick_col(df, [\"Email\", \"email\", \"user\", \"userid\", \"uid\"])\n",
    "\n",
    "    if y_col is None or s_col is None:\n",
    "        raise ValueError(\n",
    "            f\"Predictions CSV missing required columns. Found columns: {list(df.columns)}. \"\n",
    "            f\"Need y_col in {['y_true','label','target','y']} and s_col in {['y_score','prob','pred_prob','prediction']}.\"\n",
    "        )\n",
    "\n",
    "    y = df[y_col].astype(int).values\n",
    "    s = df[s_col].astype(float).values\n",
    "    pred = (s >= float(threshold)).astype(int)\n",
    "\n",
    "    # time axis\n",
    "    if t_col is None:\n",
    "        t = np.arange(len(df), dtype=float)\n",
    "        time_type = \"index\"\n",
    "    else:\n",
    "        series = df[t_col]\n",
    "        try:\n",
    "            tt = pd.to_datetime(series, errors=\"raise\")\n",
    "            t = tt.view(\"int64\") / 1e9  # seconds\n",
    "            time_type = \"datetime\"\n",
    "        except Exception:\n",
    "            try:\n",
    "                t = pd.to_numeric(series, errors=\"raise\").astype(float).values\n",
    "                time_type = \"numeric\"\n",
    "            except Exception:\n",
    "                t = np.arange(len(df), dtype=float)\n",
    "                time_type = \"index\"\n",
    "\n",
    "    def delay_for_slice(idx: np.ndarray) -> Optional[float]:\n",
    "        yy = y[idx]\n",
    "        pp = pred[idx]\n",
    "        tt = np.asarray(t)[idx]\n",
    "\n",
    "        pos_idx = np.where(yy == 1)[0]\n",
    "        if len(pos_idx) == 0:\n",
    "            return None\n",
    "        t_first_pos = tt[pos_idx].min()\n",
    "\n",
    "        tp_idx = np.where((yy == 1) & (pp == 1))[0]\n",
    "        if len(tp_idx) == 0:\n",
    "            return None\n",
    "        t_first_detect = tt[tp_idx].min()\n",
    "\n",
    "        return float(t_first_detect - t_first_pos)\n",
    "\n",
    "    delays = []\n",
    "    detected_count = 0\n",
    "    total_with_positive = 0\n",
    "\n",
    "    if e_col is not None:\n",
    "        for _, g in df.groupby(e_col, sort=False):\n",
    "            idx = g.index.values\n",
    "            if (y[idx] == 1).any():\n",
    "                total_with_positive += 1\n",
    "                d = delay_for_slice(idx)\n",
    "                if d is not None:\n",
    "                    detected_count += 1\n",
    "                    delays.append(d)\n",
    "    else:\n",
    "        if (y == 1).any():\n",
    "            total_with_positive = 1\n",
    "            d = delay_for_slice(np.arange(len(df)))\n",
    "            if d is not None:\n",
    "                detected_count = 1\n",
    "                delays.append(d)\n",
    "\n",
    "    out = {\n",
    "        \"predictions_path\": predictions_path,\n",
    "        \"threshold\": float(threshold),\n",
    "        \"time_axis\": time_type,\n",
    "        \"n_rows\": int(len(df)),\n",
    "        \"n_positive_rows\": int((y == 1).sum()),\n",
    "        \"n_predicted_positive_rows\": int((pred == 1).sum()),\n",
    "        \"n_entities_with_positive\": int(total_with_positive),\n",
    "        \"n_entities_detected\": int(detected_count),\n",
    "        \"detection_rate_entities\": float(detected_count / total_with_positive) if total_with_positive else None,\n",
    "        \"delay_mean\": float(np.mean(delays)) if delays else None,\n",
    "        \"delay_median\": float(np.median(delays)) if delays else None,\n",
    "        \"delay_p90\": float(np.quantile(delays, 0.90)) if delays else None,\n",
    "        \"delay_min\": float(np.min(delays)) if delays else None,\n",
    "        \"delay_max\": float(np.max(delays)) if delays else None,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "def extract_primary_metrics(results_json: Dict[str, Any], setting: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Supports:\n",
    "      A) Your updated stage2:\n",
    "         - cross_platform: results_json[\"metrics\"] = {accuracy,f1,auroc,auprc,threshold,...}\n",
    "         - per_platform: results_json[\"metrics\"] contains AVERAGES + best_* + n_platforms\n",
    "      B) Older temp.py-like:\n",
    "         - metrics_test: {f1, roc_auc, ...}, threshold at top-level\n",
    "    \"\"\"\n",
    "    out: Dict[str, Any] = {\"setting\": setting}\n",
    "\n",
    "    # New schema\n",
    "    if isinstance(results_json.get(\"metrics\"), dict):\n",
    "        m = results_json[\"metrics\"]\n",
    "        out.update({\n",
    "            \"accuracy\": m.get(\"accuracy\"),\n",
    "            \"f1\": m.get(\"f1\"),\n",
    "            \"auroc\": m.get(\"auroc\"),\n",
    "            \"auprc\": m.get(\"auprc\"),\n",
    "            \"threshold\": m.get(\"threshold\", results_json.get(\"threshold\")),\n",
    "        })\n",
    "        # per-platform extras (if present)\n",
    "        out.update({\n",
    "            \"n_platforms\": m.get(\"n_platforms\"),\n",
    "            \"best_accuracy\": m.get(\"best_accuracy\"),\n",
    "            \"best_accuracy_platform\": m.get(\"best_accuracy_platform\"),\n",
    "            \"best_f1\": m.get(\"best_f1\"),\n",
    "            \"best_f1_platform\": m.get(\"best_f1_platform\"),\n",
    "            \"best_auroc\": m.get(\"best_auroc\"),\n",
    "            \"best_auroc_platform\": m.get(\"best_auroc_platform\"),\n",
    "            \"best_auprc\": m.get(\"best_auprc\"),\n",
    "            \"best_auprc_platform\": m.get(\"best_auprc_platform\"),\n",
    "        })\n",
    "        return {k: v for k, v in out.items() if v is not None}\n",
    "\n",
    "    # Old schema\n",
    "    mtest = results_json.get(\"metrics_test\") or results_json.get(\"metrics\") or {}\n",
    "    out.update({\n",
    "        \"threshold\": results_json.get(\"threshold\"),\n",
    "        \"f1\": mtest.get(\"f1\"),\n",
    "        \"auroc\": mtest.get(\"roc_auc\") or mtest.get(\"auroc\"),\n",
    "        \"auprc\": mtest.get(\"pr_auc\") or mtest.get(\"auprc\"),\n",
    "        \"accuracy\": mtest.get(\"accuracy\"),\n",
    "        \"precision\": mtest.get(\"precision\"),\n",
    "        \"recall\": mtest.get(\"recall\"),\n",
    "    })\n",
    "    return {k: v for k, v in out.items() if v is not None}\n",
    "\n",
    "\n",
    "def compute_cross_platform_benefit(model: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compare cross_platform vs per_platform for a given model using results JSONs.\n",
    "    - If per_platform contains averaged metrics, deltas will be computed against those averages.\n",
    "    \"\"\"\n",
    "    per_path = _results_json_path(\"per_platform\", model)\n",
    "    cross_path = _results_json_path(\"cross_platform\", model)\n",
    "\n",
    "    per = _safe_json_load(per_path)\n",
    "    cross = _safe_json_load(cross_path)\n",
    "\n",
    "    if not per or not cross:\n",
    "        return {\n",
    "            \"model\": model,\n",
    "            \"status\": \"missing_inputs\",\n",
    "            \"per_platform_results_json\": per_path,\n",
    "            \"cross_platform_results_json\": cross_path,\n",
    "        }\n",
    "\n",
    "    per_m = extract_primary_metrics(per, setting=\"per_platform\")\n",
    "    cross_m = extract_primary_metrics(cross, setting=\"cross_platform\")\n",
    "\n",
    "    def delta(k):\n",
    "        if k in per_m and k in cross_m:\n",
    "            try:\n",
    "                return float(cross_m[k]) - float(per_m[k])\n",
    "            except Exception:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "    benefit = {\n",
    "        \"model\": model,\n",
    "        \"status\": \"ok\",\n",
    "        \"per_platform\": per_m,\n",
    "        \"cross_platform\": cross_m,\n",
    "        \"delta\": {\n",
    "            \"accuracy\": delta(\"accuracy\"),\n",
    "            \"f1\": delta(\"f1\"),\n",
    "            \"auroc\": delta(\"auroc\"),\n",
    "            \"auprc\": delta(\"auprc\"),\n",
    "        }\n",
    "    }\n",
    "    return benefit\n",
    "\n",
    "def run_all_detection_delay():\n",
    "    runs = load_stage2_run_index()\n",
    "    seen = set()\n",
    "\n",
    "    for r in runs:\n",
    "        setting = r.get(\"setting\")\n",
    "        model = r.get(\"model\")\n",
    "        if setting not in SETTINGS or model not in MODELS:\n",
    "            continue\n",
    "        key = (setting, model)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "\n",
    "        jobs = get_predictions_jobs(setting, model)\n",
    "        if not jobs:\n",
    "            print(f\"[SKIP] No prediction jobs found for {setting}__{model}\")\n",
    "            continue\n",
    "\n",
    "        for j in jobs:\n",
    "            pred_csv = j[\"predictions_csv\"]\n",
    "            if not _exists_file(pred_csv):\n",
    "                print(f\"[SKIP] Missing predictions for {j['tag']}\")\n",
    "                continue\n",
    "\n",
    "            ok, metrics = _safe_run(\n",
    "                f\"Detection Delay: {j['tag']}\",\n",
    "                compute_detection_delay_from_predictions,\n",
    "                predictions_path=pred_csv,\n",
    "                threshold=float(j[\"threshold\"]),\n",
    "            )\n",
    "            if ok and metrics is not None:\n",
    "                _safe_json_dump(metrics, j[\"output_json\"])\n",
    "\n",
    "\n",
    "def run_all_benefits():\n",
    "    for m in MODELS:\n",
    "        out_json = os.path.join(BENEFIT_DIR, f\"benefit__{m}.json\")\n",
    "        ok, benefit = _safe_run(f\"Cross-Platform Benefit: {m}\", compute_cross_platform_benefit, model=m)\n",
    "        if ok and benefit is not None:\n",
    "            _safe_json_dump(benefit, out_json)\n",
    "\n",
    "\n",
    "def generate_summary_report() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Aggregates:\n",
    "      - benefits per model\n",
    "      - delay stats per run (from stage3/delay/*.json)\n",
    "      - stage2 extracted metrics for table printing\n",
    "    \"\"\"\n",
    "    report: Dict[str, Any] = {\"out_dir\": OUT_DIR, \"models\": MODELS, \"benefits\": {}, \"delays\": {}, \"stage2_metrics\": {}}\n",
    "\n",
    "    # benefits\n",
    "    for m in MODELS:\n",
    "        p = os.path.join(BENEFIT_DIR, f\"benefit__{m}.json\")\n",
    "        report[\"benefits\"][m] = _safe_json_load(p) or {\"status\": \"missing\", \"path\": p}\n",
    "\n",
    "    # delays\n",
    "    if _exists_dir(DELAY_DIR):\n",
    "        for fn in sorted(os.listdir(DELAY_DIR)):\n",
    "            if fn.endswith(\".json\"):\n",
    "                p = os.path.join(DELAY_DIR, fn)\n",
    "                report[\"delays\"][fn.replace(\".json\", \"\")] = _safe_json_load(p) or {\"status\": \"missing\", \"path\": p}\n",
    "\n",
    "    # stage2 metrics\n",
    "    for setting in SETTINGS:\n",
    "        for m in MODELS:\n",
    "            p = _results_json_path(setting, m)\n",
    "            res = _safe_json_load(p)\n",
    "            key = f\"{setting}__{m}\"\n",
    "            if res:\n",
    "                report[\"stage2_metrics\"][key] = extract_primary_metrics(res, setting=setting)\n",
    "            else:\n",
    "                report[\"stage2_metrics\"][key] = {\"status\": \"missing\", \"path\": p}\n",
    "\n",
    "    # leaderboard by delta f1 if available\n",
    "    leaderboard = []\n",
    "    for m, b in report[\"benefits\"].items():\n",
    "        d = (b.get(\"delta\") or {}).get(\"f1\") if isinstance(b, dict) else None\n",
    "        if d is not None:\n",
    "            leaderboard.append((m, d))\n",
    "    leaderboard.sort(key=lambda x: x[1], reverse=True)\n",
    "    report[\"leaderboard_by_delta_f1\"] = [{\"model\": m, \"delta_f1\": d} for m, d in leaderboard]\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY TABLE PRINTING (requested)\n",
    "# =============================================================================\n",
    "def _read_delay_for_tag(tag: str, report_delays: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    d = report_delays.get(tag)\n",
    "    if not isinstance(d, dict):\n",
    "        return {}\n",
    "    return {\n",
    "        \"det_rate_entities\": d.get(\"detection_rate_entities\"),\n",
    "        \"delay_mean\": d.get(\"delay_mean\"),\n",
    "        \"delay_median\": d.get(\"delay_median\"),\n",
    "        \"delay_p90\": d.get(\"delay_p90\"),\n",
    "    }\n",
    "\n",
    "def _aggregate_per_platform_delay(report_delays: Dict[str, Any], model: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Aggregate per-platform delay JSONs:\n",
    "      keys look like: per_platform__{model}__{platform}\n",
    "    Return mean of means for delays, and mean det_rate_entities across platforms.\n",
    "    \"\"\"\n",
    "    prefix = f\"per_platform__{model}__\"\n",
    "    rows = []\n",
    "    for k, v in report_delays.items():\n",
    "        if k.startswith(prefix) and isinstance(v, dict) and v.get(\"predictions_path\"):\n",
    "            rows.append(v)\n",
    "\n",
    "    if not rows:\n",
    "        return {}\n",
    "\n",
    "    det_rates = [r.get(\"detection_rate_entities\") for r in rows if r.get(\"detection_rate_entities\") is not None]\n",
    "    mean_delays = [r.get(\"delay_mean\") for r in rows if r.get(\"delay_mean\") is not None]\n",
    "    med_delays = [r.get(\"delay_median\") for r in rows if r.get(\"delay_median\") is not None]\n",
    "    p90_delays = [r.get(\"delay_p90\") for r in rows if r.get(\"delay_p90\") is not None]\n",
    "\n",
    "    return {\n",
    "        \"det_rate_entities\": float(np.mean(det_rates)) if det_rates else None,\n",
    "        \"delay_mean\": float(np.mean(mean_delays)) if mean_delays else None,\n",
    "        \"delay_median\": float(np.mean(med_delays)) if med_delays else None,\n",
    "        \"delay_p90\": float(np.mean(p90_delays)) if p90_delays else None,\n",
    "        \"n_delay_platforms\": int(len(rows)),\n",
    "    }\n",
    "\n",
    "def print_summary_table(report: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prints a clean summary table across:\n",
    "      - per_platform (averages + best scores if present)\n",
    "      - cross_platform\n",
    "    Adds delay metrics:\n",
    "      - cross_platform uses its delay json directly\n",
    "      - per_platform uses mean across per-platform delay jsons (if available)\n",
    "    \"\"\"\n",
    "    stage2 = report.get(\"stage2_metrics\", {}) or {}\n",
    "    delays = report.get(\"delays\", {}) or {}\n",
    "    benefits = report.get(\"benefits\", {}) or {}\n",
    "\n",
    "    rows = []\n",
    "    for setting in SETTINGS:\n",
    "        for m in MODELS:\n",
    "            key = f\"{setting}__{m}\"\n",
    "            m2 = stage2.get(key, {})\n",
    "            if not isinstance(m2, dict):\n",
    "                m2 = {}\n",
    "\n",
    "            row = {\n",
    "                \"setting\": setting,\n",
    "                \"model\": m,\n",
    "                \"accuracy\": m2.get(\"accuracy\"),\n",
    "                \"f1\": m2.get(\"f1\"),\n",
    "                \"auroc\": m2.get(\"auroc\"),\n",
    "                \"auprc\": m2.get(\"auprc\"),\n",
    "                \"threshold\": m2.get(\"threshold\"),\n",
    "            }\n",
    "\n",
    "            # per-platform extra columns (from stage2 aggregates, if available)\n",
    "            row.update({\n",
    "                \"n_platforms_trained\": m2.get(\"n_platforms\"),\n",
    "                \"best_accuracy\": m2.get(\"best_accuracy\"),\n",
    "                \"best_accuracy_platform\": m2.get(\"best_accuracy_platform\"),\n",
    "                \"best_f1\": m2.get(\"best_f1\"),\n",
    "                \"best_f1_platform\": m2.get(\"best_f1_platform\"),\n",
    "            })\n",
    "\n",
    "            # delays\n",
    "            if setting == \"cross_platform\":\n",
    "                row.update(_read_delay_for_tag(key, delays))\n",
    "            else:\n",
    "                row.update(_aggregate_per_platform_delay(delays, model=m))\n",
    "\n",
    "            # benefit deltas (for convenience on each row)\n",
    "            b = benefits.get(m, {})\n",
    "            if isinstance(b, dict) and b.get(\"status\") == \"ok\":\n",
    "                d = b.get(\"delta\") or {}\n",
    "                row.update({\n",
    "                    \"delta_accuracy\": d.get(\"accuracy\"),\n",
    "                    \"delta_f1\": d.get(\"f1\"),\n",
    "                    \"delta_auroc\": d.get(\"auroc\"),\n",
    "                    \"delta_auprc\": d.get(\"auprc\"),\n",
    "                })\n",
    "            else:\n",
    "                row.update({\n",
    "                    \"delta_accuracy\": None,\n",
    "                    \"delta_f1\": None,\n",
    "                    \"delta_auroc\": None,\n",
    "                    \"delta_auprc\": None,\n",
    "                })\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Order rows: cross_platform first within each model, then per_platform\n",
    "    setting_order = {\"cross_platform\": 0, \"per_platform\": 1}\n",
    "    df[\"setting_rank\"] = df[\"setting\"].map(setting_order).fillna(99).astype(int)\n",
    "    df = df.sort_values([\"model\", \"setting_rank\"], ascending=[True, True]).drop(columns=[\"setting_rank\"])\n",
    "\n",
    "    # Print\n",
    "    print(\"\\n\" + \"=\" * 140)\n",
    "    print(\"SUMMARY TABLE (Stage2 + Stage3)\")\n",
    "    print(\" - cross_platform: metrics from the global model; delays from cross_platform__model delay JSON\")\n",
    "    print(\" - per_platform:   metrics are (if available) averaged in Stage2 results; delays are averaged across platforms\")\n",
    "    print(\"=\" * 140)MODELS = [\"fta_lstm\", \"fta_gru\", \"lstm\", \"gru\", \"gnn\", \"gcn\", \"gat\"] \n",
    "\n",
    "    cols = [\n",
    "        \"model\", \"setting\",\n",
    "        \"accuracy\", \"f1\", \"auroc\", \"auprc\", \"threshold\",\n",
    "        \"n_platforms_trained\", \"best_accuracy\", \"best_accuracy_platform\", \"best_f1\", \"best_f1_platform\",\n",
    "        \"det_rate_entities\", \"delay_mean\", \"delay_median\", \"delay_p90\",\n",
    "        \"delta_accuracy\", \"delta_f1\", \"delta_auroc\", \"delta_auprc\",\n",
    "    ]\n",
    "\n",
    "    # ensure columns exist\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = None\n",
    "\n",
    "    # pretty-print with formatting (avoid scientific notation)\n",
    "    df_print = df[cols].copy()\n",
    "    for c in [\"accuracy\",\"f1\",\"auroc\",\"auprc\",\"threshold\",\"det_rate_entities\",\"delay_mean\",\"delay_median\",\"delay_p90\",\n",
    "              \"delta_accuracy\",\"delta_f1\",\"delta_auroc\",\"delta_auprc\",\"best_accuracy\",\"best_f1\"]:\n",
    "        if c in df_print.columns:\n",
    "            df_print[c] = df_print[c].apply(lambda x: _fmt(x, nd=4))\n",
    "\n",
    "    print(df_print.to_string(index=False))\n",
    "    print(\"\\n\" + \"-\" * 140)\n",
    "    print(f\"Saved CSV: {SUMMARY_TABLE_CSV}\")\n",
    "    df.to_csv(SUMMARY_TABLE_CSV, index=False)\n",
    "\n",
    "    # Quick best views\n",
    "    def _best(df0: pd.DataFrame, setting: str, col: str) -> Optional[pd.Series]:\n",
    "        sub = df0[df0[\"setting\"] == setting].copy()\n",
    "        sub = sub[pd.to_numeric(sub[col], errors=\"coerce\").notna()]\n",
    "        if sub.empty:\n",
    "            return None\n",
    "        sub[col] = pd.to_numeric(sub[col], errors=\"coerce\")\n",
    "        return sub.sort_values(col, ascending=False).iloc[0]\n",
    "\n",
    "    best_cross_acc = _best(df, \"cross_platform\", \"accuracy\")\n",
    "    best_per_avg_acc = _best(df, \"per_platform\", \"accuracy\")\n",
    "\n",
    "    print(\"\\nBEST RUNS (by Accuracy):\")\n",
    "    if best_cross_acc is not None:\n",
    "        print(f\"  cross_platform best: model={best_cross_acc['model']} ACC={_fmt(best_cross_acc['accuracy'])} F1={_fmt(best_cross_acc['f1'])}\")\n",
    "    else:\n",
    "        print(\"  cross_platform best: N/A\")\n",
    "\n",
    "    if best_per_avg_acc is not None:\n",
    "        print(f\"  per_platform best (avg): model={best_per_avg_acc['model']} ACC={_fmt(best_per_avg_acc['accuracy'])} F1={_fmt(best_per_avg_acc['f1'])}\")\n",
    "        if pd.notna(best_per_avg_acc.get(\"best_accuracy_platform\")):\n",
    "            print(f\"    best platform for that model: {best_per_avg_acc.get('best_accuracy_platform')} \"\n",
    "                  f\"(best_acc={_fmt(best_per_avg_acc.get('best_accuracy'))})\")\n",
    "    else:\n",
    "        print(\"  per_platform best (avg): N/A\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "def main():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"STAGE 3 (SELF-CONTAINED)  updated for models:\")\n",
    "    print(\"  \" + \", \".join(MODELS))\n",
    "    print(f\"Stage2 OUT_DIR: {OUT_DIR}\")\n",
    "    print(f\"Stage3 OUT:     {STAGE3_DIR}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    run_all_detection_delay()\n",
    "    run_all_benefits()\n",
    "\n",
    "    ok, rep = _safe_run(\"Summary Report\", generate_summary_report)\n",
    "    if ok and rep is not None:\n",
    "        _safe_json_dump(rep, REPORT_PATH)\n",
    "        print(f\"[OK] Saved report: {REPORT_PATH}\")\n",
    "\n",
    "        # print the requested summary table\n",
    "        _ = print_summary_table(rep)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8694296d-83d5-4609-b359-764ff32b4f18",
   "metadata": {},
   "source": [
    "# Platforms Detection Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e8eb6-9d54-4f27-8894-94f5a4fcd00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "GPU-Optimized Time-Series Classification Models for Platform Detection\n",
    "\n",
    "Handles:\n",
    "- LSTM classifier (PyTorch with CUDA)\n",
    "- GRU classifier (PyTorch with CUDA)\n",
    "- FTA-GRU simplified classifier (PyTorch with CUDA)\n",
    "- FTA-LSTM simplified classifier (PyTorch with CUDA)\n",
    "- Temporal GNN classifier (timesteps-as-nodes; PyTorch with CUDA)\n",
    "- IFCNN-TPP classifier (multi-kernel temporal CNN + mean pooling; PyTorch with CUDA)   <-- UPDATED\n",
    "- CEP3 classifier (dilated temporal convolution encoder; PyTorch with CUDA)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    f1_score, classification_report, confusion_matrix,\n",
    "    roc_auc_score, roc_curve, auc\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DEVICE\n",
    "# =============================================================================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# BASIC MODELS (LSTM, GRU)\n",
    "# =============================================================================\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"LSTM-based time-series classifier\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)         # (num_layers, B, H)\n",
    "        out = self.dropout(hidden[-1])        # (B, H)\n",
    "        return self.fc(out)                   # (B, C)\n",
    "\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    \"\"\"GRU-based time-series classifier\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, hidden = self.gru(x)               # (num_layers, B, H)\n",
    "        out = self.dropout(hidden[-1])        # (B, H)\n",
    "        return self.fc(out)                   # (B, C)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FTA ENCODERS (FTA-GRU, FTA-LSTM)\n",
    "# =============================================================================\n",
    "\n",
    "class FTAGru(nn.Module):\n",
    "    \"\"\"\n",
    "    FTA-GRU:\n",
    "    - Feature-level attention (gate per feature per timestep)\n",
    "    - Multi-layer GRU\n",
    "    - Multi-head temporal attention\n",
    "    - Attention pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_classes,\n",
    "                 num_gru_layers=2, attn_dim=64, num_attn_heads=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.feature_attention = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_gru_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_gru_layers > 1 else 0.0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.temporal_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=num_attn_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.attn_pooling = nn.Sequential(\n",
    "            nn.Linear(hidden_size, attn_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attn_dim, 1)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat_w = self.feature_attention(x)\n",
    "        x = x * feat_w\n",
    "        gru_out, _ = self.gru(x)\n",
    "        attn_out, _ = self.temporal_attention(gru_out, gru_out, gru_out)\n",
    "        scores = self.attn_pooling(attn_out).squeeze(-1)\n",
    "        weights = F.softmax(scores, dim=1).unsqueeze(-1)\n",
    "        pooled = (attn_out * weights).sum(dim=1)\n",
    "        pooled = self.dropout(pooled)\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "\n",
    "class FTALstm(nn.Module):\n",
    "    \"\"\"\n",
    "    FTA-LSTM:\n",
    "    - Feature-level attention\n",
    "    - Multi-layer LSTM\n",
    "    - Multi-head temporal attention\n",
    "    - Attention pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_classes,\n",
    "                 num_lstm_layers=2, attn_dim=64, num_attn_heads=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.feature_attention = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_lstm_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_lstm_layers > 1 else 0.0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.temporal_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=num_attn_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.attn_pooling = nn.Sequential(\n",
    "            nn.Linear(hidden_size, attn_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attn_dim, 1)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat_w = self.feature_attention(x)\n",
    "        x = x * feat_w\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attn_out, _ = self.temporal_attention(lstm_out, lstm_out, lstm_out)\n",
    "        scores = self.attn_pooling(attn_out).squeeze(-1)\n",
    "        weights = F.softmax(scores, dim=1).unsqueeze(-1)\n",
    "        pooled = (attn_out * weights).sum(dim=1)\n",
    "        pooled = self.dropout(pooled)\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TEMPORAL GNN (timesteps-as-nodes)\n",
    "# =============================================================================\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"Dense graph-attention across timesteps (treat T as nodes).\"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout=0.2, alpha=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2 * out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, x, adj=None):\n",
    "        B, T, _ = x.size()\n",
    "        h = torch.matmul(x, self.W)  # (B, T, Fout)\n",
    "\n",
    "        h_repeat_1 = h.repeat_interleave(T, dim=1)\n",
    "        h_repeat_2 = h.repeat(1, T, 1)\n",
    "        h_concat = torch.cat([h_repeat_1, h_repeat_2], dim=-1)\n",
    "\n",
    "        e = self.leakyrelu(torch.matmul(h_concat, self.a)).view(B, T, T)\n",
    "\n",
    "        if adj is not None:\n",
    "            e = e.masked_fill(adj == 0, -1e9)\n",
    "\n",
    "        attention = F.softmax(e, dim=-1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        return torch.matmul(attention, h)\n",
    "\n",
    "\n",
    "class GNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal GNN:\n",
    "    - GRU encodes per-timestep hidden states\n",
    "    - GAT across timesteps (dense)\n",
    "    - Multihead temporal attention\n",
    "    - Mean pooling -> classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_layers=2, gat_heads=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        self.gat_layers = nn.ModuleList([\n",
    "            GraphAttentionLayer(hidden_size, hidden_size // gat_heads, dropout=dropout)\n",
    "            for _ in range(gat_heads)\n",
    "        ])\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        self.temporal_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)  # (B, T, H)\n",
    "        gat_outputs = [gat(gru_out) for gat in self.gat_layers]\n",
    "        gat_combined = torch.cat(gat_outputs, dim=-1)  # (B, T, H)\n",
    "\n",
    "        gru_out = self.layer_norm(gru_out + gat_combined)\n",
    "        attn_out, _ = self.temporal_attention(gru_out, gru_out, gru_out)\n",
    "        pooled = attn_out.mean(dim=1)\n",
    "\n",
    "        out = self.dropout(pooled)\n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        return self.fc2(out)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# IFCNN-TPP (UPDATED: multi-kernel temporal CNN + mean pooling)\n",
    "# =============================================================================\n",
    "\n",
    "class IFCNNTPPClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    ifcnn_tpp (multi-kernel temporal CNN proxy):\n",
    "      - Linear projection: F -> H\n",
    "      - Parallel Conv1d branches over time with different kernel sizes (e.g., 3/5/7)\n",
    "      - Average branch outputs\n",
    "      - Mean pool over time\n",
    "      - MLP head -> logits\n",
    "\n",
    "    Input:  x (B, T, F)\n",
    "    Conv1d: expects (B, H, T)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        num_classes: int,\n",
    "        kernels=(3, 5, 7),\n",
    "        dropout: float = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(hidden_size, hidden_size, kernel_size=k, padding=k // 2)\n",
    "            for k in kernels\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F) -> h: (B, T, H)\n",
    "        h = self.norm(self.in_proj(x))\n",
    "\n",
    "        # Conv over time: (B, H, T)\n",
    "        h = h.transpose(1, 2)\n",
    "\n",
    "        outs = [F.relu(conv(h)) for conv in self.convs]           # list of (B, H, T)\n",
    "        z = torch.stack(outs, dim=0).mean(dim=0)                  # (B, H, T)\n",
    "\n",
    "        # back to (B, T, H) then mean pool over time -> (B, H)\n",
    "        z = z.transpose(1, 2)\n",
    "        pooled = z.mean(dim=1)\n",
    "\n",
    "        return self.head(pooled)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CEP3 (Dilated Temporal Convolution Encoder)\n",
    "# =============================================================================\n",
    "\n",
    "class CEP3Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    CEP3-like dilated TCN:\n",
    "      - Stacked dilated Conv1d blocks with residual connections\n",
    "      - Global average pooling over time\n",
    "      - Classification head\n",
    "\n",
    "    Input: (B, T, F)\n",
    "      Conv1d works on (B, F, T)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        num_classes: int,\n",
    "        channels: int = 128,\n",
    "        kernel_size: int = 3,\n",
    "        n_blocks: int = 4,\n",
    "        dropout: float = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert kernel_size % 2 == 1, \"Use odd kernel_size for simple same-length padding.\"\n",
    "\n",
    "        self.in_proj = nn.Conv1d(input_size, channels, kernel_size=1)\n",
    "\n",
    "        blocks = []\n",
    "        for i in range(n_blocks):\n",
    "            dilation = 2 ** i\n",
    "            pad = (kernel_size // 2) * dilation\n",
    "\n",
    "            blocks.append(nn.Sequential(\n",
    "                nn.Conv1d(channels, channels, kernel_size=kernel_size, padding=pad, dilation=dilation),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Conv1d(channels, channels, kernel_size=kernel_size, padding=pad, dilation=dilation),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "            ))\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "        self.out_norm = nn.LayerNorm(channels)\n",
    "        self.fc = nn.Linear(channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F) -> (B, F, T)\n",
    "        x = x.transpose(1, 2)\n",
    "        h = self.in_proj(x)  # (B, C, T)\n",
    "\n",
    "        # residual dilated blocks (length preserved by padding choice)\n",
    "        for blk in self.blocks:\n",
    "            r = h\n",
    "            h = blk(h)\n",
    "            h = h + r\n",
    "\n",
    "        # global average pool over time: (B, C)\n",
    "        pooled = h.mean(dim=-1)          # (B, C)\n",
    "        pooled = self.out_norm(pooled)   # LayerNorm over channels\n",
    "        return self.fc(pooled)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        n_classes = pred.size(-1)\n",
    "        log_probs = F.log_softmax(pred, dim=-1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(log_probs)\n",
    "            true_dist.fill_(self.smoothing / (n_classes - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        return torch.mean(torch.sum(-true_dist * log_probs, dim=-1))\n",
    "\n",
    "\n",
    "class TimeSeriesAugmentation:\n",
    "    @staticmethod\n",
    "    def jitter(x, sigma=0.03):\n",
    "        noise = np.random.normal(0, sigma, x.shape).astype(np.float32)\n",
    "        return x + noise\n",
    "\n",
    "    @staticmethod\n",
    "    def scaling(x, sigma=0.1):\n",
    "        factor = np.random.normal(1.0, sigma, (x.shape[0], 1, x.shape[2])).astype(np.float32)\n",
    "        return x * factor\n",
    "\n",
    "\n",
    "def augment_batch(X_batch, y_batch, aug_prob=0.3):\n",
    "    if np.random.random() < aug_prob:\n",
    "        choice = np.random.choice(['jitter', 'scaling'])\n",
    "        X_aug = X_batch.detach().cpu().numpy()\n",
    "        X_aug = TimeSeriesAugmentation.jitter(X_aug) if choice == 'jitter' else TimeSeriesAugmentation.scaling(X_aug)\n",
    "        return torch.FloatTensor(X_aug).to(X_batch.device), y_batch\n",
    "    return X_batch, y_batch\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING AND PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "def load_dataset(npz_path):\n",
    "    print(f\"\\nLoading dataset from {npz_path}...\")\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "\n",
    "    X = data['X']\n",
    "    y_platform = data['y_platform']\n",
    "    y_time_bucket = data['y_time_bucket']\n",
    "    platform_classes = data['platform_classes']\n",
    "\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"Number of platform classes: {len(platform_classes)}\")\n",
    "    print(f\"Example classes: {platform_classes[:5]}\")\n",
    "    return X, y_platform, y_time_bucket, platform_classes\n",
    "\n",
    "\n",
    "def prepare_dataloaders(X, y, batch_size=64, train_ratio=0.7, val_ratio=0.15):\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=(1 - train_ratio - val_ratio), random_state=42, stratify=y\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=val_ratio / (train_ratio + val_ratio),\n",
    "        random_state=42,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "\n",
    "    print(f\"\\nDataset splits:\")\n",
    "    print(f\"  Train: {X_train.shape[0]} samples\")\n",
    "    print(f\"  Val:   {X_val.shape[0]} samples\")\n",
    "    print(f\"  Test:  {X_test.shape[0]} samples\")\n",
    "\n",
    "    X_train_t = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_t = torch.LongTensor(y_train).to(device)\n",
    "    X_val_t = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_t = torch.LongTensor(y_val).to(device)\n",
    "    X_test_t = torch.FloatTensor(X_test).to(device)\n",
    "    y_test_t = torch.LongTensor(y_test).to(device)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(TensorDataset(X_val_t, y_val_t),     batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(TensorDataset(X_test_t, y_test_t),   batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING AND EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, use_augmentation=False, aug_prob=0.3):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        if use_augmentation:\n",
    "            X_batch, y_batch = augment_batch(X_batch, y_batch, aug_prob)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    return total_loss / max(1, len(train_loader)), 100.0 * correct / max(1, total)\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += float(loss.item())\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "            all_preds.extend(predicted.detach().cpu().numpy().tolist())\n",
    "            all_labels.extend(y_batch.detach().cpu().numpy().tolist())\n",
    "            all_probs.extend(probs.detach().cpu().numpy().tolist())\n",
    "\n",
    "    return (\n",
    "        total_loss / max(1, len(data_loader)),\n",
    "        100.0 * correct / max(1, total),\n",
    "        np.array(all_preds),\n",
    "        np.array(all_labels),\n",
    "        np.array(all_probs),\n",
    "    )\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=200, learning_rate=1e-3,\n",
    "                patience=10, use_label_smoothing=True, use_augmentation=True):\n",
    "    if use_label_smoothing:\n",
    "        criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "        print(\"Using Label Smoothing (0.1)\")\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_path = '/tmp/best_model.pth'\n",
    "\n",
    "    print(\"\\nTraining started...\")\n",
    "    print(f\"{'Epoch':>5} | {'Train Loss':>12} | {'Train Acc':>10} | {'Val Loss':>12} | {'Val Acc':>10}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer,\n",
    "            use_augmentation=use_augmentation, aug_prob=0.3\n",
    "        )\n",
    "        val_loss, val_acc, _, _, _ = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"{epoch+1:5d} | {train_loss:12.4f} | {train_acc:9.2f}% | {val_loss:12.4f} | {val_acc:9.2f}%\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred, y_probs, class_names):\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "    f1_per_class = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    y_true_bin = label_binarize(y_true, classes=range(len(class_names)))\n",
    "    if len(class_names) > 2:\n",
    "        roc_auc = roc_auc_score(y_true_bin, y_probs, average='macro', multi_class='ovr')\n",
    "    else:\n",
    "        roc_auc = roc_auc_score(y_true, y_probs[:, 1])\n",
    "\n",
    "    return {\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'f1_per_class': dict(zip(class_names, f1_per_class)),\n",
    "        'roc_auc': roc_auc,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': report\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATION (unchanged)\n",
    "# =============================================================================\n",
    "\n",
    "def plot_training_history(history, output_path):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "    axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "    axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Validation Loss'); axes[0].legend(); axes[0].grid(True)\n",
    "\n",
    "    axes[1].plot(history['train_acc'], label='Train Acc')\n",
    "    axes[1].plot(history['val_acc'], label='Val Acc')\n",
    "    axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Training and Validation Accuracy'); axes[1].legend(); axes[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Training history saved to {output_path}\")\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, output_path):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.xlabel('Predicted', fontsize=12)\n",
    "    plt.ylabel('True', fontsize=12)\n",
    "    plt.title('Confusion Matrix', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right'); plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix saved to {output_path}\")\n",
    "\n",
    "\n",
    "def plot_roc_curves(y_true, y_probs, class_names, output_path):\n",
    "    y_true_bin = label_binarize(y_true, classes=range(len(class_names)))\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(class_names)))\n",
    "\n",
    "    for i, (class_name, color) in enumerate(zip(class_names, colors)):\n",
    "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_probs[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'{class_name} (AUC = {roc_auc:.3f})', color=color, linewidth=2)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=2)\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title('ROC Curves (One-vs-Rest)', fontsize=14)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"ROC curves saved to {output_path}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    OUTPUT_DIR = '/home/Passwords/Analysis/Platform_Output'\n",
    "    DATA_PATH = '/home/Passwords/Analysis/platforms.npz'\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    X, y_platform, y_time_bucket, platform_classes = load_dataset(DATA_PATH)\n",
    "    train_loader, val_loader, test_loader = prepare_dataloaders(X, y_platform, batch_size=64)\n",
    "\n",
    "    input_size = X.shape[2]\n",
    "    num_classes = len(platform_classes)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TRAINING MODELS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Input size: {input_size}\")\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "    models_to_train = {\n",
    "        'LSTM': LSTMClassifier(input_size, 128, 2, num_classes, dropout=0.3).to(device),\n",
    "        'GRU': GRUClassifier(input_size, 128, 2, num_classes, dropout=0.3).to(device),\n",
    "        'FTA-GRU': FTAGru(input_size, 128, num_classes, num_gru_layers=2, attn_dim=64, num_attn_heads=4, dropout=0.3).to(device),\n",
    "        'FTA-LSTM': FTALstm(input_size, 128, num_classes, num_lstm_layers=2, attn_dim=64, num_attn_heads=4, dropout=0.3).to(device),\n",
    "        'GNN': GNNClassifier(input_size, 128, num_classes, num_layers=2, gat_heads=4, dropout=0.3).to(device),\n",
    "\n",
    "        # UPDATED: multi-kernel CNN proxy (no GRU/LSTM encoder)\n",
    "        'IFCNN-TPP': IFCNNTPPClassifier(\n",
    "            input_size=input_size,\n",
    "            hidden_size=128,\n",
    "            num_classes=num_classes,\n",
    "            kernels=(3, 5, 7),\n",
    "            dropout=0.3,\n",
    "        ).to(device),\n",
    "\n",
    "        'CEP3': CEP3Classifier(\n",
    "            input_size=input_size,\n",
    "            num_classes=num_classes,\n",
    "            channels=128,\n",
    "            kernel_size=3,\n",
    "            n_blocks=4,\n",
    "            dropout=0.3,\n",
    "        ).to(device),\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for model_name, model in models_to_train.items():\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"Training {model_name} Model\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "\n",
    "        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Number of parameters: {num_params:,}\")\n",
    "\n",
    "        # Treat attention/CNN/TCN as \"advanced\"\n",
    "        use_advanced = model_name in ['FTA-GRU', 'FTA-LSTM', 'IFCNN-TPP', 'CEP3']\n",
    "\n",
    "        trained_model, history = train_model(\n",
    "            model, train_loader, val_loader,\n",
    "            num_epochs=200,\n",
    "            learning_rate=0.001,\n",
    "            patience=10,\n",
    "            use_label_smoothing=use_advanced,\n",
    "            use_augmentation=use_advanced\n",
    "        )\n",
    "\n",
    "        print(f\"\\nEvaluating {model_name} on test set...\")\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        test_loss, test_acc, y_pred, y_true, y_probs = evaluate(trained_model, test_loader, criterion)\n",
    "\n",
    "        print(f\"Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "        metrics = compute_metrics(y_true, y_pred, y_probs, platform_classes)\n",
    "\n",
    "        print(f\"\\nF1 Score (Macro): {metrics['f1_macro']:.4f}\")\n",
    "        print(f\"F1 Score (Weighted): {metrics['f1_weighted']:.4f}\")\n",
    "        print(f\"ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
    "\n",
    "        results[model_name] = {\n",
    "            'model': trained_model,\n",
    "            'history': history,\n",
    "            'test_metrics': metrics,\n",
    "            'test_accuracy': test_acc\n",
    "        }\n",
    "\n",
    "        plot_training_history(history, f'{OUTPUT_DIR}/{model_name}_training_history.png')\n",
    "        plot_confusion_matrix(metrics['confusion_matrix'], platform_classes, f'{OUTPUT_DIR}/{model_name}_confusion_matrix.png')\n",
    "        plot_roc_curves(y_true, y_probs, platform_classes, f'{OUTPUT_DIR}/{model_name}_roc_curves.png')\n",
    "\n",
    "        torch.save(trained_model.state_dict(), f'{OUTPUT_DIR}/{model_name}_model.pth')\n",
    "        print(f\"\\n{model_name} training complete!\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Model': list(results.keys()),\n",
    "        'Test Accuracy (%)': [r['test_accuracy'] for r in results.values()],\n",
    "        'F1 Macro': [r['test_metrics']['f1_macro'] for r in results.values()],\n",
    "        'F1 Weighted': [r['test_metrics']['f1_weighted'] for r in results.values()],\n",
    "        'ROC-AUC': [r['test_metrics']['roc_auc'] for r in results.values()]\n",
    "    }).sort_values('Test Accuracy (%)', ascending=False)\n",
    "\n",
    "    print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "    comparison_df.to_csv(f'{OUTPUT_DIR}/model_comparison.csv', index=False)\n",
    "\n",
    "    # Comparison chart\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    metrics_to_plot = [\n",
    "        ('Test Accuracy (%)', axes[0, 0]),\n",
    "        ('F1 Macro', axes[0, 1]),\n",
    "        ('F1 Weighted', axes[1, 0]),\n",
    "        ('ROC-AUC', axes[1, 1])\n",
    "    ]\n",
    "\n",
    "    for metric_name, ax in metrics_to_plot:\n",
    "        data = comparison_df.sort_values(metric_name, ascending=True)\n",
    "        colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(data)))\n",
    "        ax.barh(data['Model'], data[metric_name], color=colors)\n",
    "        ax.set_xlabel(metric_name, fontsize=11)\n",
    "        ax.set_title(f'{metric_name} Comparison', fontsize=12, fontweight='bold')\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        for i, v in enumerate(data[metric_name]):\n",
    "            ax.text(v, i, f' {v:.3f}', va='center', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{OUTPUT_DIR}/model_comparison_chart.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Comparison chart saved to {OUTPUT_DIR}/model_comparison_chart.png\")\n",
    "\n",
    "    best_model = comparison_df.iloc[0]['Model']\n",
    "    best_acc = comparison_df.iloc[0]['Test Accuracy (%)']\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"BEST MODEL: {best_model} with {best_acc:.2f}% Test Accuracy\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    print(\"\\nGenerated files:\")\n",
    "    print(\"  - *_model.pth: Trained model weights\")\n",
    "    print(\"  - *_training_history.png: Training curves\")\n",
    "    print(\"  - *_confusion_matrix.png: Confusion matrices\")\n",
    "    print(\"  - *_roc_curves.png: ROC curves\")\n",
    "    print(\"  - model_comparison.csv: Performance comparison table\")\n",
    "    print(\"  - model_comparison_chart.png: Visual comparison\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e65f06-5bdb-4c77-8986-e19b29562ba8",
   "metadata": {},
   "source": [
    "## Detection Rate and delay for Platform detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e697774-68c7-4c4e-a958-5c83e69cffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Time-to-Detection Analysis for Cross-Platform Breach Platform Classification\n",
    "\n",
    "Measures how quickly we can correctly identify the breach platform over time.\n",
    "\n",
    "Models:\n",
    "- LSTM classifier (PyTorch with CUDA)\n",
    "- GRU classifier (PyTorch with CUDA)\n",
    "- FTA-GRU simplified classifier (PyTorch with CUDA)\n",
    "- FTA-LSTM simplified classifier (PyTorch with CUDA)\n",
    "- Temporal GNN classifier (timesteps-as-nodes; PyTorch with CUDA)\n",
    "- IFCNN-TPP classifier (Multi-kernel CNN; PyTorch with CUDA)   <-- UPDATED\n",
    "- CEP3 classifier (dilated temporal convolution encoder; PyTorch with CUDA)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL ARCHITECTURES (must match training code)\n",
    "# ============================================================================\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"LSTM-based time-series classifier\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)   # hidden: (num_layers, B, H)\n",
    "        out = self.dropout(hidden[-1])  # (B, H)\n",
    "        return self.fc(out)             # (B, C)\n",
    "\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    \"\"\"GRU-based time-series classifier\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, hidden = self.gru(x)         # hidden: (num_layers, B, H)\n",
    "        out = self.dropout(hidden[-1])  # (B, H)\n",
    "        return self.fc(out)             # (B, C)\n",
    "\n",
    "\n",
    "class FTAGru(nn.Module):\n",
    "    \"\"\"FTA-GRU: Feature-Temporal Attention GRU\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_classes,\n",
    "                 num_gru_layers=2, attn_dim=64, num_attn_heads=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.feature_attention = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_gru_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_gru_layers > 1 else 0.0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.temporal_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=num_attn_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.attn_pooling = nn.Sequential(\n",
    "            nn.Linear(hidden_size, attn_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attn_dim, 1)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat_w = self.feature_attention(x)\n",
    "        x = x * feat_w\n",
    "        gru_out, _ = self.gru(x)\n",
    "        attn_out, _ = self.temporal_attention(gru_out, gru_out, gru_out)\n",
    "        scores = self.attn_pooling(attn_out).squeeze(-1)\n",
    "        weights = F.softmax(scores, dim=1).unsqueeze(-1)\n",
    "        pooled = (attn_out * weights).sum(dim=1)\n",
    "        pooled = self.dropout(pooled)\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "\n",
    "class FTALstm(nn.Module):\n",
    "    \"\"\"FTA-LSTM: Feature-Temporal Attention LSTM\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_classes,\n",
    "                 num_lstm_layers=2, attn_dim=64, num_attn_heads=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.feature_attention = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_lstm_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_lstm_layers > 1 else 0.0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.temporal_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=num_attn_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.attn_pooling = nn.Sequential(\n",
    "            nn.Linear(hidden_size, attn_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attn_dim, 1)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat_w = self.feature_attention(x)\n",
    "        x = x * feat_w\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attn_out, _ = self.temporal_attention(lstm_out, lstm_out, lstm_out)\n",
    "        scores = self.attn_pooling(attn_out).squeeze(-1)\n",
    "        weights = F.softmax(scores, dim=1).unsqueeze(-1)\n",
    "        pooled = (attn_out * weights).sum(dim=1)\n",
    "        pooled = self.dropout(pooled)\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"Graph Attention Layer for dense timesteps-as-nodes graph\"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout=0.2, alpha=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2 * out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "    def forward(self, x, adj=None):\n",
    "        B, T, _ = x.size()\n",
    "        h = torch.matmul(x, self.W)  # (B,T,Fout)\n",
    "\n",
    "        h1 = h.repeat_interleave(T, dim=1)\n",
    "        h2 = h.repeat(1, T, 1)\n",
    "        hcat = torch.cat([h1, h2], dim=-1)\n",
    "\n",
    "        e = self.leakyrelu(torch.matmul(hcat, self.a)).view(B, T, T)\n",
    "\n",
    "        if adj is not None:\n",
    "            e = e.masked_fill(adj == 0, -1e9)\n",
    "\n",
    "        att = F.softmax(e, dim=-1)\n",
    "        att = F.dropout(att, self.dropout, training=self.training)\n",
    "        return torch.matmul(att, h)\n",
    "\n",
    "\n",
    "class GNNClassifier(nn.Module):\n",
    "    \"\"\"Temporal GNN: GRU -> GAT across timesteps -> temporal attention -> pool -> classify\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_layers=2, gat_heads=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.gat_layers = nn.ModuleList([\n",
    "            GraphAttentionLayer(hidden_size, hidden_size // gat_heads, dropout=dropout)\n",
    "            for _ in range(gat_heads)\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.temporal_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)  # (B,T,H)\n",
    "        gat_outs = [gat(gru_out) for gat in self.gat_layers]\n",
    "        gat_combined = torch.cat(gat_outs, dim=-1)  # (B,T,H)\n",
    "        h = self.layer_norm(gru_out + gat_combined)\n",
    "        attn_out, _ = self.temporal_attention(h, h, h)\n",
    "        pooled = attn_out.mean(dim=1)\n",
    "        out = self.dropout(pooled)\n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        return self.fc2(out)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# UPDATED: IFCNN-TPP (code-1 multi-kernel CNN version)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "class IFCNNTPPClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    IFCNN-TPP (Multi-kernel CNN, code-1 style):\n",
    "      - in_proj: Linear(F -> H)\n",
    "      - convs: multi-kernel Conv1d over time on hidden channels\n",
    "      - mean of conv branches\n",
    "      - mean pool over time\n",
    "      - head MLP\n",
    "\n",
    "    NOTE: This matches checkpoints with keys like:\n",
    "      in_proj.*, convs.*, norm.*, head.*\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_classes, kernels=(3, 5, 7), dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(input_size, hidden_size)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(hidden_size, hidden_size, kernel_size=k, padding=k // 2)\n",
    "            for k in kernels\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,T,F)\n",
    "        h = self.norm(self.in_proj(x))  # (B,T,H)\n",
    "        h = h.transpose(1, 2)           # (B,H,T)\n",
    "\n",
    "        outs = [F.relu(conv(h)) for conv in self.convs]   # each (B,H,T)\n",
    "        z = torch.stack(outs, dim=0).mean(dim=0)          # (B,H,T)\n",
    "\n",
    "        z = z.transpose(1, 2)       # (B,T,H)\n",
    "        pooled = z.mean(dim=1)      # (B,H)\n",
    "        return self.head(pooled)    # (B,C)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CEP3 (dilated temporal convolution encoder)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "class CEP3Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    CEP3-like dilated TCN with residual blocks.\n",
    "    Input: (B, T, F)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        num_classes: int,\n",
    "        channels: int = 128,\n",
    "        kernel_size: int = 3,\n",
    "        n_blocks: int = 4,\n",
    "        dropout: float = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert kernel_size % 2 == 1, \"Use odd kernel_size.\"\n",
    "\n",
    "        self.in_proj = nn.Conv1d(input_size, channels, kernel_size=1)\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        for i in range(n_blocks):\n",
    "            dilation = 2 ** i\n",
    "            pad = (kernel_size // 2) * dilation\n",
    "            self.blocks.append(nn.Sequential(\n",
    "                nn.Conv1d(channels, channels, kernel_size=kernel_size, padding=pad, dilation=dilation),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Conv1d(channels, channels, kernel_size=kernel_size, padding=pad, dilation=dilation),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "            ))\n",
    "\n",
    "        self.out_norm = nn.LayerNorm(channels)\n",
    "        self.fc = nn.Linear(channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B,T,F) -> (B,F,T)\n",
    "        x = x.transpose(1, 2)\n",
    "        h = self.in_proj(x)  # (B,C,T)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            r = h\n",
    "            h = blk(h)\n",
    "            h = h + r\n",
    "\n",
    "        pooled = h.mean(dim=-1)  # (B,C)\n",
    "        pooled = self.out_norm(pooled)\n",
    "        return self.fc(pooled)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATA LOADING AND MODEL LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_dataset(npz_path):\n",
    "    print(f\"\\nLoading dataset from {npz_path}...\")\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "\n",
    "    X = data[\"X\"]\n",
    "    y_platform = data[\"y_platform\"]\n",
    "    platform_classes = data[\"platform_classes\"]\n",
    "\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"Number of platform classes: {len(platform_classes)}\")\n",
    "    return X, y_platform, platform_classes\n",
    "\n",
    "\n",
    "def load_trained_model(model_type, input_size, num_classes, model_path):\n",
    "    \"\"\"\n",
    "    Load a trained model (must match the training scripts architecture/hparams).\n",
    "\n",
    "    Includes a safe auto-detect for IFCNN-TPP checkpoints to avoid key mismatches.\n",
    "    \"\"\"\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "\n",
    "    if model_type == \"LSTM\":\n",
    "        model = LSTMClassifier(input_size, 128, 2, num_classes, dropout=0.3)\n",
    "\n",
    "    elif model_type == \"GRU\":\n",
    "        model = GRUClassifier(input_size, 128, 2, num_classes, dropout=0.3)\n",
    "\n",
    "    elif model_type == \"FTA-GRU\":\n",
    "        model = FTAGru(\n",
    "            input_size=input_size,\n",
    "            hidden_size=128,\n",
    "            num_classes=num_classes,\n",
    "            num_gru_layers=2,\n",
    "            attn_dim=64,\n",
    "            num_attn_heads=4,\n",
    "            dropout=0.3,\n",
    "        )\n",
    "\n",
    "    elif model_type == \"FTA-LSTM\":\n",
    "        model = FTALstm(\n",
    "            input_size=input_size,\n",
    "            hidden_size=128,\n",
    "            num_classes=num_classes,\n",
    "            num_lstm_layers=2,\n",
    "            attn_dim=64,\n",
    "            num_attn_heads=4,\n",
    "            dropout=0.3,\n",
    "        )\n",
    "\n",
    "    elif model_type == \"GNN\":\n",
    "        model = GNNClassifier(\n",
    "            input_size=input_size,\n",
    "            hidden_size=128,\n",
    "            num_classes=num_classes,\n",
    "            num_layers=2,\n",
    "            gat_heads=4,\n",
    "            dropout=0.3,\n",
    "        )\n",
    "\n",
    "    elif model_type == \"IFCNN-TPP\":\n",
    "        # Expecting code-1 multi-kernel checkpoint keys:\n",
    "        # in_proj.*, convs.*, norm.*, head.*\n",
    "        if any(k.startswith(\"in_proj.\") for k in state.keys()):\n",
    "            model = IFCNNTPPClassifier(\n",
    "                input_size=input_size,\n",
    "                hidden_size=128,\n",
    "                num_classes=num_classes,\n",
    "                kernels=(3, 5, 7),\n",
    "                dropout=0.3,\n",
    "            )\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                \"IFCNN-TPP checkpoint does not look like the multi-kernel code-1 variant. \"\n",
    "                \"Expected keys like 'in_proj.*', 'convs.*', 'norm.*', 'head.*'.\"\n",
    "            )\n",
    "\n",
    "    elif model_type == \"CEP3\":\n",
    "        model = CEP3Classifier(\n",
    "            input_size=input_size,\n",
    "            num_classes=num_classes,\n",
    "            channels=128,\n",
    "            kernel_size=3,\n",
    "            n_blocks=4,\n",
    "            dropout=0.3,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "    model.load_state_dict(state)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TIME-TO-DETECTION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_detection_time(model, X, y_true, confidence_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Detection happens at time step t when:\n",
    "      1) predicted platform == true platform\n",
    "      2) confidence(pred) >= threshold\n",
    "\n",
    "    We evaluate partial prefixes by padding to full seq_len (so all models keep same input shape).\n",
    "    \"\"\"\n",
    "    detection_times = []\n",
    "    detection_confidences = []\n",
    "\n",
    "    seq_len = X.shape[1]\n",
    "    feat_dim = X.shape[2]\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        sequence = X[i]  # (seq_len, feat_dim)\n",
    "        true_label = int(y_true[i])\n",
    "\n",
    "        detection_time = -1\n",
    "        detection_conf = 0.0\n",
    "\n",
    "        for t in range(1, seq_len + 1):\n",
    "            partial = sequence[:t]  # (t, feat_dim)\n",
    "\n",
    "            if t < seq_len:\n",
    "                padding = np.zeros((seq_len - t, feat_dim), dtype=sequence.dtype)\n",
    "                padded = np.vstack([partial, padding])\n",
    "            else:\n",
    "                padded = partial\n",
    "\n",
    "            with torch.no_grad():\n",
    "                X_input = torch.FloatTensor(padded).unsqueeze(0).to(device)  # (1, seq_len, feat_dim)\n",
    "                out = model(X_input)\n",
    "                probs = torch.softmax(out, dim=1)\n",
    "                pred = int(torch.argmax(probs, dim=1).item())\n",
    "                conf = float(probs[0, pred].item())\n",
    "\n",
    "            if pred == true_label and conf >= confidence_threshold:\n",
    "                detection_time = t\n",
    "                detection_conf = conf\n",
    "                break\n",
    "\n",
    "        detection_times.append(int(detection_time))\n",
    "        detection_confidences.append(float(detection_conf))\n",
    "\n",
    "    return detection_times, detection_confidences\n",
    "\n",
    "\n",
    "def analyze_detection_metrics(detection_times):\n",
    "    valid = [t for t in detection_times if t > 0]\n",
    "    never = sum(1 for t in detection_times if t <= 0)\n",
    "\n",
    "    return {\n",
    "        \"total_sequences\": int(len(detection_times)),\n",
    "        \"detected\": int(len(valid)),\n",
    "        \"never_detected\": int(never),\n",
    "        \"detection_rate\": float(len(valid) / len(detection_times) * 100) if len(detection_times) else 0.0,\n",
    "        \"mean_detection_time\": float(np.mean(valid)) if valid else 0.0,\n",
    "        \"median_detection_time\": float(np.median(valid)) if valid else 0.0,\n",
    "        \"min_detection_time\": float(np.min(valid)) if valid else 0.0,\n",
    "        \"max_detection_time\": float(np.max(valid)) if valid else 0.0,\n",
    "        \"std_detection_time\": float(np.std(valid)) if valid else 0.0,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def plot_detection_time_distribution(detection_times, model_name, output_path):\n",
    "    valid_times = [t for t in detection_times if t > 0]\n",
    "    if not valid_times:\n",
    "        print(f\"No valid detections for {model_name}\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    max_time = max(valid_times)\n",
    "    bins = range(1, max_time + 2)\n",
    "\n",
    "    axes[0].hist(valid_times, bins=bins, edgecolor=\"black\", alpha=0.7)\n",
    "    axes[0].set_xlabel(\"Detection Time (timesteps)\")\n",
    "    axes[0].set_ylabel(\"Frequency\")\n",
    "    axes[0].set_title(f\"{model_name} - Distribution of Detection Times\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    sorted_times = sorted(valid_times)\n",
    "    cumulative = np.arange(1, len(sorted_times) + 1) / len(detection_times) * 100\n",
    "\n",
    "    axes[1].plot(sorted_times, cumulative, marker=\"o\", markersize=4, linewidth=2)\n",
    "    axes[1].set_xlabel(\"Detection Time (timesteps)\")\n",
    "    axes[1].set_ylabel(\"Cumulative Detection Rate (%)\")\n",
    "    axes[1].set_title(f\"{model_name} - Cumulative Detection Curve\")\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].axhline(y=50, linestyle=\"--\", alpha=0.7, label=\"50%\")\n",
    "    axes[1].axhline(y=90, linestyle=\"--\", alpha=0.7, label=\"90%\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Detection distribution saved to {output_path}\")\n",
    "\n",
    "\n",
    "def plot_detection_by_platform(detection_times, y_true, platform_classes, model_name, output_path):\n",
    "    platform_times = defaultdict(list)\n",
    "\n",
    "    for t, label in zip(detection_times, y_true):\n",
    "        if t > 0:\n",
    "            platform_times[platform_classes[int(label)]].append(int(t))\n",
    "\n",
    "    if not platform_times:\n",
    "        print(f\"No valid detections for {model_name}\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    platforms = list(platform_times.keys())\n",
    "    times_by_platform = [platform_times[p] for p in platforms]\n",
    "\n",
    "    ax.boxplot(times_by_platform, labels=platforms, patch_artist=True)\n",
    "    ax.set_xlabel(\"Platform\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Detection Time (timesteps)\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_title(f\"{model_name} - Detection Time by Breach Platform\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Detection by platform saved to {output_path}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"CROSS-PLATFORM TIME-TO-DETECTION ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    OUTPUT_DIR = \"/home/Passwords/Analysis/Platform_Output\"\n",
    "    DATA_PATH = \"/home/Passwords/Analysis/platforms.npz\"\n",
    "\n",
    "    STAGE3_DIR = os.path.join(OUTPUT_DIR, \"stage3\")\n",
    "    DELAY_DIR = os.path.join(STAGE3_DIR, \"delay\")\n",
    "    os.makedirs(DELAY_DIR, exist_ok=True)\n",
    "\n",
    "    X, y_platform, platform_classes = load_dataset(DATA_PATH)\n",
    "    input_size = X.shape[2]\n",
    "    num_classes = len(platform_classes)\n",
    "\n",
    "    # NOTE: Your training script may have saved IFCNN as \"ifcnn_tpp_model.pth\"\n",
    "    # We will try a few common filenames automatically for IFCNN-TPP.\n",
    "    def resolve_model_path(primary_path: str, fallbacks: list[str]) -> str:\n",
    "        if os.path.exists(primary_path):\n",
    "            return primary_path\n",
    "        for p in fallbacks:\n",
    "            if os.path.exists(p):\n",
    "                return p\n",
    "        return primary_path  # return primary even if missing; caller will handle\n",
    "\n",
    "    models_to_analyze = {\n",
    "        \"LSTM\":      f\"{OUTPUT_DIR}/LSTM_model.pth\",\n",
    "        \"GRU\":       f\"{OUTPUT_DIR}/GRU_model.pth\",\n",
    "        \"FTA-GRU\":   f\"{OUTPUT_DIR}/FTA-GRU_model.pth\",\n",
    "        \"FTA-LSTM\":  f\"{OUTPUT_DIR}/FTA-LSTM_model.pth\",\n",
    "        \"GNN\":       f\"{OUTPUT_DIR}/GNN_model.pth\",\n",
    "        \"IFCNN-TPP\": resolve_model_path(\n",
    "            f\"{OUTPUT_DIR}/IFCNN-TPP_model.pth\",\n",
    "            [\n",
    "                f\"{OUTPUT_DIR}/ifcnn_tpp_model.pth\",\n",
    "                f\"{OUTPUT_DIR}/IFCNN_TPP_model.pth\",\n",
    "                f\"{OUTPUT_DIR}/IFCNNTPP_model.pth\",\n",
    "            ],\n",
    "        ),\n",
    "        \"CEP3\":      f\"{OUTPUT_DIR}/CEP3_model.pth\",\n",
    "    }\n",
    "\n",
    "    all_stats = []\n",
    "\n",
    "    for model_name, model_path in models_to_analyze.items():\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"Analyzing {model_name} Model\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "\n",
    "        try:\n",
    "            if not os.path.exists(model_path):\n",
    "                print(f\"Model file not found: {model_path}\")\n",
    "                print(\"Please train the models first using the training script\")\n",
    "                continue\n",
    "\n",
    "            model = load_trained_model(model_name, input_size, num_classes, model_path)\n",
    "\n",
    "            print(f\"\\nComputing detection times for {model_name}...\")\n",
    "            detection_times, confidences = compute_detection_time(\n",
    "                model, X, y_platform, confidence_threshold=0.7\n",
    "            )\n",
    "\n",
    "            stats = analyze_detection_metrics(detection_times)\n",
    "\n",
    "            print(\"\\nDetection Statistics:\")\n",
    "            print(f\"  Total sequences: {stats['total_sequences']}\")\n",
    "            print(f\"  Detected: {stats['detected']} ({stats['detection_rate']:.2f}%)\")\n",
    "            print(f\"  Never detected: {stats['never_detected']}\")\n",
    "            print(\"\\nDetection Time (timesteps):\")\n",
    "            print(f\"  Mean: {stats['mean_detection_time']:.2f}\")\n",
    "            print(f\"  Median: {stats['median_detection_time']:.2f}\")\n",
    "            print(f\"  Std: {stats['std_detection_time']:.2f}\")\n",
    "            print(f\"  Range: [{stats['min_detection_time']:.0f}, {stats['max_detection_time']:.0f}]\")\n",
    "\n",
    "            # Stage3-compatible JSON\n",
    "            delay_json = {\n",
    "                \"model\": model_name,\n",
    "                \"setting\": \"cross_platform\",\n",
    "                \"threshold\": 0.7,\n",
    "                \"n_rows\": int(len(X)),\n",
    "                \"n_entities_detected\": int(stats[\"detected\"]),\n",
    "                \"n_entities_with_positive\": int(stats[\"total_sequences\"]),\n",
    "                \"detection_rate_entities\": float(stats[\"detection_rate\"] / 100.0),\n",
    "                \"delay_mean\": float(stats[\"mean_detection_time\"]),\n",
    "                \"delay_median\": float(stats[\"median_detection_time\"]),\n",
    "                \"delay_min\": float(stats[\"min_detection_time\"]),\n",
    "                \"delay_max\": float(stats[\"max_detection_time\"]),\n",
    "            }\n",
    "\n",
    "            delay_json_path = os.path.join(\n",
    "                DELAY_DIR,\n",
    "                f\"cross_platform__{model_name.lower().replace('-', '_')}.json\"\n",
    "            )\n",
    "            with open(delay_json_path, \"w\") as f:\n",
    "                json.dump(delay_json, f, indent=2)\n",
    "            print(f\"Saved delay metrics to: {delay_json_path}\")\n",
    "\n",
    "            plot_detection_time_distribution(\n",
    "                detection_times, model_name,\n",
    "                f\"{OUTPUT_DIR}/{model_name}_detection_distribution.png\"\n",
    "            )\n",
    "\n",
    "            plot_detection_by_platform(\n",
    "                detection_times, y_platform, platform_classes, model_name,\n",
    "                f\"{OUTPUT_DIR}/{model_name}_detection_by_platform.png\"\n",
    "            )\n",
    "\n",
    "            all_stats.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Detection Rate (%)\": stats[\"detection_rate\"],\n",
    "                \"Mean Time (timesteps)\": stats[\"mean_detection_time\"],\n",
    "                \"Median Time (timesteps)\": stats[\"median_detection_time\"],\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {model_name}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "    if all_stats:\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"MODEL COMPARISON - TIME-TO-DETECTION\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        comparison_df = pd.DataFrame(all_stats).sort_values(\"Detection Rate (%)\", ascending=False)\n",
    "        print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "        comparison_df.to_csv(f\"{OUTPUT_DIR}/detection_time_comparison.csv\", index=False)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "        colors1 = plt.cm.viridis(np.linspace(0.3, 0.9, len(comparison_df)))\n",
    "        axes[0].bar(comparison_df[\"Model\"], comparison_df[\"Detection Rate (%)\"], color=colors1)\n",
    "        axes[0].set_ylabel(\"Detection Rate (%)\", fontsize=11, fontweight=\"bold\")\n",
    "        axes[0].set_title(\"Detection Rate Comparison\", fontsize=12, fontweight=\"bold\")\n",
    "        axes[0].grid(True, alpha=0.3, axis=\"y\")\n",
    "        axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "        for i, (_, row) in enumerate(comparison_df.iterrows()):\n",
    "            axes[0].text(\n",
    "                i, row[\"Detection Rate (%)\"],\n",
    "                f'{row[\"Detection Rate (%)\"]:.1f}%',\n",
    "                ha=\"center\", va=\"bottom\", fontweight=\"bold\"\n",
    "            )\n",
    "\n",
    "        colors2 = plt.cm.plasma(np.linspace(0.3, 0.9, len(comparison_df)))\n",
    "        axes[1].bar(comparison_df[\"Model\"], comparison_df[\"Mean Time (timesteps)\"], color=colors2)\n",
    "        axes[1].set_ylabel(\"Mean Detection Time (timesteps)\", fontsize=11, fontweight=\"bold\")\n",
    "        axes[1].set_title(\"Mean Detection Time Comparison\", fontsize=12, fontweight=\"bold\")\n",
    "        axes[1].grid(True, alpha=0.3, axis=\"y\")\n",
    "        axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "        for i, (_, row) in enumerate(comparison_df.iterrows()):\n",
    "            axes[1].text(\n",
    "                i, row[\"Mean Time (timesteps)\"],\n",
    "                f'{row[\"Mean Time (timesteps)\"]:.1f}',\n",
    "                ha=\"center\", va=\"bottom\", fontweight=\"bold\"\n",
    "            )\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{OUTPUT_DIR}/model_detection_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(f\"\\nComparison plot saved to {OUTPUT_DIR}/model_detection_comparison.png\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ANALYSIS COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nGenerated files:\")\n",
    "    print(\"  - *_detection_distribution.png: Detection time distributions\")\n",
    "    print(\"  - *_detection_by_platform.png: Detection times by platform\")\n",
    "    print(\"  - detection_time_comparison.csv: Model comparison table\")\n",
    "    print(\"  - model_detection_comparison.png: Visual comparison\")\n",
    "    print(\"  - stage3/delay/*.json: Detection metrics (Stage3 compatible)\")\n",
    "    print(\"\\nModels analyzed: LSTM, GRU, FTA-GRU, FTA-LSTM, GNN, IFCNN-TPP, CEP3 (cross-platform)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef5eaae-838f-4ab0-a58c-d998cbc53c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d4f1f2-24b2-4060-8aa8-f84b7fa5a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Hardcoded file paths (edit if your notebook path differs)\n",
    "# -------------------------------------------------------------------\n",
    "FILES = {\n",
    "    \"cep3\":      \"/home/afam/Passwords/Analysis/Detection_Output/per_platform__cep3/results_per_platform_cep3.json\",\n",
    "    \"gru\":       \"/home/afam/Passwords/Analysis/Detection_Output/per_platform__fta_gru/results_per_platform_fta_gru.json\",\n",
    "    \"fta_lstm\":  \"/home/afam/Passwords/Analysis/Detection_Output/per_platform__fta_lstm/results_per_platform_fta_lstm.json\",\n",
    "    \"ifcnn_tpp\": \"/home/afam/Passwords/Analysis/Detection_Output/per_platform__ifcnn_tpp/results_per_platform_ifcnn_tpp.json\",\n",
    "    \"gnn\":       \"/home/afam/Passwords/Analysis/Detection_Output/per_platform__gnn/results_per_platform_gnn.json\",\n",
    "    \"lstm\":      \"/home/afam/Passwords/Analysis/Detection_Output/per_platform__lstm/results_per_platform_lstm.json\",\n",
    "    \"fta_gru\" : \"/home/afam/Passwords/Analysis/Detection_Output/per_platform__fta_gru/results_per_platform_fta_gru.json\"\n",
    "}\n",
    "\n",
    "METRICS = [\"accuracy\", \"precision\", \"recall\", \"f1\", \"auroc\", \"auprc\"]\n",
    "\n",
    "def _is_num(x):\n",
    "    return isinstance(x, (int, float))\n",
    "\n",
    "def _is_nan(x):\n",
    "    return isinstance(x, float) and math.isnan(x)\n",
    "\n",
    "def collect_values(per_platform_metrics: dict, metric: str):\n",
    "    \"\"\"Collect numeric values for metric, excluding NaNs.\"\"\"\n",
    "    vals = []\n",
    "    for _, d in per_platform_metrics.items():\n",
    "        v = d.get(metric, None)\n",
    "        if v is None or (not _is_num(v)) or _is_nan(v):\n",
    "            continue\n",
    "        vals.append(float(v))\n",
    "    return vals\n",
    "\n",
    "def best_avg(vals):\n",
    "    \"\"\"Return (best=max, avg=mean).\"\"\"\n",
    "    if not vals:\n",
    "        return None, None, 0\n",
    "    return max(vals), sum(vals) / len(vals), len(vals)\n",
    "\n",
    "def summarize_file(path: str, model_name: str):\n",
    "    with open(path, \"r\") as f:\n",
    "        obj = json.load(f)\n",
    "\n",
    "    ppm = obj.get(\"per_platform_metrics\", {})\n",
    "    if not isinstance(ppm, dict) or not ppm:\n",
    "        raise ValueError(f\"{model_name}: 'per_platform_metrics' missing/empty in {path}\")\n",
    "\n",
    "    row = {\"model\": model_name}\n",
    "\n",
    "    # Best + Avg for each metric (NaNs ignored)\n",
    "    for m in METRICS:\n",
    "        vals = collect_values(ppm, m)\n",
    "        b, a, n = best_avg(vals)\n",
    "        row[f\"{m}_best\"] = b\n",
    "        row[f\"{m}_avg\"] = a\n",
    "        row[f\"{m}_n\"] = n  # how many platforms contributed to this metric (after dropping NaNs)\n",
    "\n",
    "    # optional: how many platforms had meaningful operating point\n",
    "    row[\"num_platforms_total\"] = len(ppm)\n",
    "    row[\"num_platforms_f1_nonzero\"] = sum(\n",
    "        1 for _, d in ppm.items()\n",
    "        if _is_num(d.get(\"f1\", 0.0)) and float(d.get(\"f1\", 0.0)) != 0.0\n",
    "    )\n",
    "\n",
    "    return row\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Build combined summary table for all models\n",
    "# -------------------------------------------------------------------\n",
    "rows = []\n",
    "for model, path in FILES.items():\n",
    "    rows.append(summarize_file(path, model))\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# nicer column ordering\n",
    "ordered_cols = [\"model\", \"num_platforms_total\", \"num_platforms_f1_nonzero\"]\n",
    "for m in METRICS:\n",
    "    ordered_cols += [f\"{m}_best\", f\"{m}_avg\", f\"{m}_n\"]\n",
    "\n",
    "df = df[ordered_cols]\n",
    "\n",
    "# format floats for display (keep full precision in df if you want)\n",
    "df_display = df.copy()\n",
    "for c in df_display.columns:\n",
    "    if c.endswith(\"_best\") or c.endswith(\"_avg\"):\n",
    "        df_display[c] = df_display[c].map(lambda x: f\"{x:.6f}\" if isinstance(x, (int, float)) and x is not None else str(x))\n",
    "\n",
    "df_display\n",
    "df.to_csv(\"/home/afam/Passwords/Analysis/Detection_Output/output.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8085c243-358f-4be5-b967-48e2be04c7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>avg_detection_rate_entities</th>\n",
       "      <th>zero_detection_rate_count</th>\n",
       "      <th>num_platforms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cep3</td>\n",
       "      <td>0.032990</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fta_gru</td>\n",
       "      <td>0.017190</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fta_lstm</td>\n",
       "      <td>0.045312</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gnn</td>\n",
       "      <td>0.080985</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gru</td>\n",
       "      <td>0.060258</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ifcnn_tpp</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lstm</td>\n",
       "      <td>0.032046</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model avg_detection_rate_entities  zero_detection_rate_count  \\\n",
       "0       cep3                    0.032990                         19   \n",
       "1    fta_gru                    0.017190                         18   \n",
       "2   fta_lstm                    0.045312                         19   \n",
       "3        gnn                    0.080985                         18   \n",
       "4        gru                    0.060258                         17   \n",
       "5  ifcnn_tpp                    0.000000                         20   \n",
       "6       lstm                    0.032046                         18   \n",
       "\n",
       "   num_platforms  \n",
       "0             22  \n",
       "1             22  \n",
       "2             22  \n",
       "3             22  \n",
       "4             22  \n",
       "5             22  \n",
       "6             22  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CONFIG: folder containing per_platform__*.json files\n",
    "# ------------------------------------------------------------\n",
    "BASE_DIR = Path(\"/home/afam/Passwords/Analysis/Detection_Output/detection_delay\")   # change if needed\n",
    "\n",
    "TARGET_MODELS = {\n",
    "    \"cep3\",\n",
    "    \"gru\",\n",
    "    \"fta_lstm\",\n",
    "    \"ifcnn_tpp\",\n",
    "    \"gnn\",\n",
    "    \"lstm\",\n",
    "    \"fta_gru\",\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helper\n",
    "# ------------------------------------------------------------\n",
    "def safe_float(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, float) and math.isnan(x):\n",
    "        return None\n",
    "    return float(x)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Collect detection_rate_entities per model\n",
    "# ------------------------------------------------------------\n",
    "records = []\n",
    "\n",
    "for json_file in BASE_DIR.glob(\"per_platform__*.json\"):\n",
    "    name = json_file.stem\n",
    "    # expected format: per_platform__MODEL__PLATFORM\n",
    "    parts = name.split(\"__\")\n",
    "    if len(parts) < 3:\n",
    "        continue\n",
    "\n",
    "    _, model, platform = parts[0], parts[1], \"__\".join(parts[2:])\n",
    "    if model not in TARGET_MODELS:\n",
    "        continue\n",
    "\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    dr = safe_float(data.get(\"detection_rate_entities\"))\n",
    "\n",
    "    records.append({\n",
    "        \"model\": model,\n",
    "        \"platform\": platform,\n",
    "        \"detection_rate_entities\": dr,\n",
    "        \"is_zero\": (dr == 0.0),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Aggregate: average + zero-count per model\n",
    "# ------------------------------------------------------------\n",
    "summary = (\n",
    "    df.groupby(\"model\")\n",
    "      .agg(\n",
    "          avg_detection_rate_entities=(\n",
    "              \"detection_rate_entities\",\n",
    "              lambda x: x.dropna().mean()\n",
    "          ),\n",
    "          zero_detection_rate_count=(\n",
    "              \"is_zero\",\n",
    "              \"sum\"\n",
    "          ),\n",
    "          num_platforms=(\"platform\", \"count\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(\"model\")\n",
    ")\n",
    "\n",
    "# Pretty formatting for display\n",
    "summary_display = summary.copy()\n",
    "summary_display[\"avg_detection_rate_entities\"] = (\n",
    "    summary_display[\"avg_detection_rate_entities\"]\n",
    "    .map(lambda x: f\"{x:.6f}\" if pd.notna(x) else \"NaN\")\n",
    ")\n",
    "\n",
    "summary_display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41098ea6-7b6c-491a-bc15-4b04b5a95167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>highest_detection_rate_entities</th>\n",
       "      <th>zero_detection_rate_count</th>\n",
       "      <th>num_platforms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cep3</td>\n",
       "      <td>0.659794</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fta_gru</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fta_lstm</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gnn</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gru</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ifcnn_tpp</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lstm</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model highest_detection_rate_entities  zero_detection_rate_count  \\\n",
       "0       cep3                        0.659794                         19   \n",
       "1    fta_gru                        0.320000                         18   \n",
       "2   fta_lstm                        0.906250                         19   \n",
       "3        gnn                        0.989691                         18   \n",
       "4        gru                        0.660000                         17   \n",
       "5  ifcnn_tpp                        0.000000                         20   \n",
       "6       lstm                        0.610000                         18   \n",
       "\n",
       "   num_platforms  \n",
       "0             22  \n",
       "1             22  \n",
       "2             22  \n",
       "3             22  \n",
       "4             22  \n",
       "5             22  \n",
       "6             22  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CONFIG: folder containing per_platform__*.json files\n",
    "# ------------------------------------------------------------\n",
    "BASE_DIR = Path(\"/home/afam/Passwords/Analysis/Detection_Output/detection_delay\")\n",
    "\n",
    "TARGET_MODELS = {\n",
    "    \"cep3\",\n",
    "    \"gru\",\n",
    "    \"fta_lstm\",\n",
    "    \"ifcnn_tpp\",\n",
    "    \"gnn\",\n",
    "    \"lstm\",\n",
    "    \"fta_gru\",\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helper\n",
    "# ------------------------------------------------------------\n",
    "def safe_float(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, float) and math.isnan(x):\n",
    "        return None\n",
    "    return float(x)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Collect detection_rate_entities per model\n",
    "# ------------------------------------------------------------\n",
    "records = []\n",
    "\n",
    "for json_file in BASE_DIR.glob(\"per_platform__*.json\"):\n",
    "    name = json_file.stem\n",
    "    # expected format: per_platform__MODEL__PLATFORM\n",
    "    parts = name.split(\"__\")\n",
    "    if len(parts) < 3:\n",
    "        continue\n",
    "\n",
    "    _, model, platform = parts[0], parts[1], \"__\".join(parts[2:])\n",
    "    if model not in TARGET_MODELS:\n",
    "        continue\n",
    "\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    dr = safe_float(data.get(\"detection_rate_entities\"))\n",
    "\n",
    "    records.append({\n",
    "        \"model\": model,\n",
    "        \"platform\": platform,\n",
    "        \"detection_rate_entities\": dr,\n",
    "        \"is_zero\": (dr == 0.0),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Aggregate: HIGHEST (not average) + zero-count per model\n",
    "# ------------------------------------------------------------\n",
    "summary = (\n",
    "    df.groupby(\"model\")\n",
    "      .agg(\n",
    "          highest_detection_rate_entities=(\n",
    "              \"detection_rate_entities\",\n",
    "              lambda x: x.dropna().max()\n",
    "          ),\n",
    "          zero_detection_rate_count=(\n",
    "              \"is_zero\",\n",
    "              \"sum\"\n",
    "          ),\n",
    "          num_platforms=(\"platform\", \"count\")\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(\"model\")\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Pretty formatting for display\n",
    "# ------------------------------------------------------------\n",
    "summary_display = summary.copy()\n",
    "summary_display[\"highest_detection_rate_entities\"] = (\n",
    "    summary_display[\"highest_detection_rate_entities\"]\n",
    "    .map(lambda x: f\"{x:.6f}\" if pd.notna(x) else \"NaN\")\n",
    ")\n",
    "\n",
    "summary_display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242cba32-7248-4d22-9c4d-a1f75f1b2a20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
